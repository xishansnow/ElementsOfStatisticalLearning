
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5.8 正则化和再生核希尔伯特空间理论 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5.9 小波光滑" href="5.9-Wavelet-Smoothing.html" />
    <link rel="prev" title="5.7 多维样条" href="5.7-Multidimensional-Splines.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.3-Subset-Selection.html">
     3.3 子集的选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多重输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5.4-Smoothing-Splines.html">
     5.4 光滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 光滑参数的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="5.9-Wavelet-Smoothing.html">
     5.9 小波光滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核光滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(\IR^p\)
     </span>
     中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     <span class="math notranslate nohighlight">
      \(\IR^p\)
     </span>
     中结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差，方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的 optimism
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.11-Bootstrap-Methods.html">
     7.11 自助法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 条件测试误差或期望测试误差？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.2-The-Bootstrap-and-Maximum-Likelihood-Methods.html">
     8.2 自助法和最大似然法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.4-Relationship-Between-the-Bootstrap-and-Bayesian-Inference.html">
     8.4 自助法和贝叶斯推断之间的关系
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5 EM 算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆栈
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14-Unsupervised-Learning/14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   核产生的函数空间
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rkhs">
   RKHS 的例子
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     带惩罚的多项式回归
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     高斯径向基函数
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     支持向量机
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1>5.8 正则化和再生核希尔伯特空间理论<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>这一节我们将样条放进更大的正规方法框架下以及 <strong>再生核希尔伯特空间 (reproducing kernel Hilbert spaces)</strong> 中．这部分非常专业 (quite technical)，因此不感兴趣或者有些畏惧的读者可以跳过．</p>
<p>一般的正则化问题形式如下</p>
<div class="math notranslate nohighlight">
\[
\underset{f\in{\cal H}}{\min}\Big[
\sum\limits_{i=1}^NL(y_i,f(x_i))+\lambda J(f)
\Big]\,,
\tag{5.42}\label{5.42}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(L(y,f(x))\)</span> 是损失函数，<span class="math notranslate nohighlight">\(J(f)\)</span> 是惩罚函数，<span class="math notranslate nohighlight">\(\cal H\)</span> 是 <span class="math notranslate nohighlight">\(J(f)\)</span> 有定义的函数空间．Girosi et al. (1995)<a class="footnote-reference brackets" href="#id15" id="id2">1</a> 描述了形如下式的非常一般的惩罚函数</p>
<div class="math notranslate nohighlight">
\[
J(f)=\int_{\IR^d}\frac{\vert \tilde f(s)\vert^2}{\tilde G(s)}ds \tag{5.43}\label{5.43}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\tilde f\)</span> 记为 <span class="math notranslate nohighlight">\(f\)</span> 的 Fourier 变换，并且 <span class="math notranslate nohighlight">\(\tilde G\)</span> 是当 <span class="math notranslate nohighlight">\(\Vert s\Vert\rightarrow \infty\)</span> 趋于 <span class="math notranslate nohighlight">\(0\)</span> 的正函数．上式想法是 <span class="math notranslate nohighlight">\(1/\tilde G\)</span> 加大对 <span class="math notranslate nohighlight">\(f\)</span> 的高频组分的惩罚．在一些额外的假设下，他们证明解有如下形式</p>
<div class="math notranslate nohighlight">
\[
f(X)=\sum\limits_{k=1}^K\alpha_k\phi_k(X)+\sum\limits_{i=1}^N\theta_iG(X-x_i)\tag{5.44}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\phi_k\)</span> 张成惩罚函数 <span class="math notranslate nohighlight">\(J\)</span> 的零空间，并且 <span class="math notranslate nohighlight">\(G\)</span> 是 <span class="math notranslate nohighlight">\(\tilde G\)</span> 的逆 Fourier 变换．光滑样条和 thin-plate 样条都属于这个框架．这个解的显著特点是当准则 \eqref{5.42} 定义在无限维空间，解是有限维．在下一节我们考虑一些具体的例子．</p>
<div class="section" id="id3">
<h2>核产生的函数空间<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>形如 \eqref{5.42} 的问题的一个重要子类是由正定核 <span class="math notranslate nohighlight">\(K(x,y)\)</span> 产生的，对应的函数空间 <span class="math notranslate nohighlight">\({\cal H}_K\)</span> 被称为 <strong>再生核希尔伯特空间 (reproducing kernel Hilbert space)</strong>，简称为 RKHS．惩罚函数也是用核来定义的．我们对这个模型类进行一个简短的介绍，这取自 Wahba (1990)<a class="footnote-reference brackets" href="#id16" id="id4">2</a> 和 Girosi et al. (1995)<a class="footnote-reference brackets" href="#id15" id="id5">1</a>，并且在 Evgeniou et al. (2000)<a class="footnote-reference brackets" href="#id17" id="id6">3</a> 中有很好的总结．</p>
<p>令 <span class="math notranslate nohighlight">\(x,y\in \IR^p\)</span>．我们考虑由 <span class="math notranslate nohighlight">\(\\{K(\cdot, y), y\in \IR^p\\}\)</span> 线性张成的函数空间；也就是，形如 <span class="math notranslate nohighlight">\(f(x)=\sum_m\alpha_mK(x, y_m)\)</span> 的任意线性组合，其中每个核可以看成第一个变量的函数，并且由第二个变量索引．假设 <span class="math notranslate nohighlight">\(K\)</span> 有 <strong>特征展开 (eigen-expansion)</strong></p>
<div class="math notranslate nohighlight">
\[
K(x, y)=\sum\limits_{i=1}^\infty \gamma_i \phi_i(x)\phi_i(y)\tag{5.45}\label{5.45}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\gamma_i\ge 0,\sum_{i=1}^\infty \gamma_i^2 &lt; \infty\)</span>．</p>
<p>!!! note “weiya 注：Mercer’s theorem”
\eqref{5.45} 的分解由 Mercer’s theorem 保证，Mercer’s theorem 将半正定矩阵的特征分解推广到半正定核函数的特征分解，具体地，
<img alt="" src="../img/12/wiki_mercer.PNG" />
图片来源: <a class="reference external" href="https://en.wikipedia.org/wiki/Mercer%27s_theorem">Wiki: Mercer’s theorem</a>．</p>
<p><span class="math notranslate nohighlight">\({\cal H}_K\)</span> 的元素是关于这些 <strong>特征函数 (eigen-functions)</strong> 的展开，即</p>
<div class="math notranslate nohighlight">
\[
f(x)=\sum\limits_{i=1}^\infty c_i\phi_i(x)\tag{5.46}
\]</div>
<p>并且约束条件为</p>
<div class="math notranslate nohighlight">
\[
\Vert f \Vert_{{\cal H}_K}^2\overset{def}{=}\sum\limits_{i=1}^\infty c_i^2/\gamma_i &lt; \infty\tag{5.47}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\Vert f\Vert_{{\cal H}\_K}\)</span> 由 <span class="math notranslate nohighlight">\(K\)</span> 导出的范数．</p>
<p>!!! note “weiya 注：Induced Norm”
希尔伯特空间是完备内积空间，对于一般的希尔伯特空间 <span class="math notranslate nohighlight">\(\cal H\)</span>，其 <strong>导出范数 (induced norm)</strong> 为
$<span class="math notranslate nohighlight">\(
    \Vert f\Vert_{{\cal H}} := \sqrt{\langle f,f\rangle_{{\cal H}}}\,.
    \)</span>$</p>
<p>!!! note “weiya 注：两种构造 RKHS 的方法”
一般地，有两种构造 RKHS 的方法：</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>第一种，给定一个半正定核 $K$，定义映射 $\Phi:\calX\mapsto \IR^\calX$ 为 $\Phi(x) = K(\cdot, x)$，然后考虑向量空间 
$$
\span(\{\Phi(x):x\in\calX\}) = \left\{f(\cdot) = \sum_{i=1}^N\alpha_iK(\cdot, x_i)\right\}\,,
$$
并对 $f = \sum_{i=1}^N\alpha_iK(\cdot, u_i), g = \sum_{i=1}^N\beta_iK(\cdot, v_i)$ 定义内积 
$$
\langle f, g\rangle = \sum_{i=1}^N\sum_{j=1}^N\alpha_i\beta_jK(u_i,v_j)\,,
$$
则 $K$ 满足核再生性质，并且 $\overline{\span(\{\Phi(x)\})}$ 定义了 RKHS.

另外一种，则是用 Mercer 定理，定义内积
$$
\langle f, g\rangle_{\calH} = \sum_{j=1}^{\infty}\frac{\langle f,\phi_j\rangle\langle g,\phi_j\rangle}{\mu_j}
$$
则
$$
\left\{f=\sum_{j=1}^\infty c_j\phi_j\mid \sum_{j=1}^\infty c_j^2/\mu_j &lt; \infty\right\}
$$
为 RKHS. 

参考 [Wainwright (2019)](https://doi.org/10.1017/9781108627771) 和 [Peter Bartlett&#39;s notes](https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf).
</pre></div>
</div>
<p>\eqref{5.42} 中空间 <span class="math notranslate nohighlight">\({\cal H}\_K\)</span> 的惩罚函数定义为二次范数 <span class="math notranslate nohighlight">\(J(f)=\Vert f\Vert_{{\cal H}\_K}^2\)</span>．<span class="math notranslate nohighlight">\(J(f)\)</span> 的值可以解释为广义岭惩罚，其中在展开式 \eqref{5.45} 中，大的特征值惩罚较小，反之亦然．</p>
<p>重写 \eqref{5.42}，我们有</p>
<div class="math notranslate nohighlight">
\[
\underset{f\in {\cal H}_K}{\min}\Big[\sum\limits_{i=1}^NL(y_i, f(x_i))+\lambda \Vert f\Vert_{{\cal H}_K}^2\Big]\tag{5.48}\label{5.48}
\]</div>
<p>或者等价地，</p>
<div class="math notranslate nohighlight">
\[
\underset{\{c_j\}_1^\infty}{\min}\Big[\sum\limits_{i=1}^NL(y_i, \sum\limits_{j=1}^\infty c_j\phi_j(x_i))+\lambda \sum\limits_{j=1}^\infty c_j^2/\gamma_j\Big]\tag{5.49}\label{5.49}
\]</div>
<p>可以证明（Wahba, 1990<a class="footnote-reference brackets" href="#id16" id="id7">2</a>, 另见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/163">练习 5.15</a>），\eqref{5.48} 的解是有限的，并且如下形式</p>
<div class="math notranslate nohighlight">
\[
f(x)=\sum\limits_{i=1}^N\alpha_i K(x, x_i)\tag{5.50}\label{5.50}
\]</div>
<p>!!! info “weiya 注：Ex. 5.15”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/163">Issue 163: Ex. 5.15</a>, 其中证明了再生核性质。</p>
<p>基函数 <span class="math notranslate nohighlight">\(h_i(x)=K(x,x_i)\)</span>（关于第一个变量的函数）被称作 <span class="math notranslate nohighlight">\({\cal H}\_K\)</span> 中 <span class="math notranslate nohighlight">\(x_i\)</span> 处的 <strong>representer of evaluation</strong>，因为对于 <span class="math notranslate nohighlight">\(f\in {\cal H}\_K\)</span>，容易看到 <span class="math notranslate nohighlight">\(\langle K(\cdot, x_i),f\rangle_{{\cal H}\_K} = f(x_i)\)</span>．类似地，<span class="math notranslate nohighlight">\(\langle K(\cdot, x_i), K(\cdot,x_j)\rangle_{{\cal H}\_K}=K(x_i, x_j)\)</span>（<span class="math notranslate nohighlight">\({\cal H}\_K\)</span> 的再生性质），</p>
<p>!!! note “weiya 注: 核再生性质 (kernel reproducing property)”
对于任意的 <span class="math notranslate nohighlight">\(x\in {\cal X}\)</span>, <span class="math notranslate nohighlight">\(K(\cdot, x) \in \cal H\)</span>，并且满足
$<span class="math notranslate nohighlight">\(
    \langle f, K(\cdot, x)\rangle_{\cal H} = f(x)\qquad \text{for all }f\in \cal H\,.
    \)</span><span class="math notranslate nohighlight">\(
    这称为 **核再生性质 (kernel reproducing property)**．另外，对于任意半正定核 \)</span>K<span class="math notranslate nohighlight">\(，存在唯一的希尔伯特空间 \)</span>\cal H<span class="math notranslate nohighlight">\( 满足核再生性质，此时 \)</span>\cal H$ 也被称之为 <strong>再生核希尔伯特空间 (RKHS)</strong>.</p>
<p>也因此对于 <span class="math notranslate nohighlight">\(f(x)=\sum_{i=1}^N\alpha_iK(x,x_i)\)</span></p>
<div class="math notranslate nohighlight">
\[
J(f)=\sum\limits_{i=1}^N\sum\limits_{j=1}^NK(x_i,x_j)\alpha_i\alpha_j\tag{5.51}\label{5.51}
\]</div>
<p>根据 \eqref{5.50} 和 \eqref{5.51}，\eqref{5.48} 退化为有限维准则</p>
<div class="math notranslate nohighlight">
\[
\underset{\aalpha}{\min} L(\y,\K\aalpha)+\lambda\aalpha^T\K\aalpha\tag{5.52}\label{5.52}
\]</div>
<p>我们正在使用向量记号，其中 <span class="math notranslate nohighlight">\(\K\)</span> 是第 <span class="math notranslate nohighlight">\(ij\)</span> 个元素为 <span class="math notranslate nohighlight">\(K(x_i,x_j)\)</span> 的 <span class="math notranslate nohighlight">\(N\times N\)</span> 的矩阵．简单的数值算法可以用来优化 \eqref{5.52}．无限维问题 \eqref{5.48} 或 \eqref{5.49} 退化为有限维优化问题的现象在支持向量机（见<span class="xref myst">第 12 章</span>）中被称为 <strong>核性质 (kernel property)</strong>．</p>
<p>这类模型有一个贝叶斯解释，其中 <span class="math notranslate nohighlight">\(f\)</span> 被解释为零均值平稳高斯过程的实现，其中先验协方差函数为 <span class="math notranslate nohighlight">\(K\)</span>．特征值分解得到一系列方差为 <span class="math notranslate nohighlight">\(\gamma_j\)</span> 的正交特征函数 <span class="math notranslate nohighlight">\(\phi_j(x)\)</span>．一般的情形是，“光滑”函数 <span class="math notranslate nohighlight">\(\phi_j\)</span> 有更大的先验方差，而“粗糙”的 <span class="math notranslate nohighlight">\(\phi_j\)</span> 有较小的先验方差．\eqref{5.48} 中的惩罚是先验对联合概率的贡献度，并且方差越小惩罚越大（与 \eqref{5.43} 相比）．</p>
<p>为了简便，我们这里处理所有 <span class="math notranslate nohighlight">\(\cal H\)</span> 中的成员都被惩罚的情形，如 \eqref{5.48}．更一般地，<span class="math notranslate nohighlight">\(\cal H\)</span> 中可能有些组分我们希望单独留下来，比如 <span class="xref myst">5.4 节</span> 中的三次光滑样条的线性函数．<span class="xref myst">5.7 节</span> 的多维 thin-plate 样条以及张量积样条也都属于这类．在这些情形下，有个更方便的表示 <span class="math notranslate nohighlight">\(\cal H=\cal H_0\oplus\cal H_1\)</span>，举个例子，其中零空间 <span class="math notranslate nohighlight">\(\cal H_0\)</span> 由没有被惩罚的 <span class="math notranslate nohighlight">\(x\)</span> 的低阶多项式组成．惩罚项变为 <span class="math notranslate nohighlight">\(J(f)=\Vert P_1f\Vert\)</span>，其中 <span class="math notranslate nohighlight">\(P_1\)</span> 是 <span class="math notranslate nohighlight">\(f\)</span> 在 <span class="math notranslate nohighlight">\(\cal H_1\)</span> 上的正交投影．这个解形式为 <span class="math notranslate nohighlight">\(f(x)=\sum_{j=1}^M\beta_jh_j(x)+\sum_{i=1}^N\alpha_iK(x,x_i)\)</span>，其中第一项表示 <span class="math notranslate nohighlight">\(\cal H_0\)</span> 中的展开．从贝叶斯的观点看，<span class="math notranslate nohighlight">\(\cal H_0\)</span> 中组分的系数的先验的方差无穷大．</p>
</div>
<div class="section" id="rkhs">
<h2>RKHS 的例子<a class="headerlink" href="#rkhs" title="Permalink to this headline">¶</a></h2>
<p>上述的机理是由核 <span class="math notranslate nohighlight">\(K\)</span> 和损失函数 <span class="math notranslate nohighlight">\(L\)</span> 的选择 <strong>驱动 (driven)</strong> 的 ．我们首先考虑采用平方误差损失的回归．将 \eqref{5.48} 中的惩罚特定为最小二乘，则解可以用对应 \eqref{5.49} 或 \eqref{5.52} 的两个等价方式进行描述：</p>
<p>对于无穷维的广义岭回归问题
$<span class="math notranslate nohighlight">\(
\underset{\{c_j\}_1^\infty}{\min}\sum\limits_{i=1}^N\Big(y_i-\sum\limits_{j=1}^\infty c_j\phi_j(x_i)\Big)^2+\lambda\sum\limits_{j=1}^\infty \frac{c_j^2}{\gamma_j}\tag{5.53}\label{5.53}
\)</span><span class="math notranslate nohighlight">\(
或者写成
\)</span><span class="math notranslate nohighlight">\(
\underset{\alpha}{\min}(\y-\K\alpha)^T(\y-\K\alpha)+\lambda \aalpha^T\K\aalpha\tag{5.54}\label{5.54}
\)</span>$</p>
<p>易得 <span class="math notranslate nohighlight">\(\alpha\)</span> 的解</p>
<div class="math notranslate nohighlight">
\[
\hat\aalpha = (\K+\lambda\I)^{-1}\y\tag{5.55}\label{5.55}
\]</div>
<p>且</p>
<div class="math notranslate nohighlight">
\[
\hat f(x)=\sum\limits_{j=1}^N\hat\alpha_jK(x,x_j)\tag{5.56}
\]</div>
<p><span class="math notranslate nohighlight">\(N\)</span> 个拟合值的向量为</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\hat{\f} &amp;=\K\hat\aalpha\notag\\
&amp;=\K(\K+\lambda\I)^{-1}\y\tag{5.57}\label{5.57}\\
&amp;=(\I+\lambda \K^{-1})^{-1}\y\tag{5.58}\label{5.58}
\end{align}
\end{split}\]</div>
<p>估计值 \eqref{5.57} 也是稀疏统计 (Cressie, 1993)<a class="footnote-reference brackets" href="#id18" id="id8">4</a> 中高斯随机域的 kriging 估计．另外 \eqref{5.58} 可以与光滑样条拟合 \eqref{5.17} 进行比较．</p>
<p>!!! note “weiya 注：Recall”
$<span class="math notranslate nohighlight">\(
    \mathbf S_\lambda=(\mathbf I+\lambda \mathbf K)^{-1}\tag{5.17}\label{5.17}
    \)</span>$</p>
<div class="section" id="id9">
<h3>带惩罚的多项式回归<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>核 <span class="math notranslate nohighlight">\(K(x,y)=(\langle x, y\rangle+1)^d\)</span> (Vapnik, 1996)<a class="footnote-reference brackets" href="#id19" id="id10">5</a>，对于 <span class="math notranslate nohighlight">\(x,y\in \IR^p\)</span>，有 <span class="math notranslate nohighlight">\(M=\binom{p+d}{d}\)</span> 个特征函数，它张成了 <span class="math notranslate nohighlight">\(\IR^p\)</span> 中总阶数为 <span class="math notranslate nohighlight">\(d\)</span> 的多项式空间．举个例子，当 <span class="math notranslate nohighlight">\(p=2, d=2,M=6\)</span>，有</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
K(x,y)&amp;=1+2x_1y_1+2x_2y_2+x_1^2y_1^2+x_2^2y_2^2+2x_1x_2y_1y_2\tag{5.59}\\
&amp;=\sum\limits_{m=1}^Mh_m(x)h_m(y)\tag{5.60}
\end{align}
\end{split}\]</div>
<p>其中</p>
<div class="math notranslate nohighlight">
\[
h(x)^T=(1,\sqrt{2}x_1, \sqrt{2}x_2, x_1^2, x_2^2, \sqrt{2}x_1x_2)\tag{5.61}\label{5.61}
\]</div>
<p>可以用 <span class="math notranslate nohighlight">\(M\)</span> 个 <span class="math notranslate nohighlight">\(K\)</span> 的特征函数和特征值来表示 <span class="math notranslate nohighlight">\(h\)</span></p>
<div class="math notranslate nohighlight">
\[
h(x)=\V\D_\gamma^{\frac 12}\phi(x)\tag{5.62}\label{5.62}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\D_\gamma=\diag(\gamma_1,\gamma_2,\ldots, \gamma_M)\)</span>，并且 <span class="math notranslate nohighlight">\(\V\)</span> 是 <span class="math notranslate nohighlight">\(M\times M\)</span> 的，且为正交．</p>
<p>!!! info “weiya 注：”
练习 5.16(a) 要求推导 \eqref{5.62}，该小问已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/164">Issue 164: Ex. 5.16</a>．</p>
<p>假设我们希望求解带惩罚的多项式回归问题</p>
<div class="math notranslate nohighlight">
\[
\underset{\{\beta_m\}_1^M}{\min}\sum\limits_{i=1}^N\Big(y_i-\sum\limits_{m=1}^M\beta_mh_m(x_i)\Big)^2+\lambda \sum\limits_{m=1}^M\beta_m^2\tag{5.63}\label{5.63}
\]</div>
<p>将 \eqref{5.62} 代入 \eqref{5.63}，我们得到 \eqref{5.53} 的展开来进行优化（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/164">练习 5.16</a>）．</p>
<p>!!! info “weiya 注：Ex. 5.16”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/164">Issue 164: Ex. 5.16</a>．</p>
<p>基函数的个数 <span class="math notranslate nohighlight">\(M=\binom{p+d}{d}\)</span> 可以非常大，通常大于 <span class="math notranslate nohighlight">\(N\)</span>．等式 \eqref{5.55} 告诉我们如果采用解函数的核表示，我们仅仅需要对核进行 <span class="math notranslate nohighlight">\(N^2\)</span> 次赋值，而且可以以 <span class="math notranslate nohighlight">\(O(N^3)\)</span> 的计算量得到解．</p>
<p>This simplicity is not without implications. \eqref{5.61} 中的每个多项式 <span class="math notranslate nohighlight">\(h_m\)</span> 从 <span class="math notranslate nohighlight">\(K\)</span> 的特定形式继承了缩放因子，这对 \eqref{5.63} 的惩罚有影响．我们将在下一节详细讨论．</p>
</div>
<div class="section" id="id11">
<h3>高斯径向基函数<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>在前面的例子中，选择核是因为能表示成多项式的展开，这样可以方便地计算高维内积．在这个例子中，选择核是因为 \eqref{5.50} 中的函数形式．</p>
<p>举个例子，在平方误差损失下，高斯核 <span class="math notranslate nohighlight">\(K(x,y)=e^{-\nu \Vert x-y\Vert^2}\)</span> 能得到展开式为高斯径向基函数的回归模型，</p>
<div class="math notranslate nohighlight">
\[
k_m(x)=e^{-\nu \Vert x-x_m\Vert^2},m=1,\ldots, N\tag{5.64}
\]</div>
<p>每个点都在训练特征向量 <span class="math notranslate nohighlight">\(x_m\)</span> 处中心化了．可以采用 \eqref{5.54} 来估计参数．</p>
<p><img alt="" src="../_images/fig5.13.png" /></p>
<p>图 5.13 采用第二章混合例子中的第一个坐标展示了 <span class="math notranslate nohighlight">\(\IR^1\)</span> 中的径向核．我们展示了 <span class="math notranslate nohighlight">\(200\)</span> 个核基函数 <span class="math notranslate nohighlight">\(k_m(x)=K(x,x_m)\)</span> 中的五个．</p>
<p><img alt="" src="../_images/fig5.14.png" /></p>
<p>图 5.14 展示了 <span class="math notranslate nohighlight">\(x\in \IR^1\)</span> 的径向核的隐式特征空间．我们计算 <span class="math notranslate nohighlight">\(200\times 200\)</span> 的核矩阵 <span class="math notranslate nohighlight">\(\K\)</span>，以及其特征分解 <span class="math notranslate nohighlight">\(\Phi\D_\gamma\Phi^T\)</span>．我们可以认为 <span class="math notranslate nohighlight">\(\Phi\)</span> 的列和 <span class="math notranslate nohighlight">\(\D_\gamma\)</span> 中对应的特征值是特征展开 \eqref{5.45} 中的经验估计．</p>
<p>!!! note “原书脚注：”
<span class="math notranslate nohighlight">\(\Phi\)</span> 的第 <span class="math notranslate nohighlight">\(\ell\)</span> 列是 <span class="math notranslate nohighlight">\(\phi_\ell\)</span> 的一个估计，这是对 <span class="math notranslate nohighlight">\(N\)</span> 个观测中的每一个进行赋值．另外，<span class="math notranslate nohighlight">\(\Phi\)</span> 的第 <span class="math notranslate nohighlight">\(i\)</span> 行是基函数 <span class="math notranslate nohighlight">\(\phi(x_i)\)</span>（在 <span class="math notranslate nohighlight">\(x_i\)</span> 处取值）的估计向量．尽管原则上，<span class="math notranslate nohighlight">\(\phi\)</span> 可以有无穷多的元素，但我们的估计至多有 <span class="math notranslate nohighlight">\(N\)</span> 个元素．</p>
<p>尽管特征向量是离散的，但我们还是可以将它们表示成 <span class="math notranslate nohighlight">\(\IR^1\)</span> 中的函数（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/165">练习 5.17</a>）．</p>
<p><img alt="" src="../_images/fig5.15.png" /></p>
<p>!!! info “weiya 注：Ex. 5.17”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/165">Issue 165: Ex. 5.17</a>．</p>
<p>图 5.15 展示了 <span class="math notranslate nohighlight">\(\K\)</span> 的最大的 <span class="math notranslate nohighlight">\(50\)</span> 个特征值．最大特征值对应的特征函数是光滑的，并且它们随着 order 增加变得更加弯曲．这使得 \eqref{5.49} 中的惩罚变成了可能，其中我们看到高阶函数的系数比低阶函数惩罚更多．图 5.14 中的右面板显示了下面特征函数对应的特征空间的表示</p>
<div class="math notranslate nohighlight">
\[
h_\ell(x)=\sqrt{\hat{\gamma_\ell}}\hat\phi_\ell(x),\ell=1,\ldots, N\tag{5.65}
\]</div>
<p>注意到 <span class="math notranslate nohighlight">\(\langle h(x_i), h(x_{i'})\rangle=K(x_i, x_{i'})\)</span>．通过特征值来缩放快速将大部分的函数降为 <span class="math notranslate nohighlight">\(0\)</span>，在这种情形下留下大约 <span class="math notranslate nohighlight">\(12\)</span> 个有效维度．对应的优化问题是如 \eqref{5.63} 中标准的岭回归．所以尽管原则上隐式的特征空间是无穷维的，但有效维度还是非常小的，因为对每个基函数应用相对大小的收缩．核缩放参数 <span class="math notranslate nohighlight">\(\nu\)</span> 在这里也起一定作用；更大的 <span class="math notranslate nohighlight">\(\nu\)</span> 意味着更多局部的 <span class="math notranslate nohighlight">\(k_m\)</span> 函数，并且也增加了特征空间的有效维度．更多细节参见 Hastie and Zhu (2006)<a class="footnote-reference brackets" href="#id20" id="id12">6</a>．</p>
<p>我们知道被称为 thin-plate 样条 (<span class="xref myst">5.7 节</span>)是关于径向基函数的展开 (Girosi et al., 1995)<a class="footnote-reference brackets" href="#id15" id="id13">1</a>，它由下列核产生</p>
<div class="math notranslate nohighlight">
\[
K(x, y)=\Vert x-y\Vert^2\LOG(\Vert x-y\Vert)\tag{5.66}
\]</div>
<p>径向基函数将在 <span class="xref myst">6.7 节</span>详细讨论．</p>
</div>
<div class="section" id="id14">
<h3>支持向量机<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p><span class="xref myst">第 12 章</span>中用于两个类别分类的支持向量机有形式 <span class="math notranslate nohighlight">\(f(x)=\alpha_0+\sum_{i=1}^N\alpha_iK(x,x_i)\)</span>，选择参数使下式最小化</p>
<div class="math notranslate nohighlight">
\[
\underset{\alpha_0,\aalpha}{\min}\Big\{
  \sum\limits_{i=1}^N[1-y_if(x_i)]_+ + \frac{\lambda}{2}\aalpha^T\K\aalpha
\Big\}
\tag{5.67}\label{5.67}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(y_i\in \\{-1, 1\\}\)</span>，并且 <span class="math notranslate nohighlight">\([z]_+\)</span> 记做 <span class="math notranslate nohighlight">\(z\)</span> 的正值部分．这可以看成是带线性约束的二次优化问题，并要求该解的二次规划算法．支持向量 (support vector) 的名字来自这样一个事实： 一般有许多的<span class="math notranslate nohighlight">\(\hat\alpha_i=0\)</span>【因为 \eqref{5.67} 中损失函数的 piecewise-zero 特性】，也因此 <span class="math notranslate nohighlight">\(\hat f\)</span> 是 <span class="math notranslate nohighlight">\(K(\cdot, x_i)\)</span> 的子集的展开．更多细节见 <a class="reference external" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels/index.html#svm_2">12.3.3节</a>．</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id15"><span class="brackets">1</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id5">2</a>,<a href="#id13">3</a>)</span></dt>
<dd><p>Girosi, F., Jones, M. and Poggio, T. (1995). Regularization theory and neural network architectures, Neural Computation 7: 219–269.</p>
</dd>
<dt class="label" id="id16"><span class="brackets">2</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id7">2</a>)</span></dt>
<dd><p>Wahba, G. (1990). Spline Models for Observational Data, SIAM, Philadelphia.</p>
</dd>
<dt class="label" id="id17"><span class="brackets"><a class="fn-backref" href="#id6">3</a></span></dt>
<dd><p>Evgeniou, T., Pontil, M. and Poggio, T. (2000). Regularization networks and support vector machines, Advances in Computational Mathematics 13(1): 1–50.</p>
</dd>
<dt class="label" id="id18"><span class="brackets"><a class="fn-backref" href="#id8">4</a></span></dt>
<dd><p>Cressie, N. (1993). Statistics for Spatial Data (Revised Edition), Wiley-Interscience, New York.</p>
</dd>
<dt class="label" id="id19"><span class="brackets"><a class="fn-backref" href="#id10">5</a></span></dt>
<dd><p>Vapnik, V. (1996). The Nature of Statistical Learning Theory, Springer, New York.</p>
</dd>
<dt class="label" id="id20"><span class="brackets"><a class="fn-backref" href="#id12">6</a></span></dt>
<dd><p>Hastie, T. and Zhu, J. (2006). Discussion of “Support vector machines with applications” by Javier Moguerza and Alberto Munoz, Statistical Science 21(3): 352–357.</p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./05-Basis-Expansions-and-Regularization"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="5.7-Multidimensional-Splines.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">5.7 多维样条</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="5.9-Wavelet-Smoothing.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">5.9 小波光滑</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>