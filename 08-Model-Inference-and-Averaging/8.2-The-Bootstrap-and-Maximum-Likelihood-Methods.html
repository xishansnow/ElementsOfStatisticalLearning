
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8.2 自助法和最大似然法 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8.3 贝叶斯方法" href="8.3-Bayesian-Methods.html" />
    <link rel="prev" title="8.1 导言" href="8.1-Introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.3-Subset-Selection.html">
     3.3 子集的选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多重输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 光滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 光滑参数的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波光滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核光滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差，方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的 optimism
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.11-Bootstrap-Methods.html">
     7.11 自助法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 条件测试误差或期望测试误差？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     8.2 自助法和最大似然法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.4-Relationship-Between-the-Bootstrap-and-Bayesian-Inference.html">
     8.4 自助法和贝叶斯推断之间的关系
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.5-The-EM-Algorithm.html">
     8.5 EM 算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆栈
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14-Unsupervised-Learning/14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/08-Model-Inference-and-Averaging/8.2-The-Bootstrap-and-Maximum-Likelihood-Methods.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F08-Model-Inference-and-Averaging/8.2-The-Bootstrap-and-Maximum-Likelihood-Methods.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   平滑的例子
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   最大似然推断
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   自助法和最大似然法
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1>8.2 自助法和最大似然法<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id2">
<h2>平滑的例子<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>自助法提供了一个通过在训练数据中取样来评估不确定性的直接计算的方式。这里我们用一个简单的一维平滑问题来解释自助法，并且展示它与最大似然之间的联系。</p>
<p><img alt="" src="../_images/fig8.1.png" /></p>
<blockquote>
<div><p>图 8.1. (左图)：光滑例子的数据。（右图：）7 个 B-样条基函数的集合。垂直虚线为放置树结点的地方。</p>
</div></blockquote>
<p>用 <span class="math notranslate nohighlight">\(\mathbf Z=\\{z_1,z_2,\ldots,z_N\\}\)</span> 表示训练数据，其中 <span class="math notranslate nohighlight">\(z_i=(x_i,y_i),i=1,2,\ldots,N.\)</span> 这里 <span class="math notranslate nohighlight">\(x_i\)</span> 是一维输入，<span class="math notranslate nohighlight">\(y_i\)</span> 为输出，可以是连续的也可以是离散的。举个例子，考虑在图 8.1 的左图中展示的 <span class="math notranslate nohighlight">\(N=50\)</span> 个数据点。</p>
<p>假设我们决定用三次样条去拟合数据，三个结点放置在 <span class="math notranslate nohighlight">\(X\)</span> 的分位数上。这是一个七维线性函数空间。</p>
<p>!!! note “weiya 注”
含 <span class="math notranslate nohighlight">\(K\)</span> 个结点的普通三次样条有 <span class="math notranslate nohighlight">\(K+4\)</span> 个基函数，而自然三次样条有 <span class="math notranslate nohighlight">\(K\)</span> 个基函数。</p>
<p>举个例子，可以通过 B-样条基函数的线性展开来表示（见 <span class="xref myst">5.9.2 节</span>）：
$<span class="math notranslate nohighlight">\(
\mu(x)=\sum\limits_{j=1}^7\beta_jh_j(x).\tag{8.1}
\)</span><span class="math notranslate nohighlight">\(
这里 \)</span>h_j(x),j=1,2,\ldots,7<span class="math notranslate nohighlight">\( 是在图 8.1 右图展示的 7 个函数。我们可以将 \)</span>\mu(x)<span class="math notranslate nohighlight">\( 看成是条件期望 \)</span>\E(Y\mid X=x)$.</p>
<p>令 <span class="math notranslate nohighlight">\(\mathbf H\)</span> 为 <span class="math notranslate nohighlight">\(N\times 7\)</span> 的矩阵，且第 <span class="math notranslate nohighlight">\(ij\)</span> 个元素为 <span class="math notranslate nohighlight">\(h_j(x_i)\)</span>。<span class="math notranslate nohighlight">\(\beta\)</span> 的一般估计通过最小化训练集上的均方误差得到，由下式给出
$<span class="math notranslate nohighlight">\(
\hat \beta=\mathbf{(H^TH)^{-1}H^Ty}\tag{8.2}\label{8.2}
\)</span>$</p>
<p>对应的拟合值 <span class="math notranslate nohighlight">\(\hat \mu(x)=\sum_{j=1}^7\hat \beta_jh_j(x)\)</span> 显示在图8.2的左上图。</p>
<p><span class="math notranslate nohighlight">\(\hat\beta\)</span> 的协方差矩阵估计为
$<span class="math notranslate nohighlight">\(
\widehat{\Var}(\hat\beta)=\mathbf{(H^TH)}^{-1}\hat\sigma^2\tag{8.3}\label{8.3}
\)</span><span class="math notranslate nohighlight">\(
其中白噪声方差的估计值由 \)</span>\hat\sigma^2=\sum_{i=1}^N(y_i-\hat\mu(x_i))^2/N<span class="math notranslate nohighlight">\( 给出。令 \)</span>h(x)^T=(h_1(x),h_2(x),\ldots,h_7(x))<span class="math notranslate nohighlight">\(,预测值 \)</span>\hat\mu(x)=h(x)^T\hat\beta<span class="math notranslate nohighlight">\( 的标准误差为
\)</span><span class="math notranslate nohighlight">\(
\widehat{se}[\hat\mu(x)]=[h(x)^T\mathbf{(H^TH)^{-1}}h(x)]^{\frac{1}{2}}\hat{\sigma}\tag{8.4}\label{8.4}
\)</span>$
<img alt="" src="../_images/fig8.2.png" /></p>
<blockquote>
<div><p>图 8.2. (左上图：)数据点的B-样条光滑。（右上图：）B-样条光滑加减1.96倍的标准误差得到的带状区域。（左下图：）B-样条光滑的自助法重复取样。（右下图：）从自助法的分布计算得到的B-样条和95%的标准误差区域带。</p>
</div></blockquote>
<p>在图 8.2 的右上图中我们画出了 <span class="math notranslate nohighlight">\(\hat\mu(x)\pm 1.96\cdot\widehat{se}[\hat\mu(x)]\)</span>,因为 1.96 是标准正态分布函数 97.5% 的那个点，这表示 <span class="math notranslate nohighlight">\(\mu(x)\)</span> 的近似 <span class="math notranslate nohighlight">\(100-2\times 2.5\%=95\%\)</span> 逐点置信域。</p>
<p>这里我们介绍怎么在这个例子中运用自助法。我们从训练集中有放回抽取 <span class="math notranslate nohighlight">\(B\)</span> 个大小均为 <span class="math notranslate nohighlight">\(N=50\)</span> 的数据集，取样单元为数据对 <span class="math notranslate nohighlight">\(z_i=(x_i,y_i)\)</span>。对于每一个自助法的数据集 <span class="math notranslate nohighlight">\(\mathbf Z^\*\)</span> 我们拟合三次样条 <span class="math notranslate nohighlight">\(\hat \mu^\*(x)\)</span>；从 10 个这样的自助样本中得到的拟合曲线如图 8.2 的左下图所示。利用 <span class="math notranslate nohighlight">\(B=200\)</span> 个自助法样本集，我们可以得到每个 <span class="math notranslate nohighlight">\(x\)</span> 处的 95% 置信区间：我们在每个 <span class="math notranslate nohighlight">\(x\)</span> 处找到第 <span class="math notranslate nohighlight">\(2.5\%\times 200=5\)</span> 大（小）的值。这在图 8.2的右下图画出来了。这些带状与右上角的带状显示，不过在终点处更宽点。</p>
<p>实际上最小二乘估计 \eqref{8.2} 和 \eqref{8.3}，自助法和最大似然法有着很紧密的联系。进一步假设模型误差为高斯分布，
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-4e010b72-699e-4a99-acf5-1070a4b1afbf">
<span class="eqno">(18)<a class="headerlink" href="#equation-4e010b72-699e-4a99-acf5-1070a4b1afbf" title="Permalink to this equation">¶</a></span>\[\begin{align}
Y&amp;=\mu(X)+\varepsilon;\; \varepsilon\sim N(0,\sigma^2),\notag\\
\mu(X)&amp;=\sum\limits_{j=1}^7\beta_jh_j(x)\notag
\end{align}\]</div>
<p>\tag{8.5}
\label{8.5}
$<span class="math notranslate nohighlight">\(
上面描述的自助法，通过从训练集中有放回地采样，称作 **非参自助法(nonparametric bootstrap)**。这实际上意味着这个方法是与模型无关的，因为它使用原始数据来得到新的数据集，而不是一个特定的含参数的模型。考虑到自助法的一种变形，称为 **参数自助法 (parametric bootstrap)**，它通过对预测值加上高斯噪声模拟新的响应：
\)</span><span class="math notranslate nohighlight">\(
y_i^*=\hat\mu(x_i)+\varepsilon_i^*;\qquad \varepsilon_i^*\sim N(0,\hat\sigma^2);\qquad i=1,2,\ldots,N\tag{8.6}
\)</span><span class="math notranslate nohighlight">\(
这一过程重复 \)</span>B<span class="math notranslate nohighlight">\( 次，这里 \)</span>B=200<span class="math notranslate nohighlight">\(。得到的自助法数据集形式为 \)</span>(x_1,y_1^*),\ldots,(x_N,y_N^*)<span class="math notranslate nohighlight">\(，而且我们可以重新对每一个计算 \)</span>B<span class="math notranslate nohighlight">\(-样条光滑。这种方法得到的置信域实际上等于当自助法样本数趋于无穷时右上图的最小二乘置信域。自助法样本 \)</span>\mathbf y^*<span class="math notranslate nohighlight">\( 的函数估计由 \)</span>\hat\mu^*(x)=h(x)^T\mathbf{(H^TH)^{-1}H^Ty^*}<span class="math notranslate nohighlight">\( 给出，而且服从分布
\)</span><span class="math notranslate nohighlight">\(
\hat\mu^*(x)\sim N(\hat \mu(x), h(x)^T\mathbf{(H^TH)^{-1}}h(x)\hat\sigma^2)\tag{8.7}
\)</span>$
注意到该分布的均值是最小二乘估计，标准误差和近似公式 \eqref{8.4} 相同。</p>
</div>
<div class="section" id="id3">
<h2>最大似然推断<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>结果证明在前面的例子中参数自助法和最小二乘是一致的，因为模型 \eqref{8.5} 有可加的高斯误差。一般地，参数自助法跟最小二乘不一致，但跟最大似然一致，我们现在再检验一下。</p>
<p>首先确定我们观测值的概率密度或者概率质量函数
$<span class="math notranslate nohighlight">\(
z_i\sim g_\theta(z)\tag{8.8}
\)</span><span class="math notranslate nohighlight">\(
这个表达式中 \)</span>\theta<span class="math notranslate nohighlight">\( 表示一个或多个未知的决定 \)</span>Z<span class="math notranslate nohighlight">\( 分布的参数。这称为 \)</span>Z<span class="math notranslate nohighlight">\( 的参数模型。举个例子，如果 \)</span>Z<span class="math notranslate nohighlight">\( 服从均值为 \)</span>\mu<span class="math notranslate nohighlight">\(、方差为 \)</span>\sigma^2<span class="math notranslate nohighlight">\( 的正态分布，则
\)</span><span class="math notranslate nohighlight">\(
\theta=(\mu, \sigma^2)\tag{8.9}
\)</span><span class="math notranslate nohighlight">\(
和
\)</span><span class="math notranslate nohighlight">\(
g_\theta(z)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(z-\mu)^2/\sigma^2}\tag{8.10}
\)</span><span class="math notranslate nohighlight">\(
最大似然基于似然函数，似然函数由下式给出
\)</span><span class="math notranslate nohighlight">\(
L(\theta;\mathbf Z)=\prod\limits_{i=1}^Ng_\theta(z_i)\tag{8.11}
\)</span>$</p>
<p>这也是观测数据在模型 <span class="math notranslate nohighlight">\(g_\theta\)</span> 下的概率。这个似然是在忽略正乘数情形下定义的，这里我们取为 1。我们将 <span class="math notranslate nohighlight">\(L(\theta;\Z)\)</span> 看成当 <span class="math notranslate nohighlight">\(\Z\)</span> 固定时 <span class="math notranslate nohighlight">\(\theta\)</span> 的函数。</p>
<p>记 <span class="math notranslate nohighlight">\(L(\theta;\mathbf Z)\)</span> 的对数为
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-398de1ef-5d3f-4024-a050-7e98d2d2846d">
<span class="eqno">(19)<a class="headerlink" href="#equation-398de1ef-5d3f-4024-a050-7e98d2d2846d" title="Permalink to this equation">¶</a></span>\[\begin{align}
\ell(\theta;\mathbf Z)&amp;=\sum\limits_{i=1}^N\ell(\theta;z_i)\notag\\
&amp;=\sum\limits_{i=1}^N\log g_\theta(z_i)\qquad \tag{8.12}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
有时简记为 $\ell(\theta)$. 该表达式称为对数似然，并且每个值 $\ell(\theta;z_i)=\mathbb{log} g_\theta(z_i)$ 称为对数似然组分。最大似然法选择 $\theta=\hat\theta$ 来最大化 $\ell(\theta;\mathbf Z)$.\\似然函数可以被用来评估 $\hat\theta$ 的准确性。我们需要更多的定义，**得分函数 (score function)** 由下式给出
\end{aligned}\end{align} \]</div>
<p>\dot\ell(\theta;\mathbf Z)=\sum\limits_{i=1}^N\dot \ell(\theta;z_i)\tag{8.13}
$<span class="math notranslate nohighlight">\(
其中 \)</span>\ell(\theta;z_i)=\partial \ell(\theta;z_i)/\partial \theta<span class="math notranslate nohighlight">\(。假设概率在参数空间的内部取得最大值，则 \)</span>\dot\ell(\hat\theta,\mathbf Z)=0<span class="math notranslate nohighlight">\(。信息矩阵为
\)</span><span class="math notranslate nohighlight">\(
\mathbf I(\theta)=-\sum\limits_{i=1}^N\frac{\partial ^2\ell(\theta;z_i)}{\partial \theta\partial \theta^T}\tag{8.14}
\)</span><span class="math notranslate nohighlight">\(
当 \)</span>\mathbf I(\theta)<span class="math notranslate nohighlight">\( 在 \)</span>\theta=\hat\theta<span class="math notranslate nohighlight">\( 处取值，则通常称为观测信息量。Fisher 信息量（或者期望信息量）是
\)</span><span class="math notranslate nohighlight">\(
\mathbf i(\theta)=E_\theta[\mathbf I(\theta)]\tag{8.15}
\)</span>$</p>
<!--**weiya**注:
> 设随机变（向）量$X$来自分布族$\cal F=\{p(x;\theta):\theta\in\Theta\}$,其中$p(x;\theta)$为其概率密度函数，$\Theta$为开区间，假设$p(x;\theta)$关于$\theta$可导，且
> $$
> \begin{align}
> 0&=\frac{d}{d\theta}\int_{-\infty}^\infty p(x;\theta)dx\\
> &=\int_{-\infty}^\infty \frac{\partial p(x;\theta)}{\partial \theta}dx\\
> &=\int_{-\infty}^\infty\frac{\partial \mathbb{log}p(x;\theta)}{\partial\theta}p(x;\theta)dx\\
> &=E_\theta[\frac{\partial \mathbb{log}p(X;\theta)}{\partial \theta}]
> \end{align}
> $$
> 从而
> $$
> \begin{align}
> I(\theta):&=\mathrm{Var}_\theta\{\frac{\partial \mathrm{log}p(X;\theta)}{\partial \theta}\}\\
> &=E_\theta[\frac{\partial\mathrm{log}p(x;\theta)}{\partial\theta}]^2\\
> &=\int_{-\infty}^\infty(\frac{\partial \mathrm{log}p(x;\theta)}{\partial \theta})^2p(x;\theta)dx
> \end{align}
> $$
> $I(\theta)$称为$X$或分布族$\cal F$的Fisher Information。
>
> 如果$\frac{\partial^2}{\partial\theta^2}p(x;\theta)$对任意的$\theta\in\Theta$都存在，且积分与求导可以交换，则
> $$
> \begin{align}
> 0&=\frac{d^2}{d\theta^2}\int_{-\infty}^\infty p(x;\theta)dx\\
> &=\int_{-\infty}^{\infty}\frac{\partial}{\partial \theta}[\frac{\partial \mathrm{log }p(x;\theta)}{\partial \theta}p(x;\theta)]dx\\
> &=\int_{-\infty}^\infty[\frac{\partial^2\mathrm{log}p(x;\theta)}{\partial \theta^2}p(x;\theta)]dx+\int_{-\infty}^\infty[\frac{\partial\mathrm{log} p(x;\theta)}{\partial \theta}]^2p(x;\theta)dx
> \end{align}
> $$
> 从而
> $$
> I(\theta)=-E_\theta[\frac{\partial^2\mathrm{log}p(X;\theta)}{\partial \theta^2}]
> $$
>
-->
<p>!!! note “weiya 注”
设随机变（向）量 <span class="math notranslate nohighlight">\(X\)</span> 来自分布族 <span class="math notranslate nohighlight">\(\cal F=\\{p(x;\theta):\theta\in\Theta\\}\)</span>，其中 <span class="math notranslate nohighlight">\(p(x;\theta)\)</span> 为其概率密度函数，<span class="math notranslate nohighlight">\(\Theta\)</span>为开区间，假设 <span class="math notranslate nohighlight">\(p(x;\theta)\)</span> 关于 <span class="math notranslate nohighlight">\(\theta\)</span> 可导，且
$<span class="math notranslate nohighlight">\(
    \begin{align*}
    0&amp;=\frac{d}{d\theta}\int_{-\infty}^\infty p(x;\theta)dx\\
    &amp;=\int_{-\infty}^\infty \frac{\partial p(x;\theta)}{\partial \theta}dx\\
    &amp;=\int_{-\infty}^\infty \frac{\partial \log p(x;\theta)}{\partial\theta}p(x;\theta)dx\\
    &amp;=E_\theta\left[\frac{\partial \log p(X;\theta)}{\partial \theta}\right]
    \end{align*}
    \)</span><span class="math notranslate nohighlight">\(
    从而
    \)</span><span class="math notranslate nohighlight">\(
    \begin{align*}
    I(\theta):&amp;=\Var_\theta\left\{\frac{\partial \log p(X;\theta)}{\partial \theta}\right\}\\
    &amp;=E_\theta\left[\frac{\partial\log p(x;\theta)}{\partial\theta}\right]^2\\
    &amp;=\int_{-\infty}^\infty\left(\frac{\partial \log p(x;\theta)}{\partial \theta}\right)^2p(x;\theta)dx
    \end{align*}
    \)</span><span class="math notranslate nohighlight">\(
    \)</span>I(\theta)<span class="math notranslate nohighlight">\( 称为 \)</span>X<span class="math notranslate nohighlight">\( 或分布族 \)</span>\cal F$ 的 Fisher Information。</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>如果 $\frac{\partial^2}{\partial\theta^2}p(x;\theta)$ 对任意的 $\theta\in\Theta$ 都存在，且积分与求导可以交换，则
$$
\begin{align*}
0&amp;=\frac{d^2}{d\theta^2}\int_{-\infty}^\infty p(x;\theta)dx\\
&amp;=\int_{-\infty}^{\infty}\frac{\partial}{\partial \theta}\left[\frac{\partial \log p(x;\theta)}{\partial \theta}p(x;\theta)\right]dx\\
&amp;=\int_{-\infty}^\infty\left[\frac{\partial^2\log p(x;\theta)}{\partial \theta^2}p(x;\theta)\right]dx + \int_{-\infty}^\infty\left[\frac{\partial\log p(x;\theta)}{\partial \theta}\right]^2p(x;\theta)dx
\end{align*}
$$
从而
$$
I(\theta)=-E_\theta\left[\frac{\partial^2\log p(X;\theta)}{\partial \theta^2}\right]\,.
$$
</pre></div>
</div>
<p>最后，记 <span class="math notranslate nohighlight">\(\theta\)</span> 的真值为 <span class="math notranslate nohighlight">\(\theta_0\)</span>.</p>
<p>一个标准的结果表明最大似然估计量的样本分布渐近服从正态分布
$<span class="math notranslate nohighlight">\(
\hat\theta\longrightarrow N(\theta_0,\mathbf i(\theta_0)^{-1})\tag{8.16}
\)</span><span class="math notranslate nohighlight">\(
当 \)</span>N\longrightarrow \infty<span class="math notranslate nohighlight">\(。这表明 \)</span>\hat\theta<span class="math notranslate nohighlight">\( 的样本分布可能近似服从
\)</span><span class="math notranslate nohighlight">\(
N(\hat\theta,\mathbf i(\hat\theta)^{-1})\text{   or   }N(\hat{\theta},\mathbf I(\hat\theta)^{-1})\tag{8.17}\label{8.17}
\)</span><span class="math notranslate nohighlight">\(
其中，\)</span>\hat\theta$ 表示由观测数据得到的最大似然估计。</p>
<p><span class="math notranslate nohighlight">\(\hat\theta_j\)</span> 的标准误差对应的估计由下式给出
$<span class="math notranslate nohighlight">\(
\sqrt{\mathbf i(\hat\theta)_{jj}^{-1}}\text{ and }\sqrt{\mathbf I(\hat\theta)^{-1}_{jj}}\tag{8.18}
\)</span><span class="math notranslate nohighlight">\(
\)</span>\theta_j<span class="math notranslate nohighlight">\( 的置信点可以通过 \eqref{8.17} 的任意一个近似得到。这样的置信点分别有如下形式
\)</span><span class="math notranslate nohighlight">\(
\hat\theta_j-z^{(1-\alpha)}\cdot\sqrt{\mathbf i(\hat\theta)_{jj}^{-1}}\text{ or }\hat\theta_j-z^{(1-\alpha)}\cdot\sqrt{\mathbf I(\hat\theta)_{jj}^{-1}}
\)</span><span class="math notranslate nohighlight">\(
其中 \)</span>z^{(1-\alpha)}<span class="math notranslate nohighlight">\( 为标准正态分布的 \)</span>1-\alpha<span class="math notranslate nohighlight">\( 分位数。更精确的置信区间可以由似然函数得到，通过利用卡方分布的近似
\)</span><span class="math notranslate nohighlight">\(
2[\ell(\hat\theta)-\ell(\theta_0)]\sim \chi_p^2\tag{8.19}
\)</span><span class="math notranslate nohighlight">\(
其中 \)</span>p<span class="math notranslate nohighlight">\( 为 \)</span>\theta<span class="math notranslate nohighlight">\( 组分的个数。\)</span>1-2\alpha<span class="math notranslate nohighlight">\( 置信区间是所有满足 \)</span>2[\ell(\hat\theta)-\ell(\theta_0)]\le {\chi_p^2}^{(1-2\alpha)}<span class="math notranslate nohighlight">\( 的集合，其中，\)</span>{\chi_p^2}^{(1-2\alpha)}<span class="math notranslate nohighlight">\( 是自由度为 \)</span>p<span class="math notranslate nohighlight">\( 的卡方分布的 \)</span>1-2\alpha$ 分位数。</p>
<p>回到光滑化的例子看看最大似然法的结果。参数为 <span class="math notranslate nohighlight">\(\theta=(\beta,\sigma^2)\)</span>。对数似然为
$<span class="math notranslate nohighlight">\(
\ell(\theta)=-\frac{N}{2}\mathbb{log}\sigma^22\pi-\frac{1}{2\sigma^2}\sum\limits_{i=1}^N(y_i-h(x_i)^T\beta)^2\tag{8.20}
\)</span><span class="math notranslate nohighlight">\(
最大似然估计由 \)</span>\partial \ell/\partial \beta=0<span class="math notranslate nohighlight">\( 和 \)</span>\partial \ell/\partial \sigma^2<span class="math notranslate nohighlight">\( 给出
\)</span>$</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat\beta&amp;=\mathbf{(H^TH)^{-1}H^Ty}\\
\hat\sigma&amp;=\frac{1}{N}\sum{(y_i-\hat\mu(x_i))^2}
\end{align*}\]</div>
<p>\tag{8.21}
$$
这与 \eqref{8.2} 给出的一般估计和 \eqref{8.3} 是一样的。</p>
<p>对于 <span class="math notranslate nohighlight">\(\theta=(\beta,\sigma^2)\)</span> 的信息矩阵是分块对角，对应于 <span class="math notranslate nohighlight">\(\beta\)</span> 的对角元为
$<span class="math notranslate nohighlight">\(
\mathbf I(\beta)=\mathbf{(H^TH)}/\sigma^2\tag{8.22}
\)</span><span class="math notranslate nohighlight">\(
因此估计的方差 \)</span>\mathbf{(H^TH)}^{-1}\hat\sigma^2$ 与最小二乘一致。</p>
</div>
<div class="section" id="id4">
<h2>自助法和最大似然法<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>本质上自助法是非参最大似然或者参数最大似然法的计算机实现。</p>
<p>!!! note “weiya 注：nonparametric MLE”
<a class="reference external" href="https://www.ctanujit.org/uploads/2/5/3/9/25393293/_theory_of_point_estimation.pdf">Theory of Point Estimation (2nd Eds) p519</a> 中也写道：</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&gt; The bootstrap can be thought of as a “nonparametric” MLE.
</pre></div>
</div>
<p>与最大似然法相比自助法的好处是允许我们在没有公式的情况下计算标准误差和其他一些量的最大似然估计。</p>
<p>在我们的例子中，假设我们通过对定义了 B 样条的结点的数目和位置进行交叉验证来自适应地选择，而不是事先固定住。记 <span class="math notranslate nohighlight">\(\lambda\)</span> 为结点和它们位置的集合。则标准误差和置信域说明了 <span class="math notranslate nohighlight">\(\lambda\)</span> 的自适应选择，但是没有分析的方法可以实现。有了自助法，我们通过对每个自助法样本进行自适应选择来计算B-样条光滑。最终曲线的分位数既捕捉目标里面的噪声变化以及 <span class="math notranslate nohighlight">\(\hat\lambda\)</span> 的变化。在这个例子中置信域（没有显示出来）看起来与固定 <span class="math notranslate nohighlight">\(\lambda\)</span> 后的置信域没有太大不同。但是在其他问题里面，当使用更多的自适应，可以得到更多重要的结果。</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./08-Model-Inference-and-Averaging"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="8.1-Introduction.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">8.1 导言</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="8.3-Bayesian-Methods.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">8.3 贝叶斯方法</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>