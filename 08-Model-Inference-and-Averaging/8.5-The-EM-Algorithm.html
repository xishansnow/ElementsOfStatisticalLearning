
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8.5 EM 算法 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8.6 从后验分布采样的 MCMC" href="8.6-MCMC-for-Sampling-from-the-Posterior.html" />
    <link rel="prev" title="8.3 贝叶斯方法" href="8.3-Bayesian-Methods.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.3-Subset-Selection.html">
     3.3 子集的选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 光滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 光滑参数的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波光滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核光滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差、方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的乐观估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 “条件测试误差”还是“测试误差的期望”？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     8.5 EM 算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆栈
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14-Unsupervised-Learning/14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   两个组分的混合模型
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   广义 EM 算法
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   EM 作为一个最大化-最大化的过程
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="em">
<h1>8.5 EM 算法<a class="headerlink" href="#em" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<style>p{text-indent:2em;2}</style>
<p>EM 算法是简化复杂极大似然问题的一种很受欢迎的工具。我们首先在一个简单的混合模型中讨论它。</p>
<div class="section" id="id1">
<h2>两个组分的混合模型<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>这一节我们描述一个密度估计的简单混合模型，以及对应的求解极大似然估计的 EM 算法。这与贝叶斯推断中的 Gibbs 采样方法有着本质的联系。混合模型在本书其他部分的章节有讨论，特别是 <span class="xref myst">6.8</span>，<span class="xref myst">12.7</span> 和 <span class="xref myst">13.2.3 节</span>。</p>
<div class="admonition-gibbs-sampling admonition">
<p class="admonition-title">Gibbs sampling</p>
<p>假设我们需要从 <span class="math notranslate nohighlight">\(\mathbf X=(x_1,x_2,\ldots,x_n)\)</span> 中得到 <span class="math notranslate nohighlight">\(k\)</span> 个样本，联合分布为 <span class="math notranslate nohighlight">\(p(x_1,x_2,\ldots,x_n)\)</span>。</p>
<p>记第 <span class="math notranslate nohighlight">\(i\)</span> 个样本为 <span class="math notranslate nohighlight">\(\mathbf X^{(i)}=(x_1^{(i)},\ldots,x_n^{(i)})\)</span>。我们按下列步骤进行：
<br><br></p>
<ol class="simple">
<li><p>以初始值 <span class="math notranslate nohighlight">\(X^{(i)}\)</span> 开始</p></li>
<li><p>需要下一个样本，记为 <span class="math notranslate nohighlight">\(X^{(i+1)}\)</span>。因为 <span class="math notranslate nohighlight">\(\mathbf X^{(i+1)}=(x_1^{(i+1)},\ldots,x_n^{(i+1)})\)</span> 是向量，我们需要对向量的每一个组分进行抽样，基于 <span class="math notranslate nohighlight">\(p(x_j^{(i+1)}\mid x_1^{(i+1)},\ldots,x_{j-1}^{(i+1)},x_{j+1}^{(i)},\ldots,x_n^{(i)})\)</span> 的分布对 <span class="math notranslate nohighlight">\(x_j^{(i+1)}\)</span> 抽样.</p></li>
<li><p>重复上述步骤 <span class="math notranslate nohighlight">\(k\)</span> 次。</p></li>
</ol>
</div>
<p>图 8.5 的左图显示了表 8.1 中的 20 个模拟数据的直方图。</p>
<p><img alt="" src="../_images/fig8.5.png" /></p>
<blockquote>
<div><p>图 8.5. 混合模型的例子。（左图：）数据的直方图。（右图：）高斯密度的最大似然拟合（红色实线）和观测值 <span class="math notranslate nohighlight">\(y\)</span> 的左边成分的解释度（绿色点线）作为 <span class="math notranslate nohighlight">\(y\)</span> 的函数。</p>
</div></blockquote>
<p><img alt="" src="../_images/tab8.1.png" /></p>
<blockquote>
<div><p>表 8.1. 图 8.5 中两个组分混合的例子中使用的 20 个模拟数据。</p>
</div></blockquote>
<p>我们想要建立数据点的密度模型，然后由于数据点呈现明显的双峰，高斯分布不是合适的选择。这里似乎有两个潜在的分开的形式，所以我们将 <span class="math notranslate nohighlight">\(Y\)</span> 看成两个正态分布混合的模型：</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Y_1&amp;\sim N(\mu_1,\sigma^2_1)\\
Y_2&amp;\sim N(\mu_2,\sigma_2^2)\qquad \qquad \qquad\tag{8.36}\\
Y&amp;=(1-\Delta)\cdot Y_1 + \Delta\cdot Y_2
\end{align*}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\Delta\in \{0,1\}\)</span>，且 <span class="math notranslate nohighlight">\(\mathrm{Pr}(\Delta =1)=\pi\)</span>。产生过程是很显然的：以概率 <span class="math notranslate nohighlight">\(\pi\)</span> 产生 <span class="math notranslate nohighlight">\(\Delta\in\{0,1\}\)</span>，然后根据输出结果，分配给 <span class="math notranslate nohighlight">\(Y_1\)</span> 或 <span class="math notranslate nohighlight">\(Y_2\)</span>。令 <span class="math notranslate nohighlight">\(\phi_{\theta}(x)\)</span> 记为参数为<span class="math notranslate nohighlight">\(\theta=(\mu,\sigma^2)\)</span> 的正态分布。则 <span class="math notranslate nohighlight">\(Y\)</span> 的密度为</p>
<div class="math notranslate nohighlight">
\[
g_Y(y)=(1-\pi)\phi_{\theta_1}(y)+\pi\phi_{\theta_2}(y)\tag{8.37}
\]</div>
<p>现在假设我们希望通过极大似然估计来拟合图 8.5 中数据的模型。参数为</p>
<div class="math notranslate nohighlight">
\[
\theta=(\pi,\theta_1,\theta_2)=(\pi,\mu_1,\sigma^2_1,\mu_2,\sigma_2^2)\tag{8.38}
\]</div>
<p>基于 <span class="math notranslate nohighlight">\(N\)</span> 个训练集的对数概率为</p>
<div class="math notranslate nohighlight">
\[
\ell(\theta;\mathbf Z)=\sum\limits_{i=1}^N\log[(1-\pi)\phi_{\theta_1}(y_i)+\pi\phi_{\theta_2}(y_i)]\tag{8.39}
\]</div>
<p>直接对 <span class="math notranslate nohighlight">\(\ell(\theta;\mathbf Z)\)</span> 进行最大化在数值上是很困难的，因为求和项在 <span class="math notranslate nohighlight">\(\log\)</span> 函数里面。然而，这里有一个更简单的方式。我们考虑一个类似 式（ 8.36 ） 中取 0 或 1 的潜变量 <span class="math notranslate nohighlight">\(\Delta_i\)</span>：若 <span class="math notranslate nohighlight">\(\Delta_i=1\)</span> 则 <span class="math notranslate nohighlight">\(Y_i\)</span> 来自模型 2，否则来自模型 1。假设我们已经知道了 <span class="math notranslate nohighlight">\(\Delta_i\)</span> 的值。则对数概率为</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\ell_0(\theta;\bf Z,\bf{\Delta})&amp;=\sum\limits_{i=1}^N[(1-\Delta_i)\log\phi_{\theta_1}(y_i)+\Delta_i\log\phi_{\theta_2}(y_i)]\\
&amp;+\sum\limits_{i=1}^N[(1-\Delta_i)\log(1-\pi)+\Delta_i\log\pi]\tag{8.40} 
\end{align*}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="math notranslate nohighlight">
\[
L(\theta,\mathbf{Z,\Delta}) = \prod\limits_{i=1}^n[(1-\pi)\phi_{\theta_1}(y_i)]^{1-\Delta_i}[\pi\phi_{\theta_2}(y_i)]^{\Delta_i}.
\]</div>
<!-- 假设我们已经知道 $\Delta_i$ 的值，设 $\Delta_i=0,i\in \mathcal I$，且$ \Delta_i=1,i\in\mathcal J$。注意到 $p(\Delta_i=0)=1-\pi;p(\Delta_i=1)=\pi$，则似然函数为
$$
L(\theta,\mathbf{Z,\Delta}) = \prod\limits_{i\in \mathcal I} (1-\Delta_i)(1-\pi)\phi_{\theta_1}(y_i)\prod\limits_{i\in \mathcal J}\Delta_i\pi\phi_{\theta_2}(y_i)
$$
上式 $i\in \mathcal I$ 部分，乘以 $(1-\Delta_i)$ 也就相当于乘以 1，同理对于 $i\in \mathcal J$。这样取对数似然便有 (8.40) 式。 -->
</div>
<p>而且 <span class="math notranslate nohighlight">\(\mu_1\)</span> 和 <span class="math notranslate nohighlight">\(\sigma_1^2\)</span> 的极大似然估计为 <span class="math notranslate nohighlight">\(\Delta_i=0\)</span> 时样本均值和方差，类似地对于 <span class="math notranslate nohighlight">\(\mu_2\)</span> 和 <span class="math notranslate nohighlight">\(\sigma_2^2\)</span> 的极大似然估计为 <span class="math notranslate nohighlight">\(\Delta_i=1\)</span> 时的样本均值和方差。<span class="math notranslate nohighlight">\(\pi\)</span> 的估计为 <span class="math notranslate nohighlight">\(\Delta_i=1\)</span> 的比例。</p>
<p>因为 <span class="math notranslate nohighlight">\(\Delta_i\)</span> 的值实际上是不知道的，我们用一种迭代方式，替换 式（ 8.40 ） 中的每个 <span class="math notranslate nohighlight">\(\Delta_i\)</span>，它的期望值</p>
<div class="math notranslate nohighlight">
\[
\gamma_i(\theta)=\mathbb{E}(\Delta_i\mid\theta,\mathbf Z)=\mathrm{Pr}(\Delta_i=1\mid \theta,\mathbf Z)\tag{8.41}
\]</div>
<p>也称为模型 2 对于每个观测 <span class="math notranslate nohighlight">\(i\)</span> 的责任 (responsibility)。我们用一种称作 EM 算法（算法 8.1 中给出）的过程来求解这个特殊的高斯混合模型。在期望 (expectation) 这一步，我们对每一个模型的每一个观测做一个软赋值：根据每个模型下训练集点的相对密度，参数的当前估计用来给 responsibilities 赋值。在最大化(maximization) 那一步，对极大似然估计中使用的 responsibilities 进行加权用来更新参数估计。</p>
<p>构造初始的 <span class="math notranslate nohighlight">\(\hat\mu_1\)</span> 和 <span class="math notranslate nohighlight">\(\hat\mu_2\)</span> 的一种很好的方式便是简单地随机选择 <span class="math notranslate nohighlight">\(y_i\)</span> 中的两个值。<span class="math notranslate nohighlight">\(\hat\sigma^2_1\)</span> 和 <span class="math notranslate nohighlight">\(\hat\sigma^2_2\)</span> 都等于整体的样本方差 <span class="math notranslate nohighlight">\(\sum_{i=1}^N(y_i-\bar y)^2/N\)</span>。最大比例的 <span class="math notranslate nohighlight">\(\hat\pi\)</span> 可以从 0.5 开始。</p>
<p>注意到实际中概率的最大值发生在当我们固定一个数据点，换句话说，对于一些 <span class="math notranslate nohighlight">\(i\)</span> 令 <span class="math notranslate nohighlight">\(\hat\mu_1=y_i\)</span>，<span class="math notranslate nohighlight">\(\hat\sigma^2_1=0\)</span>。这给出了无限大的概率，但是这不是一个有用的解。因此实际上我们寻找概率的一个良好的局部最大值，满足 <span class="math notranslate nohighlight">\(\hat\sigma^2_1,\hat\sigma^2_2&gt;0\)</span>。进一步，可以有多个局部最大值满足<span class="math notranslate nohighlight">\(\hat\sigma^2_1,\hat\sigma^2_2&gt;0\)</span>。在我们例子中，我们用一系列不同的初始参数值来运行 EM 算法，所有的都满足 <span class="math notranslate nohighlight">\(\hat\sigma^2_k&gt;0.5\)</span>，然后选择使得概率最大的那个。图 8.6 显示了在最大化对数概率的 EM 算法的过程。表 8.2 显示了在给定迭代次数的 EM 过程下 <span class="math notranslate nohighlight">\(\hat\pi=\sum_i\hat\gamma_i/N\)</span> 是类别 2 中观测值比例的极大似然估计。</p>
<p><img alt="" src="../_images/alg8.1.png" /></p>
<p><img alt="" src="../_images/tab8.2.png" /></p>
<blockquote>
<div><p>表 8.2. 对于混合模型选定的几次迭代的 EM 算法结果</p>
</div></blockquote>
<p><img alt="" src="../_images/fig8.6.png" /></p>
<blockquote>
<div><p>图 8.6. EM算法：观测数据的对数似然关于迭代次数的函数</p>
</div></blockquote>
<p>最后的极大似然估计为</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat\mu_1=4.62\qquad \sigma^2_1=0.87\\
\hat\mu_2=1.06\qquad \sigma^2_2=0.77\\
\hat\pi=0.546
\end{split}\]</div>
<p>图 8.5 的右图显示了从这个过程估计的混合高斯分布的密度（实心红色曲线），以及 responsibilities（绿色点曲线）。注意到混合在监督学习中也很有用；在 <span class="xref myst">6.7 节</span>我们显示了高斯混合模型怎样导出 radial 基函数的版本。</p>
</div>
<div class="section" id="id2">
<h2>广义 EM 算法<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>上面的过程是对于特定问题的类别下最大化概率的 EM（或者 Baum-Welch）算法。这些问题的概率最大化是困难的，但是通过运用潜在数据（未观测）增大样本会变得简单。这也称作数据增广。这里潜在数据是模型成员 <span class="math notranslate nohighlight">\(\Delta_i\)</span>。在其它问题中，潜在数据是理应被观测到的实际数据但是缺失了。</p>
<p>算法 8.2 给出了 EM 算法的一般形式。我们的观测数据是 <span class="math notranslate nohighlight">\(\mathbf Z\)</span>，其对数概率 <span class="math notranslate nohighlight">\(\ell(\theta;\mathbf Z)\)</span> 取决于参数 <span class="math notranslate nohighlight">\(\theta\)</span>。潜在数据或者缺失数据为 <span class="math notranslate nohighlight">\(\mathbf Z^m\)</span>，因此完整数据为 <span class="math notranslate nohighlight">\(\mathbf {T=(Z,Z^m)}\)</span>，对数似然函数为 <span class="math notranslate nohighlight">\(\ell_0(\theta;\mathbf T)\)</span>，<span class="math notranslate nohighlight">\(\ell_0\)</span> 基于完整的密度函数。在混合问题中，<span class="math notranslate nohighlight">\((\mathbf{Z,Z^m)=(y,}\Delta)\)</span>，且 <span class="math notranslate nohighlight">\(\ell_0(\theta;\mathbf T)\)</span> 由 式（ 8.40 ） 式给出。</p>
<p><img alt="" src="../_images/alg8.2.png" /></p>
<p>在我们的混合例子中，<span class="math notranslate nohighlight">\(\mathbb{E}(\ell_0(\theta';\mathbf T)\mid \mathbf Z,\hat \theta^{(j)})\)</span> 是将式 式（ 8.40 ） 中的 <span class="math notranslate nohighlight">\(\Delta_i\)</span> 替换成了解释度 <span class="math notranslate nohighlight">\(\hat\gamma_i(\hat \theta)\)</span>。第三步的最大化仅仅是加权均值和方差。</p>
<p>我们现在给出一个为什么一般情况下 EM 算法有用的解释。</p>
<p>因为</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Pr}(\mathbf Z^m\mid \mathbf Z,\theta')=\frac{\mathrm{Pr}(\mathbf Z^m,\mathbf Z\mid \theta')}{\mathrm{Pr}(\mathbf Z\mid \theta')}\tag{8.44}
\]</div>
<p>我们可以写成</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Pr}(\mathbf Z\mid \theta')=\frac{\mathrm{Pr}(\mathbf T\mid \theta')}{\mathrm{Pr}(\mathbf Z^m\mid \mathbf Z,\theta')}\tag{8.45}
\]</div>
<p>表示成对数似然函数，我们有 <span class="math notranslate nohighlight">\(\ell(\theta';\mathbf Z)=\ell_0(\theta';\mathbf T)-\ell_1(\theta';\mathbf{Z^m\mid Z})\)</span>，其中 <span class="math notranslate nohighlight">\(\ell_1\)</span> 是基于条件密度 <span class="math notranslate nohighlight">\(\mathrm{Pr}(\mathbf{Z^m\mid Z,\theta'})\)</span>。取关于由参数 <span class="math notranslate nohighlight">\(\theta\)</span> 确定的 <span class="math notranslate nohighlight">\(\mathbf{T\mid Z}\)</span> 分布的条件期望有</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\ell(\theta';\mathbf Z)&amp;=\mathbb{E}[\ell_0(\theta';\mathbf T)\mid \mathbf Z,\theta]-\mathbb{E}[\ell_1(\theta';\mathbf{Z^m\mid Z)\mid Z,\theta}]\\
&amp;\equiv Q(\theta',\theta)-R(\theta',\theta)\qquad\tag{8.46} 
\end{align*}\]</div>
<p>在最大化那一步，EM 算法最大化关于 <span class="math notranslate nohighlight">\(\theta'\)</span> 的 <span class="math notranslate nohighlight">\(Q(\theta',\theta)\)</span>，而不是实际的目标函数 <span class="math notranslate nohighlight">\(\ell(\theta';\mathbf Z)\)</span>。为什么这样能成功地最大化 <span class="math notranslate nohighlight">\(\ell(\theta';\mathbf Z)\)</span>？注意到 <span class="math notranslate nohighlight">\(R(\theta^*,\theta)\)</span> 是关于 <span class="math notranslate nohighlight">\(\theta^*\)</span> 的对数密度的期望，得到的密度是关于 <span class="math notranslate nohighlight">\(\theta\)</span> 的，因此（由琴生不等式）当 <span class="math notranslate nohighlight">\(\theta^*=\theta\)</span> 时（见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/125">练习 8.1</a>）最大化关于 <span class="math notranslate nohighlight">\(\theta^*\)</span> 的函数。</p>
<p>所以如果 <span class="math notranslate nohighlight">\(\theta'\)</span> 最大化 <span class="math notranslate nohighlight">\(Q(\theta',\theta)\)</span>，我们可以看到</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\ell(\theta';\mathbf Z) - \ell(\theta;\mathbf Z) &amp;= [Q(\theta',\theta)-Q(\theta,\theta)]-[R(\theta',\theta)-R(\theta,\theta)]\\
&amp;\ge 0\qquad\qquad\qquad \tag{8.47}
\end{align*}\]</div>
<p>因此 EM 迭代不会降低对数似然值。</p>
<p>这个论据也让我们明白在最大化那一步整体最大化不是必要的：我们仅仅需要找到一个值 <span class="math notranslate nohighlight">\(\hat\theta^{(j+1)}\)</span> 使得 <span class="math notranslate nohighlight">\(Q(\theta',\hat\theta^{(j)})\)</span> 关于第一个变量是增的，也就是 <span class="math notranslate nohighlight">\(Q(\hat\theta^{(j+1)},\hat\theta^{(j)}) &gt; Q(\hat\theta^{(j)},\hat\theta^{(j)})\)</span>。这一过程称之为 GEM（广义 EM）算法。EM 算法也可以看成是最小化的过程：见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/126">练习 8.7</a>。</p>
</div>
<div class="section" id="id3">
<h2>EM 作为一个最大化-最大化的过程<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>这里从一个不同的角度来看 EM 过程，看成一个 <strong>联合最大化 (joint maximization)</strong> 算法。考虑函数</p>
<div class="math notranslate nohighlight">
\[
F(\theta',\tilde P) = \mathbb{E}_{\tilde P}[\ell_0(\theta';\mathbf T)] - \mathbb{E}_{\tilde P}[\log\tilde P(\mathbf Z^m)]\tag{8.48}
\]</div>
<p>这里 <span class="math notranslate nohighlight">\(\tilde P(\mathbf Z^m)\)</span> 是潜在数据 <span class="math notranslate nohighlight">\(\mathbf Z^m\)</span> 的任意分布。在混合例子中，<span class="math notranslate nohighlight">\(\tilde P(\mathbf Z^m)\)</span> 构成了概率 <span class="math notranslate nohighlight">\(\gamma_i=\mathrm{Pr}(\Delta_i=1\mid \theta,\mathbf Z)\)</span> 的集合。注意到从 式（ 8.46 ） 式看，<span class="math notranslate nohighlight">\(F\)</span> 是观测数据的对数似然函数（在 <span class="math notranslate nohighlight">\(\tilde P(\mathbf Z^m)=\mathrm{Pr}(\mathbf Z^m\mid \mathbf Z,\theta')\)</span> 上取值）。函数 <span class="math notranslate nohighlight">\(F\)</span> 扩大了对数似然的定义域来，使得能够进行最大化。</p>
<p>EM 算法可以看成 <span class="math notranslate nohighlight">\(F\)</span> 关于 <span class="math notranslate nohighlight">\(\theta'\)</span> 和 <span class="math notranslate nohighlight">\(\tilde P(\mathbf Z^m)\)</span> 的联合最大化，通过固定一个变量来最大化另外一个变量。固定 <span class="math notranslate nohighlight">\(\theta'\)</span> 来对 <span class="math notranslate nohighlight">\(\tilde P(\mathbf Z^m)\)</span> 最大化可以证明是</p>
<div class="math notranslate nohighlight">
\[
\tilde P(\mathbf Z^m) = \mathrm{Pr}(\mathbf Z^m\mid \mathbf Z,\theta')\tag{8.49}
\]</div>
<p>这是在求期望的步骤 E 计算得到的分布，举个例子，如在混合的例子中计算得到的 <span class="math notranslate nohighlight">\((8.42)\)</span>。在 M 步骤时，我们固定 <span class="math notranslate nohighlight">\(\tilde P\)</span> 来对 <span class="math notranslate nohighlight">\(\theta'\)</span> 最大化 <span class="math notranslate nohighlight">\(F(\theta',\tilde P)\)</span>：因为第二项不涉及 <span class="math notranslate nohighlight">\(\theta'\)</span>，所以这与最大化第一项 <span class="math notranslate nohighlight">\(\mathbb{E}_{\tilde P}[\ell_0(\theta';\mathbf T)\mid \mathbf Z,\theta]\)</span> 是一样的。</p>
<p>最后，因为当 <span class="math notranslate nohighlight">\(\tilde P(\mathbf Z^m)=\mathrm{Pr}(\mathbf Z^m\mid \mathbf Z,\theta')\)</span> 时，<span class="math notranslate nohighlight">\(F(\theta',\tilde P)\)</span> 和观测数据的对数似然函数是一致的，对前者的最大化也实现了对后者的最大化。图 8.7 展现了这一过程的示意图。</p>
<p><img alt="" src="../_images/fig8.7.png" /></p>
<blockquote>
<div><p>图 8.7. EM 算法的最大化-最大化角度。图中画出了（增广）观测数据对数似然函数 <span class="math notranslate nohighlight">\(F(\theta',\tilde P)\)</span> 的等高线。步骤 E 等价于在潜在数据分布的参数上最大化对数似然函数。步骤 M 在对数似然参数上进行最大化。红色曲线对应观测数据的对数似然函数，这是对每个 <span class="math notranslate nohighlight">\(\theta'\)</span> 值进行最大化 <span class="math notranslate nohighlight">\(F(\theta',\tilde P)\)</span> 得到的曲线。</p>
</div></blockquote>
<p>EM 算法的这个角度导出了 <strong>轮换最大化过程 (alternative maximization procedure)</strong>。举个例子，虽然不需要一次性对所有潜在数据参数进行最大化，但是可以每次最大化其中的一个，通过在步骤 M 来 <strong>轮换 (alternate)</strong>。</p>
<p>!!! note “weiya 注”
有点类似于 <strong>坐标轮换 (univariate search)</strong>，关于坐标轮换及其其他优化方法的介绍，可以参见<a class="reference external" href="https://sites.ualberta.ca/~aksikas/nlpm.pdf">nlpm</a>。</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./08-Model-Inference-and-Averaging"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="8.3-Bayesian-Methods.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">8.3 贝叶斯方法</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="8.6-MCMC-for-Sampling-from-the-Posterior.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">8.6 从后验分布采样的 MCMC</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>