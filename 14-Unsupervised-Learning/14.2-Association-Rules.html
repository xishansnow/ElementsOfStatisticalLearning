
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>14.2 关联规则 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="14.3 聚类分析" href="14.3-Cluster-Analysis.html" />
    <link rel="prev" title="14.1 导言" href="14.1-Introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.3-Subset-Selection.html">
     3.3 选择预测变量的子集
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 平滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 平滑参数
     <span class="math notranslate nohighlight">
      \(\lambda\)
     </span>
     的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波平滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核平滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     维空间中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     维空间中的结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差、方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的乐观估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 “条件测试误差”还是“测试误差的期望”？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5
     <code class="docutils literal notranslate">
      <span class="pre">
       EM
      </span>
      <span class="pre">
       算法
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆叠
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/14-Unsupervised-Learning/14.2-Association-Rules.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F14-Unsupervised-Learning/14.2-Association-Rules.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   （）市场篮子分析
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#apriori">
   （）Apriori 算法
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   （）广义关联规则
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   （）监督学习方法的选择
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   （）例子：市场篮子分析（继续）
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>14.2 关联规则</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   （）市场篮子分析
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#apriori">
   （）Apriori 算法
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   （）广义关联规则
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   （）监督学习方法的选择
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   （）例子：市场篮子分析（继续）
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>14.2 关联规则<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p><strong>关联规则分析 (Association rule analysis)</strong> 已经成为挖掘贸易数据的流行工具。目标是寻找变量 <span class="math notranslate nohighlight">\(X=(X_1,X_2,\ldots,X_p)\)</span> 在数据中出现最频繁的联合值。在二值数据 <span class="math notranslate nohighlight">\(X_j\in\\{0,1\\}\)</span> 中应用最多，也称作“市场篮子”分析。这种情形下观测值为销售交易，比如出现在商店收银台的商品。变量表示所有在商店中出售的商品。对于观测 <span class="math notranslate nohighlight">\(i\)</span>，每个变量 <span class="math notranslate nohighlight">\(X_j\)</span> 取值为 0 或 1；如果第 <span class="math notranslate nohighlight">\(j\)</span> 个商品作为该次交易购买的一部分则 <span class="math notranslate nohighlight">\(x_{ij}=1\)</span>，而如果没有购买则 <span class="math notranslate nohighlight">\(x_{ij}=0\)</span>。这些经常有联合值的变量表示物品经常被一起购买。这个信息对于货架、跨营销的促销活动、商品目录的设计，以及基于购买模式的消费者划分都是很有用的。</p>
<p>更一般地，关联分析的基本目标是寻找 <strong>特征向量 (feature vector)</strong> <span class="math notranslate nohighlight">\(X\)</span> 的 <strong>原型 <span class="math notranslate nohighlight">\(X\)</span> 值 (prototype <span class="math notranslate nohighlight">\(X\)</span>-values)</strong> <span class="math notranslate nohighlight">\(v_1,\ldots,v_L\)</span> 的集合，使得概率密度 <span class="math notranslate nohighlight">\(\mathrm{Pr}(v_l)\)</span> 在这些值上的取值相对大。在一般框架下，这个问题可以看成是“<strong>模式寻找 (mode finding)</strong>”或者“<strong>碰撞狩猎 (bump hunting)</strong>”。如所阐释的，这个问题是非常困难。每个 <span class="math notranslate nohighlight">\(\mathrm{Pr}(v_l)\)</span> 一个很自然的估计是 <span class="math notranslate nohighlight">\(X=v_l\)</span> 时观测的比例。对于涉及不止一个变量的问题，每个变量都可以假定不止一个值，要得到可靠估计，<span class="math notranslate nohighlight">\(X=v_l\)</span> 的观测值的个数几乎总是太少。</p>
<p>进行第一次简化来修改目标。与其寻找 <span class="math notranslate nohighlight">\(\mathrm{Pr}(x)\)</span> 很大时的 <span class="math notranslate nohighlight">\(x\)</span> <strong>值 (values)</strong>，不如在 <span class="math notranslate nohighlight">\(X\)</span> 空间中寻找相对它们大小和支撑集而言的大概率部分的 <strong>区域 (regions)</strong>。令 <span class="math notranslate nohighlight">\(\mathcal S_j\)</span> 表示第 <span class="math notranslate nohighlight">\(j\)</span> 个变量的所有可能值的集合（<strong>支撑集 (support)</strong>），并且令  <span class="math notranslate nohighlight">\(s_j\subseteq\mathcal S_j\)</span> 为这些值的子集。修改后的目标可以叙述成试图寻找变量值的子集 <span class="math notranslate nohighlight">\(s_1,\ldots,s_p\)</span> 来使得每个变量同时取对应子集的某个值的概率相对地大，则也就是使得</p>
<div class="math notranslate nohighlight">
\[
\Pr\Big[\bigcap_{j=1}^p(X_j\in s_j)\Big]\tag{14.2}
\]</div>
<p>相对大。子集的交 <span class="math notranslate nohighlight">\(\cap_{j=1}^p(X_j\in s_j)\)</span> 称作 <strong>联合规则 (conjunctive rule)</strong>。对于定性变量，子集 <span class="math notranslate nohighlight">\(s_j\)</span> 为邻接区间；对于类别型变量，子集是明确界定的。注意到如果子集实际上是整个集合 <span class="math notranslate nohighlight">\(s_j=\mathcal S_j\)</span>，经常是这种情形，变量 <span class="math notranslate nohighlight">\(X_j\)</span> 被称为没有出现在规则 式（ 14.2 ） 中。</p>
<div class="section" id="id2">
<h2>（）市场篮子分析<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>求解 式（ 14.2 ） 的一般方法将在 <span class="xref myst">14.2.5 节</span>讨论。这在许多应用中是很有用的。然而，它们对于非常多 (<span class="math notranslate nohighlight">\(p\approx 10^4,N\approx 10^8\)</span>) 的交易数据是不可行的，但市场篮子分析经常要应用到这些数据上。所以需要对 式（ 14.2 ） 进一步简化。首先，只考虑两种类型的子集；<span class="math notranslate nohighlight">\(s_j\)</span> 要么只包含 <span class="math notranslate nohighlight">\(X_j\)</span> 的单个值 <span class="math notranslate nohighlight">\(s_j=v_{0j}\)</span>，要么包含 <span class="math notranslate nohighlight">\(X_j\)</span> 的所有值，<span class="math notranslate nohighlight">\(s_j=\mathcal S_j\)</span>。这将 式（ 14.2 ） 简化为寻找元素为整数的子集 <span class="math notranslate nohighlight">\(\mathcal J\subset\\{1,\ldots,p\\}\)</span>，以及对应的值 <span class="math notranslate nohighlight">\(v_{0j},j\in\mathcal J\)</span>，使得</p>
<div class="math notranslate nohighlight">
\[
\Pr\Big[\bigcap_{j\in \mathcal J}(X_j=v_{0j})\Big]\tag{14.3}
\]</div>
<p>相对大。图 14.1 解释了这个假设。</p>
<p><img alt="" src="../_images/fig14.1.png" /></p>
<blockquote>
<div><p>图14.1. 关联规则的简化。这里有两个输入 <span class="math notranslate nohighlight">\(X_1\)</span> 和 <span class="math notranslate nohighlight">\(X_2\)</span>，分别取 4 和 6 个不同的值。红色方块表示高密度的区域。为了简化计算，我们假设导出的子集要么对应输入的单个值，要么对应所有值。有了这个假设，我们可以找到图中中间或者右边的模式，而不是左边的模式。</p>
</div></blockquote>
<p>可以应用 <strong>虚拟变量 (dummy variables)</strong> 的技巧将 式（ 14.3 ） 转换为只涉及二值变量的问题。这里我们假设支撑集 <span class="math notranslate nohighlight">\(\mathcal S_j\)</span> 对于每个变量 <span class="math notranslate nohighlight">\(X_j\)</span> 都是有限的。具体地，构造新的变量集 <span class="math notranslate nohighlight">\(Z_1,\ldots,Z_K\)</span>，对于可由每个原始变量 <span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_p\)</span> 取得的每个值 <span class="math notranslate nohighlight">\(v_{lj}\)</span> ，创建一个这样的变量。虚拟变量的数目 <span class="math notranslate nohighlight">\(K\)</span> 为</p>
<div class="math notranslate nohighlight">
\[
K=\sum\limits_{j=1}^p\vert \mathcal S_j\vert\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\vert \mathcal S_j\vert\)</span> 为从 <span class="math notranslate nohighlight">\(X_j\)</span> 得到的唯一值的个数。</p>
<p>如果与其相关联的变量取 <span class="math notranslate nohighlight">\(Z_k\)</span> 对应的值，则每个虚拟变量被赋值为 <span class="math notranslate nohighlight">\(Z_k=1\)</span>，否则 <span class="math notranslate nohighlight">\(Z_k=0\)</span>。</p>
<blockquote>
<div><p>notes “weiya 注”
每个 <span class="math notranslate nohighlight">\(v_{lj}\)</span>（可看成是 <span class="math notranslate nohighlight">\(p\)</span> 维列向量中的一个元素）都有一个虚拟变量 <span class="math notranslate nohighlight">\(Z_{\ell}\)</span>。</p>
</div></blockquote>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>![](../img/14/pho14.1.png)

似乎采用虚拟变量后，不再限制变量要么取单个值要么取全部，比如图中对于 $X_1$，可以取 $3, 4$。即便这样，也还是进行了简化，仍不能返回图 14.1 中左图的模式，只能得到如下阴影部分的模式。

![](../img/14/pho14.1c.jpg)
</pre></div>
</div>
<p>这将 式（ 14.3 ） 转换为寻找整数集 <span class="math notranslate nohighlight">\({\mathcal K}\subset\\{1,\ldots,K\\}\)</span> 使得下式的值大。</p>
<div class="math notranslate nohighlight">
\[
\Pr\Big[\bigcap_{k\in\mathcal K}(Z_k=1)\Big]=\Pr\Big[\prod\limits_{k\in\mathcal K}Z_k=1\Big]\tag{14.4}
\]</div>
<p>这是市场篮子问题的标准形式。集合 <span class="math notranslate nohighlight">\(\mathcal K\)</span> 称为“<strong>项目集 (items set)</strong>”。项目集中的变量 <span class="math notranslate nohighlight">\(Z_k\)</span> 的个数称为“大小 (size)”（注意到这个大小不大于 <span class="math notranslate nohighlight">\(p\)</span>）。式（ 14.4 ） 的估计值取为在数据集中式 式（ 14.4 ） 中关联为真的观测的比例：</p>
<div class="math notranslate nohighlight">
\[
\widehat{\Pr}\Big[\prod\limits_{k\in\mathcal K}(Z_k=1)\Big]=\frac{1}{N}\sum\limits_{i=1}^N\prod_{k\in\mathcal K}z_{ik}\tag{14.5}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(z_{ik}\)</span> 为 <span class="math notranslate nohighlight">\(Z_k\)</span> 的第 <span class="math notranslate nohighlight">\(i\)</span> 种情形的值。上式称作项目集 <span class="math notranslate nohighlight">\(\mathcal K\)</span> 的“支撑 (support)”或“流行 (prevalence)” <span class="math notranslate nohighlight">\(T(\mathcal K)\)</span>。<span class="math notranslate nohighlight">\(\prod_{k\in\mathcal k}z_{ik}=1\)</span> 的观测 <span class="math notranslate nohighlight">\(i\)</span> 称作“包含 (contain)”项目集 <span class="math notranslate nohighlight">\(\mathcal K\)</span> 。</p>
<p>在关联规则挖掘中确定支撑的下界 <span class="math notranslate nohighlight">\(t\)</span>，然后寻找所有可以由变量 <span class="math notranslate nohighlight">\(Z_1,\ldots,Z_k\)</span> 组成的项目集 <span class="math notranslate nohighlight">\(\mathcal K_l\)</span>，并且支撑集大于下界 <span class="math notranslate nohighlight">\(t\)</span>，也就是</p>
<div class="math notranslate nohighlight">
\[
\{{\mathcal K_l}\mid T({\mathcal K_l})&gt;t\}\tag{14.6}
\]</div>
</div>
<div class="section" id="apriori">
<h2>（）Apriori 算法<a class="headerlink" href="#apriori" title="Permalink to this headline">¶</a></h2>
<p>如果调整阈值 <span class="math notranslate nohighlight">\(t\)</span> 使得式 式（ 14.6 ） 仅由所有 <span class="math notranslate nohighlight">\(2^K\)</span> 中可能的项集合中的一小部分构成，则对于非常大的数据集，可以通过可行的计算得到 式（ 14.6 ） 的解。“Apriori”算法（Argawal et al.，1995<a class="footnote-reference brackets" href="#id8" id="id3">1</a>）探究维数灾难的某些方面，采用小部分的数据传递求解 式（ 14.6 ）。具体地，对于给定的支撑阈值 <span class="math notranslate nohighlight">\(t\)</span>：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\\{{\mathcal K}\mid T(\mathcal K)&gt;t \\}\)</span> 的 <strong>基数(cardinality)</strong> 相对小。</p></li>
<li><p>任意由项目集 <span class="math notranslate nohighlight">\(\mathcal K\)</span> 中子集组成的集合 <span class="math notranslate nohighlight">\(\mathcal L\)</span> 必须有比 <span class="math notranslate nohighlight">\(\mathcal K\)</span> 大或相等的支撑，即 <span class="math notranslate nohighlight">\({\mathcal L\subseteq K}\Rightarrow T({\mathcal L})\ge T({\mathcal K})\)</span></p></li>
</ul>
<p>第一次传递数据计算所有单项目集合的支撑。舍弃那些支撑小于阈值的项目集。第二次传递数据计算所有可以通过第一次传递中保留下来的单项目集合匹配成对的项目集（此时大小为 2）的支撑。换句话说，为了产生所有大小为 <span class="math notranslate nohighlight">\(\vert\mathcal K\vert=m\)</span> 的高频率项目集，我们仅仅需要考虑部分候选项目集，这些候选项目集的所有（即 <span class="math notranslate nohighlight">\(m\)</span> 个）大小为 <span class="math notranslate nohighlight">\(m-1\)</span> 的上一级项目集是高频率的。</p>
<blockquote>
<div><p>note “weiya 注：”
对于给定的大小为 <span class="math notranslate nohighlight">\(m\)</span> 的项目集 <span class="math notranslate nohighlight">\(\mathcal K\)</span>，它有 <span class="math notranslate nohighlight">\(m\)</span> 个大小为 <span class="math notranslate nohighlight">\(m-1\)</span> 的父项目集。</p>
</div></blockquote>
<p>舍弃那些支撑小于阈值的大小为 2 的项目集。每个后继的传递数据只考虑了那些可以通过结合上一次传递数据存留的项目集与第一次传递数据保留下的项目集得到的项目集。数据传递过程一直进行下去，直到来自上一次传递的所有候选规则的支撑都小于指定阈值。Apriori 算法仅要求对 <span class="math notranslate nohighlight">\(\vert\mathcal K\vert\)</span> 个值的每一个进行一次数据传递，这是很重要的，因为我们假设数据不能放在计算机的主存中。如果数据是充分稀疏（或者如果阈值 <span class="math notranslate nohighlight">\(t\)</span> 充分大），则会在合理次数之后终止过程，甚至对非常大的数据集也是如此。</p>
<p>许多额外的技巧可以作为这个策略的一部分来提高速度和收敛（Agrawal et al., 1995<a class="footnote-reference brackets" href="#id8" id="id4">1</a>）。Apriori 算法标志数据挖掘技术的主要进步。</p>
<p>通过 Apriori 算法返回的每个高支撑的数据集 <span class="math notranslate nohighlight">\(\mathcal K\)</span> 式（ 14.6 ） 被放到“关联规则”的集合中。项目 <span class="math notranslate nohighlight">\(Z_k,k\in\mathcal K\)</span> 被分成两个分离的子集，<span class="math notranslate nohighlight">\(A\cup B=\mathcal K\)</span>，并且写成</p>
<p><span class="math notranslate nohighlight">\(
A\Rightarrow B\tag{14.7}
\)</span>$</p>
<p>第一项子集 <span class="math notranslate nohighlight">\(A\)</span> 被称作“先因 (antecedent)”，第二个子集 <span class="math notranslate nohighlight">\(B\)</span> 被称为“后果 (consequent)”。关联规则基于在数据库中 antecedent 和 consequent 项目集的流行程度，来定义具有一些性质。规则 <span class="math notranslate nohighlight">\(T(A\Rightarrow B)\)</span> 的“支撑”是在先行项目集和后果项目集的并集中的观测的占比，这恰恰是导出它们的项目集 <span class="math notranslate nohighlight">\(\mathcal K\)</span> 的支撑。可以看成在随机选择的市场篮子中同时观测项目集 <span class="math notranslate nohighlight">\(\mathrm{Pr}(A\; and\; B)\)</span> 的概率的估计 式（ 14.5 ）。该规则的“<strong>置信度 (confidence)</strong>” 或“<strong>可预测性 (predictability)</strong>” <span class="math notranslate nohighlight">\(C(A\Rightarrow B)\)</span> 是它的支撑除以 antecedent 的支撑</p>
<p><span class="math notranslate nohighlight">\(
C(A\Rightarrow B)=\frac{T(A\Rightarrow B)}{T(A)}\tag{14.8}
\)</span>$</p>
<p>可以看成是 <span class="math notranslate nohighlight">\(\mathrm{Pr}(B\mid A)\)</span> 的估计。记号 <span class="math notranslate nohighlight">\(\mathrm{Pr}(A)\)</span> 是在篮子中出现项目集 <span class="math notranslate nohighlight">\(A\)</span> 的概率，是 <span class="math notranslate nohighlight">\(\mathrm{Pr}(\prod_{k\in A}Z_k=1)\)</span> 的缩写。“<strong>期望置信度</strong>”定义为 consequent 的支撑 <span class="math notranslate nohighlight">\(T(B)\)</span>，是无条件的概率 <span class="math notranslate nohighlight">\(\mathrm{Pr}(B)\)</span> 的估计。最后，规则的 “lift” 定义为置信度除以期望置信度</p>
<div class="math notranslate nohighlight">
\[
L(A\mid B)=\frac{C(A\Rightarrow B)}{T(B)}
\]</div>
<p>这是关联衡量 <span class="math notranslate nohighlight">\(\mathrm{Pr}(A\; and\; B)/\mathrm{Pr}(A)\mathrm{Pr}(B)\)</span> 的估计。</p>
<blockquote>
<div><p>note “weiya 注：lift vs. Lift Chart “
在评估预测模型时，有时会作出 Lift Chart，与这里的 lift 看起来很像。画 Lift Chart 时，</p>
</div></blockquote>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1. 首先对测试样本进行预测计算某一事件（比如有 $K$ 个类别，考虑其中的类 a）的类别概率，根据类别概率从大到小排序；
2. 给定一个**概率阈值**，计算第 1 步中所有小于该概率的样本中该事件发生的比例（比如类别概率小于 0.8 的样本中类别 a 的比例）
3. 计算**基准 (baseline) 事件率**，也就是全样本集中所考察的事件的比例（比如类别 a 的比例），第 2 步的比例除以基准事件率则得到该阈值下的 **lift**。

很直观地，可以发现关联规则中的 lift 与 Lift Chart 的计算方法很像，概率阈值就相当于**支撑阈值 $t$**，基准事件率对应 $T(B)$，而小于阈值时样本中事件发生的比例对应 $C(A\Rightarrow B)$。

R 中作出 Lift Chart 的命令也很简单，直接调用 `caret` 包中的 `lift` 函数可以作出如下的 Lift Chart（采用的是示例代码）

![](../notes/apriori/lift2.png)
</pre></div>
</div>
<p>举个例子，假设项目集为 <span class="math notranslate nohighlight">\({\mathcal K}=\)</span> {<code class="docutils literal notranslate"><span class="pre">peanut</span> <span class="pre">butter</span></code>, <code class="docutils literal notranslate"><span class="pre">jelly</span></code>, <code class="docutils literal notranslate"><span class="pre">bread</span></code>}，并且考虑规则 {<code class="docutils literal notranslate"><span class="pre">peanut</span> <span class="pre">butter</span></code>, <code class="docutils literal notranslate"><span class="pre">jelly</span></code>} <span class="math notranslate nohighlight">\(\Rightarrow\)</span> {<code class="docutils literal notranslate"><span class="pre">bread</span></code>}。<span class="math notranslate nohighlight">\(0.03\)</span> 的支撑值表示 <code class="docutils literal notranslate"><span class="pre">peanut</span> <span class="pre">butter</span></code>，<code class="docutils literal notranslate"><span class="pre">jelly</span></code> 和 <code class="docutils literal notranslate"><span class="pre">bread</span></code>同时出现在 <span class="math notranslate nohighlight">\(3\%\)</span> 的市场篮子中。这个规则的 0.82 置信度表示当购买了 <code class="docutils literal notranslate"><span class="pre">peanut</span> <span class="pre">butter</span></code> 和 <code class="docutils literal notranslate"><span class="pre">jelly</span></code>，<span class="math notranslate nohighlight">\(82\%\)</span> 的情形下也会购买 <code class="docutils literal notranslate"><span class="pre">bread</span></code>。如果<code class="docutils literal notranslate"><span class="pre">bread</span></code> 在 <span class="math notranslate nohighlight">\(43\%\)</span> 的市场篮子中，则规则 {<code class="docutils literal notranslate"><span class="pre">peanut</span> <span class="pre">butter</span></code>, <code class="docutils literal notranslate"><span class="pre">jelly</span></code>}<span class="math notranslate nohighlight">\(\Rightarrow\)</span> {<code class="docutils literal notranslate"><span class="pre">bread</span></code>} 的 lift 为 <span class="math notranslate nohighlight">\(1.95\)</span>。</p>
<p>这个分析的目标是得到支撑和置信度 式（ 14.8 ） 都高的关联规则 式（ 14.7 ）。Apriori 算法返回由支撑阈值 <span class="math notranslate nohighlight">\(t\)</span> 式（ 14.6 ） 定义的所有高支撑的项目集。设定置信度阈值 <span class="math notranslate nohighlight">\(c\)</span>，报告所有可以从这些项目集 式（ 14.6 ） 中组成的置信度大于 <span class="math notranslate nohighlight">\(c\)</span> 的规则，也就是</p>
<div class="math notranslate nohighlight">
\[
\{A\Rightarrow B\mid C(A\Rightarrow B)&gt;c\}\tag{14.9}
\]</div>
<p>对于大小为 <span class="math notranslate nohighlight">\(\vert\mathcal K\vert\)</span> 的项目集 <span class="math notranslate nohighlight">\(\mathcal K\)</span>，有 <span class="math notranslate nohighlight">\(2^{\vert{\mathcal K}\vert-1}-1\)</span> 条形式为 <span class="math notranslate nohighlight">\(A\Rightarrow ({\mathcal K}-A),A\subset \mathcal K\)</span> 的规则。</p>
<blockquote>
<div><p>note “weiya 注：”
因为大小为 <span class="math notranslate nohighlight">\(\vert\mathcal K\vert\)</span> 的集合的子集个数为 <span class="math notranslate nohighlight">\(2^{\vert \mathcal K\vert}\)</span>，除去空集以及全集，则应该有 <span class="math notranslate nohighlight">\(2^{\vert \mathcal K\vert} - 2\)</span> 种分割，作者有可能是把 <span class="math notranslate nohighlight">\(A\rightarrow {\mathcal K}-A\)</span> 与 <span class="math notranslate nohighlight">\({\mathcal K}-A \rightarrow A\)</span> 视为同一种形式，则</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[    \frac{1}{2}(2^{\vert\mathcal K\vert}-2)=2^{\vert{\mathcal K}\vert-1}-1
    
\]</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>如果不要求 $A$ 和 $B$ 的并集为整个项目集，[Pang-Ning Tan, Michael Steinbach, Anuj Karpatne, and Vipin Kumar. 2018. Introduction to Data Mining (2nd Edition) (2nd. ed.). Pearson.](https://www-users.cs.umn.edu/~kumar001/dmbook/ch6.pdf) 给出一个结论，对于大小为 $d$ 的项目集，总共有 $3^d - 2^{d+1} + 1$ 种可能的规则，
</pre></div>
</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}    \begin{align*}
    &amp;\sum_{k=1}^{d-1} \binom{d}{k}\sum_{i=1}^{d-k}\binom{d-k}{i} \\
    =&amp; \sum_{k=1}^{d-1}\binom{d}{k}(2^{d-k}-1) \\
    =&amp; \sum_{k=1}^{d-1}\binom{d}{k}2^{d-k}- \sum_{k=1}^{d-1}\binom{d}{k}\\
    =&amp; ((1 + 2)^d - 2^d - 1) - (2^{d} - 2)\\
    =&amp; 3^d - 2^{d+1} + 1\,.
    \end{align*}
    
$
Agrawal et al. (1995)[^1] 提出 Apriori 算法的一个变体，它可以从由项目集 式（ 14.6 ） 构造的所有可能的规则中快速确定哪些规则会在置信阈值 式（ 14.9 ） 下存留下来。\end{split}\\整个分析的输出是满足下面约束的关联规则 式（ 14.7 ） 的集合。\end{aligned}\end{align} \]</div>
<p>T(A\Rightarrow B)&gt;t\qquad and\qquad C(A\Rightarrow B)&gt;c</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}这些一般保存在数据库中，可以被用户查询到。一般的查询请求可能是按照置信度，lift 或者支撑的大小顺序排列规则。更具体地，可能会要查询在 antecedent中 含特定的项目或在 consequent 中含特定的项目的条件下的列表。举个例子，一条查询请求可能如下：\\&gt; 显示 ice skates 为 consequent，置信度大于 $80\%$ 且支撑大于 $2\%$ 的所有交易。\\这可以提供能够预测 ice skates 销量的项（antecedent）的信息。关注特定的结果（consequent）便将问题转换成了监督学习的框架。\\关联规则成为了在相关的市场篮子的设定下用于分析非常大的交易数据库的流行工具。这是当数据可以转换成多维邻接表的形式时。输出是以容易理解并且可解释的关联规则 式（ 14.4 ） 的形式展现的。Apriori 算法允许分析可以用到大的数据库中，更大的数据库适用于其他类型的分析。关联规则是数据挖掘最大的成功之一。\\除了对我们可以应用的数据有限制外，关联规则还有其它的限制。计算可行性的关键是支撑阈值 式（ 14.6 ）。项目集的解的个数，它们的大小，以及对数据需要传递的次数随着下界的下降指数型增长。因此，不会发现有高置信度或者高 lift，但是低支撑的规则。举个例子，比如由于后果（consequent）`caviar` 的低销售量，将不会找到 `vodka` $\Rightarrow$ `caviar` 的高置信度规则。\\## （）例子：市场篮子分析\\我们将在中等大小的人口统计数据库中解释 Apriori 算法的使用。数据集包含 $N=9409$ 份问卷，由旧金山湾区 (San Francisco Bay Area) 的购物商场里的消费者填写的（Impact Resources, Inc., Columbus OH, 1987）。这里我们采用前 14 个与人口统计有关的问题的回答来说明。数据中包含顺序型和（无序）类别变量，后者中的许多具有多个值。并且有许多缺失数据。\\我们采用 Apriori 算法的免费软件实现，这归功于 Christian Borgelt。（见 [http://fuzzy.cs.uni-magdeburg.de/~borgelt](http://www.borgelt.net/)）。除去缺失数据的观测，每个顺序型预测变量在中值处分开并且用两个虚拟变量来编码；每个含有 $k$ 个类别的类别型预测变量用 $k$ 个虚拟变量编码。得到 $6876\times 50$ 阶矩阵，$6876$ 为观测个数，$50$ 为虚拟变量个数。\\这个算法总共找到 $6288$ 条关联规则，涉及 $\le 5$ 个预测变量，支撑至少为 $10\%$。理解这个大的规则集合本身是具有挑战的数据分析工作。我们这里将不会试图解决，但仅仅在图 14.2 中说明每个虚拟变量在数据中的相对频率（上）和在关联规则中的相对频率（下）。受欢迎的类别趋向于在规则中频繁出现，例如，排第一的类别是 language (English)。然而，其他的像职业 (occupation) 则表现出**代表人数不足 (under-represented)**，除了第一和第五个水平。\\![](../img/14/fig14.2.png)\\&gt; 图14.2. 市场篮子分析：每个虚拟变量（对输入类别编码）在数据中的相对频率（上），以及在由 Apriori 算法找到的关联规则中的相对频率（下）\\下面是通过 Apriori 算法找出的关联规则的三个例子：\\- 关联规则 1：25% 的支撑，99.7% 的置信度，1.03 的 lift\\![](../img/14/ar1.png)\\- 关联规则 2：13.4% 的支撑，80.8% 的置信度，2.13 的 lift\\![](../img/14/ar2.png)\\- 关联规则 3：26.5% 的支撑，82.8% 的置信度，2.15 的 lift\\![](../img/14/ar3.png)\\根据高支撑，我们选择第一和第三条规则。第二条规则是有高收入的 consequent 的关联规则，并且可以用来试图挑出高收入的个体。\\正如上面叙述的，我们对每个输入预测变量构造虚拟变量，举个例子，根据收入低于和高于中位数得到 $Z_1=I(\text{income} &lt; \$40,000)$ 和 $Z_2=I(\text{income}\ge \$40,000)$。如果我们仅仅对寻找与高收入类别的关联感兴趣，我们可能会包含 $Z_2$但不包含 $Z_1$。实际市场篮子问题中经常是这种情形，我们感兴趣的是找到与现存的相对罕见项的关联，而不是跟它缺失有关的关联。\\## （）作为监督学习的非监督\\这里我们讨论将密度估计问题转化为一种监督函数近似的技巧。这是将在下一节描述的广义关联规则的基础。\\令 $g(x)$ 为需要估计的未知数据概率密度，且 $g_0(x)$ 为用作参考的确定的概率密度函数。举个例子，$g_0(x)$ 可能是在变量定义域上的均匀概率密度。其他的概率类型在下面讨论。假定数据集 $x_1,x_2,\ldots,x_N$ 是从 $g(x)$ 中抽取的独立同分布的样本。大小为 $N_0$ 的样本可以通过蒙特卡洛法从 $g_0(x)$ 中抽取。混合这两个数据集，并且对从 $g(x)$ 抽取的样本赋权 $\omega=N_0/(N+N_0)$，对从 $g_0(x)$ 中抽取的样本赋权 $\omega_0=N/(N+N_0)$，得到从混合密度 $(g(x)+g(x_0))/2$ 中抽取的随机样本。如果对从 $g(x)$ 中抽取的样本赋值为 $Y=1$，对从 $g_0(x)$ 中抽取的样本赋值为 $Y=0$，则\end{aligned}\end{align} \]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mu(x)=\mathbb{E}(Y|x)&amp;=\frac{g(x)}{g(x)+g_0(x)}\\
&amp;=\frac{g(x)/g_0(x)}{1+g(x)/g_0(x)}\tag{14.10}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[可以通过将下面的混合样本作为训练数据，然后采用监督学习的方法来估计\]</div>
<p>(y_1,x_1),(y_2,x_2),\ldots,(y_{N+N_0},x_{N+N_0})\tag{14.11}
$$</p>
<p>估计的结果 <span class="math notranslate nohighlight">\(\hat \mu(x)\)</span> 可以反解得 <span class="math notranslate nohighlight">\(g(x)\)</span> 的估计</p>
<div class="math notranslate nohighlight">
\[
\hat g(x)=g_0(x)\frac{\hat \mu(x)}{1-\hat\mu(x)}\tag{14.12}
\]</div>
<p>广义逻辑斯蒂回归（<span class="xref myst">4.4 节</span>）是非常适用于这个方法，因为 log-odd 是直接估计的</p>
<div class="math notranslate nohighlight">
\[
f(x)=\log\frac{g(x)}{g_0(x)}\tag{14.13}
\]</div>
<p>此时我们有</p>
<div class="math notranslate nohighlight">
\[
\hat g(x)=g_0(x)e^{\hat f(x)}\tag{14.14}
\]</div>
<p>图 14.3 显示了一个例子。我们在左边图中产生大小为 200 的训练集。右边显示了在图中长方形区域内均匀产生的参考点（蓝色）。训练样本被标号为 1，而参考样本标号为 0，并且采用逻辑斯蒂回归模型对数据进行拟合，逻辑斯蒂回归模型使用自然样条的张量积（<span class="xref myst">5.2.1 节</span>）。<span class="math notranslate nohighlight">\(\mu(x)\)</span> 的概率等高线展示在右图中；它们也是密度估计 <span class="math notranslate nohighlight">\(\hat g(x)\)</span> 的等高线，因为 <span class="math notranslate nohighlight">\(\hat g(x)=\hat\mu(x)/(1-\hat\mu(x))\)</span> 是单调函数。等高线大致捕捉了数据的密度。</p>
<p><img alt="" src="../_images/fig14.3.png" /></p>
<blockquote>
<div><p>图14.3. 通过分类的密度估计。（左）200 个数据点的训练集。（右）加上在矩形区域内均匀产生 200 个参考数据点的训练集。训练样本标记为类别 1，参考数据点为类别 0，对数据用半参逻辑斯蒂回归模型进行拟合。图中显示了 <span class="math notranslate nohighlight">\(\hat g(x)\)</span> 的等高线。</p>
</div></blockquote>
<p>原则上任意参考密度都可以作为 式（ 14.14 ） 的 <span class="math notranslate nohighlight">\(g_0(x)\)</span>。实际中估计 <span class="math notranslate nohighlight">\(\hat g(x)\)</span> 的准确性非常依赖于特定的选择。好的选择会取决于数据密度 <span class="math notranslate nohighlight">\(g(x)\)</span> 和用来估计 式（ 14.10 ） 和 式（ 14.13 ） 的过程。如果目标是正确性，应该选择 <span class="math notranslate nohighlight">\(g_0(x)\)</span> 使得最终函数 <span class="math notranslate nohighlight">\(\mu(x)\)</span> 和<span class="math notranslate nohighlight">\(f(x)\)</span> 可以简单地被使用的方法来近似。然而，准确性不总是主要目标。<span class="math notranslate nohighlight">\(\mu(x)\)</span> 和 <span class="math notranslate nohighlight">\(f(x)\)</span> 是概率比率 <span class="math notranslate nohighlight">\(g(x)/g_0(x)\)</span> 的单调函数。它们可以看成是提供关于数据密度 <span class="math notranslate nohighlight">\(g(x)\)</span> 与 <span class="math notranslate nohighlight">\(g_0(x)\)</span> 偏离的信息的“差异”统计量。因此，在数据分析设定中，<span class="math notranslate nohighlight">\(g_0(x)\)</span> 的选择是由偏离的类型决定的，实际中特定问题的情况下似乎是最有趣的。举个例子，如果我们感兴趣的是均匀的偏离，则 <span class="math notranslate nohighlight">\(g_0(x)\)</span> 可能是变量值域上的均匀密度函数。如果与联合正态的偏离是我们感兴趣的，<span class="math notranslate nohighlight">\(g_0(x)\)</span> 的一个好的选择是与原数据相同的均值向量和协方差均值。与独立性的偏离可以通过用下式表征：</p>
<p><span class="math notranslate nohighlight">\(
g_0(x)=\prod\limits_{j=1}^pg_j(x_j)\tag{14.15}
\)</span>$</p>
<p>其中 <span class="math notranslate nohighlight">\(g_j(x_j)\)</span> 为 <span class="math notranslate nohighlight">\(X_j\)</span>（<span class="math notranslate nohighlight">\(X\)</span> 的第 <span class="math notranslate nohighlight">\(j\)</span> 个坐标）的边缘密度。通过对每个变量的数据应用不同的随机排列，独立密度 式（ 14.15 ） 的样本可以很简单地从数据本身产生。</p>
<blockquote>
<div><p>note “weiya 注：”
<a class="reference internal" href="14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html"><span class="doc std std-doc">第 14.7 节</span></a> 介绍了互信息量，它是独立性的一个自然度量，是联合密度与其边缘密度乘积间的 Kullback-Leibler 距离。</p>
</div></blockquote>
<p>正如上面讨论的那样，非监督学习关心揭示数据密度 <span class="math notranslate nohighlight">\(g(x)\)</span> 的性质。每个技巧关注于特定的性质或者某些性质。尽管这种将问题转换为监督学习的方式 式（ 14.10 ）-式（ 14.14 ） 看起来在一段时间内成为<strong>统计民俗学 (statistics folklore)</strong> 的一部分，但它似乎没有太大的影响，尽管它有将已经研究透彻的监督学习方法应用到非监督学习的潜力。其中一个原因可能是这个问题必须要用蒙特卡洛技巧产生的模拟数据来扩大。因为数据集的大小至少和数据样本一样大 <span class="math notranslate nohighlight">\(N_0\ge N\)</span>，这个估计过程的计算和内存上的要求至少双倍。另外，产生蒙特卡洛样本本身可能需要大量的计算。尽管在过去这是一个限制，但是这增加的计算需求不再成为一个负担，因为增加的资源也变成了可能。我们将在下一节说明在非监督学习中监督学习方法的使用。</p>
</div>
<div class="section" id="id5">
<h2>（）广义关联规则<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>在数据空间中寻找高密度区域的更一般问题 式（ 14.2 ） 可以通过使用上面描述的监督学习的方法来解决。尽管不能应用对市场篮子分析可行的大数据库，但可以从适当大小的数据集中得到有用的信息。问题 式（ 14.2 ） 可以用公式叙述为，寻找整数子集 <span class="math notranslate nohighlight">\(\mathcal J\subset\\{1,2,\ldots,p\\}\)</span> 和与对应的变量 <span class="math notranslate nohighlight">\(X_j\)</span> 对应的子集 <span class="math notranslate nohighlight">\(s_j,j\in \mathcal J\)</span> 使得下式的值大。</p>
<div class="math notranslate nohighlight">
\[
\widehat{\Pr}\Big(\bigcap_{j\in\mathcal J}(X_j\in s_j)\Big) = \frac{1}{N}\sum\limits_{i=1}^NI\Big(\bigcap_{j\in\mathcal J}(x_{ij}\in s_j)\Big)\tag{14.16}
\]</div>
<p>根据关联规则的数据，<span class="math notranslate nohighlight">\(\\{(X_j\in s_j)\\}_{i\in \mathcal J}\)</span> 将被称为“广义”项目集。对于定量变量的子集 <span class="math notranslate nohighlight">\(s_j\)</span> 取为在它们值域中的邻接区间，对于类别型变量的子集可以涉及不止一个单值。这种方法的这条“野心的 (ambitious)” 性质阻碍了对所有广义项目集的全面搜索，来寻找式 式（ 14.16 ） 大于特定最小值的支撑。必须用到启发式搜索算法，而且最希望有个寻找这样广义项目集的集合。</p>
<p>市场篮子分析 式（ 14.5 ） 和广义公式 式（ 14.16 ） 都隐式地引用均匀概率分布。如果所有的联合数据值 <span class="math notranslate nohighlight">\((x_1,x_2,\ldots,x_N)\)</span> 都是均匀分布的，则寻找比期望更频繁的项目集。这有助于寻找单个边缘组分 <span class="math notranslate nohighlight">\((X_j\in s_j)\)</span> 是频繁的项目集，也就是，下式的值大。</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{N}\sum_{i=1}^NI(x_{ij}\in s_j)\tag{14.17}
\]</div>
<p>频繁子集 式（ 14.17 ） 的联合在高支撑的项目集 式（ 14.16 ） 中趋向于比不频繁子集的联合更经常出现。这也就是为什么尽管高关联（lift）的规则 <code class="docutils literal notranslate"><span class="pre">vodka</span></code><span class="math notranslate nohighlight">\(\Rightarrow\)</span><code class="docutils literal notranslate"><span class="pre">caviar</span></code> 不可能被发现；因为没有一项含有高的边缘支撑，所以它们的联合支撑特别地小。引用均匀分布可以得出高频率项目集，其组成部分之间具有低关联性，在最高支撑项目集的集合中占主导优势。</p>
<p>高频率子集 <span class="math notranslate nohighlight">\(s_j\)</span> 由最频繁的 <span class="math notranslate nohighlight">\(X_j\)</span> 值的 <strong>并集 (disjunction)</strong> 构成。</p>
<blockquote>
<div><p>note “weiya 注：disjunction”
参考维基词条，<a class="reference external" href="https://en.wikipedia.org/wiki/Logical_disjunction">Logical disjunction</a> 为逻辑关系词“或”，此处直接翻译为并集。</p>
</div></blockquote>
<p>采用变量边缘数据密度的积 式（ 14.15 ） 作为参考分布，移除了对在已发现项目集中单个变量的高频值的偏好。这是因为不管单个变量值的频率分布，如果变量中没有关联（完全独立），密度比率 <span class="math notranslate nohighlight">\(g(x)/g_0(x)\)</span> 是均匀的。像 <code class="docutils literal notranslate"><span class="pre">vodka</span></code><span class="math notranslate nohighlight">\(\Rightarrow\)</span><code class="docutils literal notranslate"><span class="pre">caviar</span></code> 的规则会有机会进行合并。然而，怎样将参考分布而不是均匀分布纳入进 Apriori 算法中是不清晰的。正如在 <span class="xref myst">14.2.4 节</span>中解释的那样，给定原始数据集，从积密度 式（ 14.15 ） 中产生样本是直接的。</p>
<p>选择完参考分布后，并且根据它抽采样本，如 式（ 14.11 ），则得到一个关于二值输出变量 <span class="math notranslate nohighlight">\(Y\in\\{0,1\\}\)</span> 的监督学习问题。目标是利用这些数据去寻找区域</p>
<p><span class="math notranslate nohighlight">\(
R=\bigcup_{j\in \mathcal J}(X_j\in s_j)\tag{14.18}
\)</span>$</p>
<p>使得目标函数 <span class="math notranslate nohighlight">\(\mu(x)=\mathbb{E}(Y\mid x)\)</span> 相对地大。另外，可能希望要求这些区域的数据支撑</p>
<div class="math notranslate nohighlight">
\[
T(R)=\int_{x\in R}g(x)dx\tag{14.19}
\]</div>
<p>不要太小。</p>
</div>
<div class="section" id="id6">
<h2>（）监督学习方法的选择<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>区域 式（ 14.18 ） 由联合规则定义。因此在这种情形下学习这些规则的监督学习方法会是最合适的。CART 决策树的终止结点由形式为 式（ 14.18 ） 的规则精确定义。对混合数据 式（ 14.11 ） 应用 CART 会得到决策树，决策树试图在整个数据空间用不相交的区域集合对目标 式（ 14.10 ） 建模。每个区域由形式为式（ 14.18 ） 中的一条规则而定义。这些有较高 <span class="math notranslate nohighlight">\(y\)</span> 平均值</p>
<div class="math notranslate nohighlight">
\[
\bar y_t=\text{ave}(y_i\mid x_i\in t)
\]</div>
<p>是高支撑广义项目集 式（ 14.16 ） 的候选者。实际的（数据）支撑由下式给出</p>
<div class="math notranslate nohighlight">
\[
T(R)=\bar y_t\cdot \frac{N_t}{N_t+N_0}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(N_t\)</span> 是由终止结点表示的区域中（混合）观测的个数。通过检查得到的决策树，可能会发现相对高支撑的感兴趣的项目集。这些可以分化出在搜寻高置信度和/或 lift 的广义关联规则中的 antecedent 和 consequent。</p>
<p>用于这个目的的另外一种自然学习的方法是在<span class="xref myst">第 9.3 节</span>描述的<strong>耐心规则诱导法 (PRIM)</strong>，它也产生了形为 式（ 14.18 ） 的精确规则，但是它是特别为寻找高支撑区域来在其中最大化目标 式（ 14.10 ） 的平均值而设计的，而不是试图用在整个数据空间中建立目标函数的模型。它也控制了在支撑/平均目标值之间权衡。</p>
<p><a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/94">练习 14.3</a> 提出一个问题，问题产生于任何一种方法从边缘分布的积中生成随机数据的时候。</p>
</div>
<div class="section" id="id7">
<h2>（）例子：市场篮子分析（继续）<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>我们在表 14.1 的人口统计数据的基础上说明 PRIM 的用法。</p>
<p><img alt="" src="../_images/tab14.1.png" /></p>
<p>从PRIM分析中合并后的高支撑的广义项目集中的三个如下：</p>
<p>项目集 1：支撑 = 24%</p>
<p><img alt="" src="../_images/is1.png" /></p>
<p>项目集 2：支撑 = 24%</p>
<p><img alt="" src="../_images/is2.png" /></p>
<p>项目集 3：支撑 = 15%</p>
<p><img alt="" src="../_images/is3.png" /></p>
<p>从这些项目集中以大于 95% 的置信度 式（ 14.8 ） 导出广义关联规则如下：</p>
<p>关联规则1：支撑 25%，置信度 99.7%，以及 lift 1.35
<img alt="" src="../_images/p500ar1.png" />
关联规则2：支撑 25%，置信度 98.7%，以及 lift 1.97
<img alt="" src="../_images/p500ar2.png" />
关联规则3：支撑 25%，置信度 95.9%，以及 lift 2.61
<img alt="" src="../_images/p500ar3.png" />
关联规则4：支撑 15%，置信度 95.4%，以及 lift 1.50
<img alt="" src="../_images/p501ar4.png" /></p>
<p>这些特殊的规则中没有太大的惊奇。其中的大部分符合直观。在其他缺少先验信息的情形下，期望之外的结果有更大的可能会进行合并。这些结果缺失说明了广义关联规则可以提供的信息种类，以及说明了规则导出方法的监督学习方法，比如 CART 或 PRIM，可以揭示在它们成分中保持高关联的项目集。</p>
<p>这些广义关联规则与之前通过 Apriori 算法找出的规则相比怎么样呢？因为 Apriori 过程给出了成千上万条规则，很难去比较它们。然而，可以对一些一般的点进行比较。Apriori 算法是穷举的——它找出所有支撑大于特定值的所有规则。相反地，PRIM 是贪婪算法，并且不保证给出“最优”的规则集。另一方面，Apriori 算法仅仅可以处理虚拟变量，也因此不能找到上面的一些规则。举个例子，因为 <code class="docutils literal notranslate"><span class="pre">type</span> <span class="pre">of</span> <span class="pre">home</span></code> 是类别型输入，对于每一层有一个虚拟变量，Apriori 不能找到涉及集合</p>
<div class="math notranslate nohighlight">
\[ \text{type of home} \neq \text{apartment}
\]</div>
<p>的规则。为了找到这个集合，我们必须对 <code class="docutils literal notranslate"><span class="pre">apartment</span></code> 和其他 home 的类别变量用虚拟变量编码。一般地对所有潜在的感兴趣的比较进行预先编码是不可行的。</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id8"><span class="brackets">1</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Agrawal, R., Mannila, H., Srikant, R., Toivonen, H. and Verkamo, A. I. (1995). Fast discovery of association rules, Advances in Knowledge Discovery and Data Mining, AAAI/MIT Press, Cambridge, MA.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./14-Unsupervised-Learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="14.1-Introduction.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">14.1 导言</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="14.3-Cluster-Analysis.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">14.3 聚类分析</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>