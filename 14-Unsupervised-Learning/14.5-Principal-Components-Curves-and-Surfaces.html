
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>14.5 主成分，主曲线和主曲面 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="14.6 非负矩阵分解" href="14.6-Non-negative-Matrix-Factorization.html" />
    <link rel="prev" title="14.4 自组织图" href="14.4-Self-Organizing-Maps.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.3-Subset-Selection.html">
     3.3 子集的选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 光滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 光滑参数的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波光滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核光滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差、方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的乐观估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 “条件测试误差”还是“测试误差的期望”？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5 EM 算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆栈
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   主成分
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     例子：手写数字
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#procrustes">
     例子：Procrustes 转换和形状平均
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   主曲线和主曲面
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   核主成分
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   稀疏主成分
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>14.5 主成分，主曲线和主曲面<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>!!! note “更新笔记”
&#64;2018-01-18 完成第一小节（不包含例子），并完成<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/45">Ex. 14.7</a>
&#64;2018-01-19 完成主曲线（面）和谱聚类。</p>
<p>主成分已经在 <span class="xref myst">3.4.1 节</span>中讨论了，主成分阐释了岭回归的收缩机理。主成分是数据的一系列投影，互相不相关且按照方差大小排序。在下一节我们将要把主成分表示成逼近 <span class="math notranslate nohighlight">\(N\)</span> 个点 <span class="math notranslate nohighlight">\(x_i\in \mathbb{R}^p\)</span> 的 <strong>线性流形 (linear manifolds)</strong>。接着在 <a class="reference external" href="#_3">14.5.2 节</a>讨论非线性的推广。最近提出的关于非线性逼近流形的方法将在 <span class="xref myst">14.9 节</span>讨论。</p>
<p>!!! note “weiya 注：流形学习”
参考<a class="reference external" href="https://www.zhihu.com/question/24015486/answer/26524937">&#64;Jason Gu</a>的知乎回答,</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&gt; 流形学习（manifold learning）是机器学习、模式识别中的一种方法，在维数约简方面具有广泛的应用。它的主要思想是将高维的数据映射到低维，使该低维的数据能够反映原高维数据的某些本质结构特征。流形学习的前提是有一种假设，即某些高维数据，实际是一种低维的流形结构嵌入在高维空间中。流形学习的目的是将其映射回低维空间中，揭示其本质。
</pre></div>
</div>
<div class="section" id="id2">
<h2>主成分<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p><span class="math notranslate nohighlight">\(\mathbb{R}^p\)</span> 中数据的主成分给出了这些数据在秩 <span class="math notranslate nohighlight">\(q\le p\)</span> 下最好的线性逼近。</p>
<p>记观测值为 <span class="math notranslate nohighlight">\(x_1,x_2,\ldots,x_N\)</span>，然后考虑用秩为 <span class="math notranslate nohighlight">\(q\)</span> 的线性模型来表示它们</p>
<div class="math notranslate nohighlight">
\[
f(\lambda)=\mu+\mathbf V_q\lambda\tag{14.49}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\mu\)</span> 是 <span class="math notranslate nohighlight">\(\mathbb{R}^p \)</span>中的位置向量，<span class="math notranslate nohighlight">\(\mathbf V_q\)</span> 是有 <span class="math notranslate nohighlight">\(q\)</span> 个正交单位列向量的 <span class="math notranslate nohighlight">\(p\times q\)</span> 的矩阵，<span class="math notranslate nohighlight">\(\lambda\)</span> 是一个 <span class="math notranslate nohighlight">\(q\)</span> 维的参数向量。这是一个秩为 <span class="math notranslate nohighlight">\(q\)</span> 的仿射超平面的系数表示。图 14.20 和图 14.21 分别展示了<span class="math notranslate nohighlight">\(q=1\)</span> 和 <span class="math notranslate nohighlight">\(q=2\)</span> 的情形。对数据进行最小二乘拟合这个模型等价最小化 <strong>重构误差 (reconstruction error)</strong></p>
<div class="math notranslate nohighlight">
\[
\underset{\mu,\{\lambda_i\},\mathbf V_q}{\min}\sum\limits_{i=1}^N\Vert x_i-\mu-\mathbf V_q\lambda_i\Vert^2\tag{14.50}
\]</div>
<p>我们可以对上式关于 <span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\lambda_i\)</span>（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/45">练习 14.7</a>）求微分得到</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat\mu&amp;=\bar x\tag{14.51}\\
\hat\lambda_i&amp;=\mathbf V_q^T(x_i-\bar x)\tag{14.52}
\end{align*}
\end{split}\]</div>
<p>!!! note “weiya注: Ex. 14.7”
式（ 14.51 ） 和 式（ 14.52 ） 的解并不是唯一的，<span class="math notranslate nohighlight">\(\mu -\bar x\)</span> 属于 <span class="math notranslate nohighlight">\(\mathbf I-\V_q\V_q^T\)</span> 的 Null Space，而注意到 <span class="math notranslate nohighlight">\(\mathrm{rank}(\mathbf I-\V_q\V_q^T) = p-q &gt;0\)</span>（除非 <span class="math notranslate nohighlight">\(q=p\)</span>），所以解不唯一。
具体解题过程参见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/45">Issue: Ex. 14.7</a></p>
<p>接下来需要去寻找正交矩阵 <span class="math notranslate nohighlight">\(\mathbf V_q\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\underset{\mathbf V_q}{\min}\sum\limits_{i=1}^N\Vert (x_i-\bar x)-\mathbf V_q\mathbf V_q^T(x_i-\bar x)\Vert^2\tag{14.53}
\]</div>
<p>为了方便，我们假设 <span class="math notranslate nohighlight">\(\bar x=0\)</span>（否则我们只需要简单地对数据进行中心化 <span class="math notranslate nohighlight">\(\tilde x_i=x_i-\bar x\)</span>）。<span class="math notranslate nohighlight">\(p\times p\)</span> 矩阵 <span class="math notranslate nohighlight">\(\mathbf H_q=\mathbf V_q\mathbf V_q^T\)</span> 是 <strong>投影矩阵 (projection matrix)</strong>，并且将每个点 <span class="math notranslate nohighlight">\(x_i\)</span> 投影到它的秩为 <span class="math notranslate nohighlight">\(q\)</span> 的重构 <span class="math notranslate nohighlight">\(\mathbf H_qx_i\)</span> 上，这是 <span class="math notranslate nohighlight">\(x_i\)</span> 在由 <span class="math notranslate nohighlight">\(\mathbf V_q\)</span> 的列张成的子空间上的正交投影。</p>
<p>!!! note “weiya 注：投影矩阵”
投影是从一个向量空间到其自身的线性变换，并且投影矩阵满足<span class="math notranslate nohighlight">\(\mathbf P^2=\mathbf P\)</span>。
<img alt="" src="../_images/projection_20200515_064251115_iOS.png" />
首先，根据<span class="math notranslate nohighlight">\(\mathbf H_qx_i=\mathbf V_q\mathbf V^T_qx_i\)</span>可以得出投影点是在<span class="math notranslate nohighlight">\(\mathbf V_q\)</span>的列所张成的子空间中；其次，对于该子空间中任一点 <span class="math notranslate nohighlight">\(\mathbf V_qy\)</span>，因为
$<span class="math notranslate nohighlight">\(
    \begin{align*}
    (x_i-\mathbf H_qx_i)\cdot \mathbf V_qy_i&amp;=(\mathbf I-\mathbf H_q)x_i\cdot \mathbf V_qy_i\\
    &amp;=x_i^T(\mathbf I-\mathbf H_q)^T\mathbf V_qy_i\\
    &amp;=x_i^T\mathbf O_{p\times q}y_i\\
    &amp;=0
    \end{align*}
    \)</span>$
故为正交投影。</p>
<p>式（ 14.53 ） 的解可以按如下形式表示。将（中心化的）观测值放进 <span class="math notranslate nohighlight">\(N\times p\)</span> 的矩阵 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 的行中。构造 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 的奇异值分解：</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X=UDV^T}\tag{14.54}
\]</div>
<p>这是数值分析中标准的分解，并且对该分解有很多的算法（比如，Golub and Van Loan, 1983<a class="footnote-reference brackets" href="#id15" id="id3">1</a>）。这里 <span class="math notranslate nohighlight">\(\mathbf U\)</span> 是<span class="math notranslate nohighlight">\(N\times p\)</span> 的正交矩阵 (<span class="math notranslate nohighlight">\(\mathbf{U^TU}=\mathbf I_p\)</span>)，它的列向量 <span class="math notranslate nohighlight">\(\mathbf u_j\)</span> 称为 <strong>左奇异向量 (left singular vectors)</strong>，<span class="math notranslate nohighlight">\(\mathbf V\)</span> 是<span class="math notranslate nohighlight">\(p\times p\)</span> 的正交矩阵 (<span class="math notranslate nohighlight">\(\mathbf V^T\mathbf V=\mathbf I_p\)</span>)，其中的列向量 <span class="math notranslate nohighlight">\(\mathbf v_j\)</span> 称之为 <strong>右奇异向量 (right singular vectors)</strong>。对每个秩 <span class="math notranslate nohighlight">\(q\)</span>，式（ 14.53 ） 的解 <span class="math notranslate nohighlight">\(\mathbf V_q\)</span> 包含 <span class="math notranslate nohighlight">\(\mathbf V\)</span> 的前 <span class="math notranslate nohighlight">\(q\)</span> 列。<span class="math notranslate nohighlight">\(\mathbf{UD}\)</span> 的列称为 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 的主成分（见 <a class="reference external" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions/index.html#_1">3.5.1 节</a>）。式（ 14.52 ） 中 <span class="math notranslate nohighlight">\(N\)</span> 个最优的 <span class="math notranslate nohighlight">\(\hat\lambda_i\)</span> 由前 <span class="math notranslate nohighlight">\(q\)</span> 个主成分给出（<span class="math notranslate nohighlight">\(N\times q\)</span> 的矩阵 <span class="math notranslate nohighlight">\(\mathbf U_q\mathbf D_q\)</span> 的 <span class="math notranslate nohighlight">\(N\)</span> 个行向量）。</p>
<p>图 14.20 展示了 <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> 中的一维主成分分析。</p>
<p><img alt="" src="../_images/fig14.20.png" /></p>
<p>对于每个数据点 <span class="math notranslate nohighlight">\(x_i\)</span>，在直线上有个离它最近的点，由 <span class="math notranslate nohighlight">\(u_{i1}d_1v_1\)</span> 给出。这里 <span class="math notranslate nohighlight">\(v_1\)</span> 是该直线的方向，并且 <span class="math notranslate nohighlight">\(\hat \lambda_i=u_{i1}d_1\)</span> 衡量了沿着直线离原点的距离。类似地，图 14.21 展示了拟合 half-sphere 数据的二维主成分曲面（左图）。右图显示了数据在前两个主成分上的投影。这个投影是之前介绍的 SOM 方法的初始化的基础。这个过程在分离簇方面表现得非常成功。因为 half-sphere 是非线性的，非线性的投影会做得更好，这将是下一节的主题。</p>
<p><img alt="" src="../_images/fig14.21.png" /></p>
<p>主成分还有许多其它的性质，举个例子，线性组合 <span class="math notranslate nohighlight">\(\mathbf Xv_1\)</span> 在特征的所有线性组合中有最大的方差；<span class="math notranslate nohighlight">\(\mathbf Xv_2\)</span> 在满足<span class="math notranslate nohighlight">\(v_2\)</span> 正交 <span class="math notranslate nohighlight">\(v_1\)</span> 的所有线性组合中有最大的方差，以此类推。</p>
<div class="section" id="id4">
<h3>例子：手写数字<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>主成分是降低和压缩维度的有效工具。我们用第一章中描述的手写数字的例子来说明这个特点。图 14.22 显示了从 658 个 ‘3’ 中抽取的 130 个 ‘3’ 的样本，每一个都是 <strong>数字化的 (digitized)</strong> <span class="math notranslate nohighlight">\(16\times 16\)</span> 的灰度图象。我们看到书写风格、字体粗细以及字体方向上有显著差异。我们将这些图象看成是 <span class="math notranslate nohighlight">\(\mathbb{R}^{256}\)</span> 中的点 <span class="math notranslate nohighlight">\(x_i\)</span>，并且通过 SVD 式（ 14.54 ） 来计算它们的主成分。</p>
<p><img alt="" src="../_images/fig14.22.png" /></p>
<p>图 14.23 显示了这些数据的前两个主成分。</p>
<p><img alt="" src="../_images/fig14.23.png" /></p>
<p>对于前两个主成分 <span class="math notranslate nohighlight">\(u_{i1}d_1\)</span> 和 <span class="math notranslate nohighlight">\(u_{i2}d_2\)</span>，我们计算 5%, 25%, 50%, 75%, 95% 分位数，并且用它们去定义叠加在图中的长方形网格。圆点表明靠近该网格顶点的图象，而距离主要用这些投影点的坐标来衡量，但也给正交子空间中的组分一些权重。右图显示了对应这些圆点的图象。这帮助我们观察前两个主成分的本质。我们看到 <span class="math notranslate nohighlight">\(v_1\)</span>（水平方向）主要与手写‘3’的下尾有关，而 <span class="math notranslate nohighlight">\(v_2\)</span>（垂直方向）与字体粗细有关。用 式（ 14.49 ） 的参数化模型表示，这两个组分的模型有如下形式</p>
<!--
$$
\begin{align*}
\hat f(\lambda)&=\bar x+\lambda_1b_1+\lambda_2v_2\\
&=\includegraphics[height=5.6ex]{../img/14/s1.png}+\lambda_1\cdot
\includegraphics[height=5.6ex]{../img/14/s2.png}+\lambda_2\cdot
\includegraphics[height=5.6ex]{../img/14/s3.png}
\end{align*}
$$
-->
<p><img alt="" src="../_images/eq1455.png" /></p>
<p>这里我们以图象形式展示了前两个主成分的方向，<span class="math notranslate nohighlight">\(v_1\)</span> 和 <span class="math notranslate nohighlight">\(v_2\)</span>。尽管有 256 个可能的主成分，但大约 50 个主成分解释了 90% 的方差，12 个主成分解释了 63% 的方差。</p>
<p><img alt="" src="../_images/fig14.24.png" /></p>
<p>图 14.24 比较了奇异值和相等大小的不相关数据的奇异值，后者通过对 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 的每一列进行随机扰动得到。</p>
<p>!!! note “weiya 注：奇异值”
在 SVD 分解中，<span class="math notranslate nohighlight">\(\mathbf{D}\)</span> 为 <span class="math notranslate nohighlight">\(p\times p\)</span> 的对角矩阵，对角元 <span class="math notranslate nohighlight">\(d_1\ge d_2 \ge \cdots \ge d_p \ge 0\)</span> 称作 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的奇异值。如果一个或多个 <span class="math notranslate nohighlight">\(d_j=0\)</span>，则 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 为奇异的。</p>
<p>数字图象中的像素点本质上是相关的，而且因为所有这些图象都是同一个数字，因此相关性甚至更强。相对小的主成分子集可以看成表示高维数据的极佳低维特征。</p>
</div>
<div class="section" id="procrustes">
<h3>例子：Procrustes 转换和形状平均<a class="headerlink" href="#procrustes" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/fig14.25.png" /></p>
<p>图 14.25 在同一张图中展示了两个集合的点，橘黄色和绿色。这个例子中，这些点表示手写’S’的两个数字化版本，这是从”Suresh”签名中提取的。图 14.26 展示了整个签名（第三和第四幅图）。这些签名是采用 touch-screen 设备（超市中很常见的设备）动态采集的。每个 <span class="math notranslate nohighlight">\(S\)</span> 用 <span class="math notranslate nohighlight">\(N=96\)</span> 个点来表示，记为 <span class="math notranslate nohighlight">\(N\times 2\)</span> 的矩阵 <span class="math notranslate nohighlight">\(\mathbf X_1\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf X_2\)</span>。这些点之间存在对应关系，<span class="math notranslate nohighlight">\(\mathbf X_1\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf X_2\)</span> 的第 <span class="math notranslate nohighlight">\(i\)</span> 行表示沿着两个’S’的同一位置。用 <strong>形态测量 (morphometrics)</strong> 的术语说就是这些点表示两个物体的 landmarks。怎样寻找这样一个 landmark 一般是很困难的，而且因情况而异。在这里，我们采用沿着每个签名的速度信号的 dynamic time warping (Hastie et al., 1992<a class="footnote-reference brackets" href="#id16" id="id5">2</a>)，但是在这里不展开讨论。</p>
<p>!!! note “weiya 注：warping function”
听过的一个报告 <a class="reference external" href="https://stats.hohoweiya.xyz/2020/01/21/registration/">Registration Problem in Functional Data Analysis</a>，其中介绍到为了将曲线 <span class="math notranslate nohighlight">\(w_i\)</span> 匹配到参考曲线 <span class="math notranslate nohighlight">\(w_j\)</span>，我们需要一个 <strong>翘曲函数 (warping function)</strong> <span class="math notranslate nohighlight">\(h_i(t)\)</span> 使得
$<span class="math notranslate nohighlight">\(
    w_i^\mathrm{st}ar(t) = w_i[h_i(t)]
    \)</span><span class="math notranslate nohighlight">\(
    以及
    \)</span><span class="math notranslate nohighlight">\(
    w_i^\mathrm{st}ar(t_{j,\text{landmark}}) \approx w_j(t_{j,\text{landmark}})\,.
    \)</span>$</p>
<p>右图中，我们已经对绿色点已经采用了 <strong>平移 (translation)</strong> 和 <strong>旋转 (rotation)</strong> 的方式来尽可能与橘黄色点匹配——这称之为 Procrustes 变换（如，Mardia et al., 1979<a class="footnote-reference brackets" href="#id17" id="id6">3</a>）。</p>
<p>!!! note “weiya 注：Procrustes 变换”
<strong>普罗库鲁斯提斯 (Procrustes)</strong> 是希腊神话中非洲的一个土匪，他经常用一个铁床来折磨别人，把抓来的人绑在铁床上，然后根据铁床的长度来裁剪他们的身体长度：那些身材短的人被拉长，那些身材长的人被砍掉多余的部分。</p>
<p>考虑下面的问题：</p>
<div class="math notranslate nohighlight">
\[
\underset{\mu, \mathbf R}{\min}\Vert \mathbf X_2-(\mathbf X_1\mathbf R+\boldsymbol 1\mu^T)\Vert_F\tag{14.56}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mathbf X_1\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf X_2\)</span> 都是对应点的 <span class="math notranslate nohighlight">\(N\times p\)</span> 矩阵，<span class="math notranslate nohighlight">\(\mathbf R\)</span> 是标准正交 <span class="math notranslate nohighlight">\(p\times p\)</span> 的矩阵，<span class="math notranslate nohighlight">\(\mu\)</span> 是 <span class="math notranslate nohighlight">\(p\)</span> 维的位置向量。</p>
<p>!!! note “原书注：<span class="math notranslate nohighlight">\(\mathbf R\)</span>”
为了简化问题，只考虑包含反射和旋转的正交矩阵（<span class="math notranslate nohighlight">\(O(p)\)</span>群）；尽管这里不可能有反射，这些方法可以进一步限制为只允许旋转（<span class="math notranslate nohighlight">\(SO(p)\)</span>群）。</p>
<p>这里 <span class="math notranslate nohighlight">\(\Vert \mathbf X\Vert_F^2=\mathrm{trace}(\mathbf X^T\mathbf X)\)</span> 是 Frobenius 矩阵范数的平方。</p>
<p>令 <span class="math notranslate nohighlight">\(\bar x_1\)</span> 和 <span class="math notranslate nohighlight">\(\bar x_2\)</span> 是矩阵的列均值向量，<span class="math notranslate nohighlight">\(\tilde{\mathbf X}_1\)</span> 和 <span class="math notranslate nohighlight">\(\tilde{\mathbf X}_2\)</span> 是这些矩阵减去均值得到的。考虑 SVD 分解 <span class="math notranslate nohighlight">\(\tilde {\mathbf X}_1^T\tilde{\mathbf X}_2=\mathbf U\mathbf D\mathbf V^T\)</span>。则 式（ 14.56 ） 的解由下式给出（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/46">练习 14.8</a>）</p>
<p>!!! info “weiya 注：Ex. 14.8”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/46">Issue 46: Ex. 14.8</a>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\mathbf R}&amp;=\mathbf U\mathbf V^T\\
\hat\mu&amp;=\bar x_2-\hat{\mathbf R}\bar x_1
\end{align*}
\tag{14.57}
\end{split}\]</div>
<p>并且这个最小距离被称为 Procrustes 距离。从该解的形式来看，我们可以将每个矩阵在其列中心点处进行中心化，接着完全忽略掉位置向量。下文假设是这种情形。</p>
<p><strong>带尺度的 Procrustes 距离 (Procrustes distance with scaling)</strong> 解决了更一般的问题</p>
<div class="math notranslate nohighlight">
\[
\underset{\beta,\mathbf R}{\min}\Vert \mathbf X_2-\beta\mathbf X_1\mathbf R\Vert_F\tag{14.58}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\beta&gt;0\)</span> 是正的标量值。<span class="math notranslate nohighlight">\(\mathbf R\)</span> 的解和前面一样，<span class="math notranslate nohighlight">\(\hat\beta=\mathrm{trace}(D)/\Vert \mathbf X_1\Vert_F^2\)</span>.</p>
<p>与 Procrustes 距离有关的是 <span class="math notranslate nohighlight">\(L\)</span> 个形状的 Procrustes 平均，它解决了下面的问题</p>
<div class="math notranslate nohighlight">
\[
\underset{\{\mathbf R_\ell\}_1^L,M}{\min}\sum\limits_{\ell=1}^L\Vert \mathbf X_\ell \mathbf R_\ell -\mathbf M\Vert_F^2\tag{14.59}
\]</div>
<p>也就是，寻找到所有形状的平均 Procrustes 距离平方最近的形状 <span class="math notranslate nohighlight">\(\mathbf M\)</span>。这可以通过简单的算法实现：</p>
<ol class="simple">
<li><p>初始化 <span class="math notranslate nohighlight">\(\mathbf M=\mathbf X_1\)</span>（举个例子）</p></li>
<li><p>固定 <span class="math notranslate nohighlight">\(\mathbf M\)</span>，求解 <span class="math notranslate nohighlight">\(L\)</span> 个 Procrustes 旋转问题，得到 <span class="math notranslate nohighlight">\(\mathbf X_\ell'\leftarrow \mathbf X\hat{\mathbf R}_\ell\)</span></p></li>
<li><p>令 <span class="math notranslate nohighlight">\(\mathbf M\leftarrow \frac 1L\sum\limits_{\ell=1}^L\mathbf X_\ell'\)</span></p></li>
</ol>
<p>重复步骤 2 和 3 准则直至 式（ 14.59 ） 收敛。</p>
<p><img alt="" src="../_images/fig14.26.png" /></p>
<p>图 14.26 显示了三个形状的简单例子。注意到我们仅仅希望得到旋转的一个解；另外，我们加上约束，使得 <span class="math notranslate nohighlight">\(\mathbf M\)</span> 是上三角形式，来强制解是唯一的。我们可以很简单地把缩放合并到定义 式（ 14.59 ）；见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/47">练习 14.9</a>。</p>
<p>!!! info “weiya 注：Ex. 14.9”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/47">Issue 47: Ex. 14.9</a>.</p>
<p>更一般地，我们可以通过下式来定义一系列形状的 affine-invariant 平均：</p>
<div class="math notranslate nohighlight">
\[
\underset{\{\mathbf A_\ell\}_1^L,\mathbf M}{\min}\sum\limits_{\ell=1}^L\Vert\mathbf X_\ell\mathbf A_\ell-\mathbf M\Vert_F^2\tag{14.60}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mathbf A_\ell\)</span> 是任意 <span class="math notranslate nohighlight">\(p\times p\)</span> 的非奇异矩阵。这里我们要求标准化，使得 <span class="math notranslate nohighlight">\(\mathbf M^T\mathbf M=\mathbf I\)</span>，来避免平凡解。这个解是吸引人的，并且可以不用迭代便可以计算（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/48">练习 14.10</a>）：</p>
<ol class="simple">
<li><p>令 <span class="math notranslate nohighlight">\(\mathbf H_\ell=\mathbf X_\ell(\mathbf X_\ell^T\mathbf X_\ell)^{-1}\mathbf X_\ell^T\)</span> 为由 <span class="math notranslate nohighlight">\(\mathbf X_\ell\)</span> 定义的秩为 <span class="math notranslate nohighlight">\(p\)</span> 的投影矩阵</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf M\)</span> 是 <span class="math notranslate nohighlight">\(N\times p\)</span> 的矩阵，其由 <span class="math notranslate nohighlight">\(\bar{\mathbf H}=\frac{1}{L}\sum\limits_{\ell=1}^L\mathbf H_\ell\)</span> 的最大 <span class="math notranslate nohighlight">\(p\)</span> 个特征向量所构成</p></li>
</ol>
<p>!!! info “weiya 注：Ex. 14.10”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/48">Issue 48: Ex. 14.10</a>.</p>
</div>
</div>
<div class="section" id="id7">
<h2>主曲线和主曲面<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>主曲线推广了主成分直线，用一维光滑曲线来近似 <span class="math notranslate nohighlight">\(\mathbb{R}^p\)</span> 中的数据点。主曲面更一般化，它给出了二维或更高维的流形近似。</p>
<p>我们首先定义随机变量 <span class="math notranslate nohighlight">\(X\in \mathbb{R}^p\)</span> 的主曲线，然后讨论有限数据的情形。令 <span class="math notranslate nohighlight">\(f(\lambda)\)</span> 为 <span class="math notranslate nohighlight">\(\mathbb{R}^p\)</span> 中参数化的光滑曲线。因此 <span class="math notranslate nohighlight">\(f(\lambda)\)</span> 是有着 <span class="math notranslate nohighlight">\(p\)</span> 个坐标的向量函数，每个都是关于单参数 <span class="math notranslate nohighlight">\(\lambda\)</span> 的光滑函数。举个例子，可以选择参数 <span class="math notranslate nohighlight">\(\lambda\)</span> 为沿着曲线到固定原点的弧长。对于每个数据点 <span class="math notranslate nohighlight">\(x\)</span>，令 <span class="math notranslate nohighlight">\(\lambda_f(x)\)</span> 为曲线上离 <span class="math notranslate nohighlight">\(x\)</span> 最近的点。如果满足</p>
<div class="math notranslate nohighlight">
\[
f(\lambda) = \mathbb{E}(X\mid \lambda_f(X)=\lambda)\tag{14.61}
\]</div>
<p>则 <span class="math notranslate nohighlight">\(f(\lambda)\)</span> 称为随机向量 <span class="math notranslate nohighlight">\(X\)</span> 的分布的主曲线。这也就是说 <span class="math notranslate nohighlight">\(f(\lambda)\)</span> 是投影到曲线上的所有数据点的平均，这些点也称为有“责任”的点。这也称作 self-consistency 性质。尽管在实际中，多元连续随机变量的分布有无穷多个主曲线(Duchamp and Stuetzle, 1996<a class="footnote-reference brackets" href="#id18" id="id8">4</a>)，但是我们主要对光滑的主曲线感兴趣。图 14.27 展示了一个主曲线。</p>
<p><img alt="" src="../_images/fig14.27.png" /></p>
<p>!!! note “weiya 注：”
主成分可以看成是主曲线的特殊情形。
首先，式（ 14.49 ） 可以写成
$<span class="math notranslate nohighlight">\(
    f(\lambda) = [\mu + \V_{q1}\lambda, \mu + \V_{q2}\lambda, \ldots, \mu + \V_{qp}\lambda]\,,
    \)</span><span class="math notranslate nohighlight">\(
    其中 \)</span>\V_{qi}<span class="math notranslate nohighlight">\( 是 \)</span>\V_q<span class="math notranslate nohighlight">\( 的第 \)</span>i$ 个行向量.
其次，对比图 14.20 和图 14.27，</p>
<p><strong>主点 (Principal points)</strong> 是与之相关的一个有趣的概念。考虑含 <span class="math notranslate nohighlight">\(k\)</span> 个原型的集合，对于在分布的支撑集中的每个点 <span class="math notranslate nohighlight">\(x\)</span>，选出最近的原型，也就是，为之负责的那个原型。这导出了对特征空间的划分，得到 Voronoi 区域。这 <span class="math notranslate nohighlight">\(k\)</span> 个点最小化了 <span class="math notranslate nohighlight">\(X\)</span> 到其原型的期望距离，它们称为该分布的主点。每个主点是 self-consistent，因为它等于其 Voronoi 区域的 <span class="math notranslate nohighlight">\(X\)</span> 的均值。举个例子，当 <span class="math notranslate nohighlight">\(k=1\)</span>，一个 <strong><a class="reference external" href="https://en.wikipedia.org/wiki/Von_Mises_distribution">圆形正态分布</a></strong> 的主点是均值向量；当 <span class="math notranslate nohighlight">\(k=2\)</span> 时，成对的点对称排列在通过均值向量的射线上。主点类似 <span class="math notranslate nohighlight">\(K\)</span>-means 聚类中的重心的分布。主曲线可以看成是 <span class="math notranslate nohighlight">\(k=\infty\)</span> 时的主点，但是限制为光滑曲线，用类似的方式，SOM 限制 K-means 聚类中心的落在一个光滑流形上。</p>
<p>!!! question “weiya 注：”
<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/197">Issue 197: Self-consistency of pricipal points? Reasons for k = 1, 2, 3? </a></p>
<p>为了寻找某分布的主曲线 <span class="math notranslate nohighlight">\(f(\lambda)\)</span>，我们考虑坐标函数 <span class="math notranslate nohighlight">\(f(\lambda)=[f_1(\lambda),f_2(\lambda),\ldots, f_p(\lambda)]\)</span>，并且令<span class="math notranslate nohighlight">\(X^T=(X_1, X_2,\ldots, X_p)\)</span>。考虑下面的轮换过程：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
(a) &amp; \hat{f_j}(\lambda)\leftarrow \mathbb{E}(X_j\mid \lambda(X)=\lambda); \;j=1,2,\ldots, p\\
(b) &amp; \hat \lambda_f(x)\leftarrow \argmin_{\lambda'}\Vert x-\hat f(\lambda')\Vert^2
\end{align*}
\tag{14.62} 
\end{split}\]</div>
<p>第一个等式固定 <span class="math notranslate nohighlight">\(\lambda\)</span>，并且加上 self-consistentcy 的要求 式（ 14.61 ）。第二个等式固定曲线，并在曲线上寻找距离每个点最近的点。在有限的数据情形下，主曲线算法以线性主成分开始，迭代 式（ 14.62 ） 中的两步直至收敛。散点图光滑器用于估计步骤 (a) 中的条件期望，这通过将每个 <span class="math notranslate nohighlight">\(X_j\)</span> 看成关于弧长 <span class="math notranslate nohighlight">\(\hat \lambda(X)\)</span> 的函数来光滑，而且 (b) 中的投影对于每个观测数据点来实现。证明一般情况下的收敛是很困难的，但是可以证明如果散点图光滑中采用线性最小二乘拟合，则该过程将会收敛至第一线性主成分，这等价寻找矩阵最大特征值的幂法。</p>
<p>主曲面与主曲线有着完全相同的形式，不过是在更高维度下的。使用最普遍的是二维主曲面，其坐标函数为</p>
<div class="math notranslate nohighlight">
\[
f(\lambda_1,\lambda_2)=[f_1(\lambda_1,\lambda_2),\ldots, f_p(\lambda_1, \lambda_2)]
\]</div>
<p>上述步骤(a)中的估计通过二维曲面光滑器得到。维数大于2的主曲面很少用到，因为在高维光滑的可视化不是很吸引人。</p>
<p><img alt="" src="../_images/fig14.28.png" /></p>
<p>图 14.28 展示了对 half-sphere 数据进行主曲面光滑的结果。图中将数据点看成是估计的非线性坐标 <span class="math notranslate nohighlight">\(\hat \lambda_1(x_i), \hat \lambda_2(x_i)\)</span> 的函数。图中的类别划分是很显然的。</p>
<p>!!! note “weiya 注：Recall”
$<span class="math notranslate nohighlight">\(
    m_j=\frac{\sum w_kx_k}{\sum w_k}\tag{14.48}
    \)</span>$</p>
<p>主曲面非常类似 <strong>自组织图 (self-organizing maps)</strong>。如果我们采用核曲面光滑器来估计坐标函数 <span class="math notranslate nohighlight">\(f_j(\lambda_1,\lambda_2)\)</span>，这与 SOMs 的 batch 版本 式（ 14.48 ） 有着相同的形式。SOM 的权重 <span class="math notranslate nohighlight">\(w_k\)</span> 恰恰是核的权重。然而，有一个区别，主曲面估计对每个数据点 <span class="math notranslate nohighlight">\(x_i\)</span> 估计单独的原型 <span class="math notranslate nohighlight">\(f(\lambda_1(x_i),\lambda_2(x_i))\)</span>，而 SOM 会在所有数据中间共享一小部分的原型点。结果是，SOM 与主曲面仅仅当 SOM 原型的个数非常大时两者才一致。</p>
<p>两者之间还有一个概念上的区别。主曲面给出了关于坐标函数的整个流形的光滑参量化，而 SOMs 是离散的并且仅仅产生近似数据的那些估计的原型。主曲面的光滑参量化保持局部的距离：在图 14.28 中，红色聚类点比绿色或蓝色聚类点更紧凑。</p>
<p>!!! note “weiya 注：Recall”
对于 SOM, 因为没有使用二维的距离，没有迹象能表明 SOM 投射中关于红色簇比其它的簇更紧。</p>
<p>在简单的例子中，估计的坐标函数本身是可以知道的：见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/49">练习 14.13</a>。</p>
<p>!!! info “weiya 注：Ex. 14.13”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/49">Issue 49: Ex. 14.13</a>.</p>
<p>##　谱聚类</p>
<p>像 K-means 这样传统的聚类方法采用 spherical 或者 elliptical 度量来对数据点进行划分。因此当簇是非凸的时候效果并不好，比如图 14.29 中左上角的同心圆。</p>
<p><img alt="" src="../_images/fig14.29.png" /></p>
<p>谱聚类是标准聚类方法的推广，而且也是为这些情形所设计的。它与局部多维缩放技巧有着紧密的联系（<span class="xref myst">14.9 节</span>）。</p>
<p>出发点是所有观测点对间的成对相似性 <span class="math notranslate nohighlight">\(s_{ii'}\ge 0\)</span> 构成的 <span class="math notranslate nohighlight">\(N\times N\)</span> 矩阵。我们将这些观测用无向相似性图 <span class="math notranslate nohighlight">\(G=\langle V, E \rangle\)</span> 来表示。<span class="math notranslate nohighlight">\(N\)</span> 个顶点 <span class="math notranslate nohighlight">\(v_i\)</span> 表示观测值，如果成对顶点的相似性为正值（或者超出某个阈值），则它们之间用一条边相连。边的权重为 <span class="math notranslate nohighlight">\(s_{ii'}\)</span>。我们希望对这个图进行划分，使得不同类之间的边有较低的权重，而在类间有着较高的权重。在谱聚类中，思想是构造相似性图来表示观测点间的局部邻居关系。</p>
<p>更精确地，考虑 <span class="math notranslate nohighlight">\(N\)</span> 个点 <span class="math notranslate nohighlight">\(x_i\in \mathbb{R}^p\)</span>，令 <span class="math notranslate nohighlight">\(d_{ii'}\)</span> 为 <span class="math notranslate nohighlight">\(x_i\)</span> 和 <span class="math notranslate nohighlight">\(x_{i'}\)</span> 间的欧几里得距离。我们将 <strong>radical-kernel gram</strong> 矩阵作为我们的相似性矩阵；也就是 <span class="math notranslate nohighlight">\(s_{ii'}=\exp(-d_{ii'}^2/c)\)</span>，其中 <span class="math notranslate nohighlight">\(c &gt; 0\)</span> 是缩放参数。</p>
<p>有许多方式来定义相似性矩阵及其反映局部行为的相似性图。最流行的方式是 <strong>mutual K-nearest-neighbor graph</strong>。定义 <span class="math notranslate nohighlight">\({\mathcal{N}}\_K\)</span> 为邻居点的对称子集；特别地，如果点 <span class="math notranslate nohighlight">\(i\)</span> 在 <span class="math notranslate nohighlight">\(i'\)</span> 的 <span class="math notranslate nohighlight">\(K\)</span>-最近邻中，则点对 <span class="math notranslate nohighlight">\((i,i')\)</span> 在 <span class="math notranslate nohighlight">\({\mathcal{N}}\_K\)</span> 中，反之亦然。接着我们连接所有的对称最近邻，然后给出边的权重 <span class="math notranslate nohighlight">\(w_{ii'}=s_{ii'}\)</span>；否则边的权重为 <span class="math notranslate nohighlight">\(0\)</span>。等价地，我们对不属于 <span class="math notranslate nohighlight">\(\mathcal{N}_K\)</span> 的点的成对相关性赋为 <span class="math notranslate nohighlight">\(0\)</span>，然后画出这个修改版本的矩阵的图。</p>
<p>另外，全连接图包含所有的成对边，权重为 <span class="math notranslate nohighlight">\(w_{ii'}=s_{ii'}\)</span>，局部行为通过缩放参数 <span class="math notranslate nohighlight">\(c\)</span> 来控制。</p>
<p>从相似图得到的边的矩阵 <span class="math notranslate nohighlight">\(\mathbf W=\\{w_{ii'}\\}\)</span> 称为 <strong>邻接矩阵 (adjacency matrix)</strong>。结点 <span class="math notranslate nohighlight">\(i\)</span> 的 <strong>度(degree)</strong> 为 <span class="math notranslate nohighlight">\(g_i=\sum_iw_{ii'}\)</span>，这是与该点相连的权重之和。令 <span class="math notranslate nohighlight">\(\mathbf G\)</span> 表示对角元为 <span class="math notranslate nohighlight">\(g_i\)</span> 的对角矩阵。</p>
<p>最后，graph laplacian 定义为</p>
<div class="math notranslate nohighlight">
\[
\mathbf{L=G-W}\tag{14.63}
\]</div>
<p>这称为未标准化的 graph lapacian，人们提出一系列标准化的版本——对 laplacian 关于结点的度进行标准化，举个例子，<span class="math notranslate nohighlight">\(\tilde{\mathbf L}=\mathbf I-\mathbf G^{-1}\mathbf W\)</span>。</p>
<p>谱聚类寻找 <span class="math notranslate nohighlight">\(\mathbf L\)</span> 最小的 <span class="math notranslate nohighlight">\(m\)</span> 个特征值对应的 <span class="math notranslate nohighlight">\(m\)</span> 个特征向量 <span class="math notranslate nohighlight">\(\mathbf Z_{N\times m}\)</span>（忽略平凡的常值特征向量）。采用如 K-means 的标准方法，我们可以对 <span class="math notranslate nohighlight">\(\mathbf Z\)</span> 的行聚类得到原始数据点的聚类。</p>
<p>图 14.29 展示了一个例子。左上图显示了 3 个圆形类别中的 450 个模拟数据点。K-means 聚类很明显对于簇外的点不容易进行分类。我们采用 10 最近邻相似图的谱聚类，并且左下图展示了对应 graph laplacian 的第二和第三最小特征值的特征向量。这两个特征向量找出了是哪个簇，并且特征向量矩阵 <span class="math notranslate nohighlight">\(\mathbf Y\)</span> 的行的散点图清晰地将簇分隔开。对变换后的点应用 K-means 聚类的过程同样能得到三个类。</p>
<p>为什么谱聚类有效？对于任意向量 <span class="math notranslate nohighlight">\(\mathbf f\)</span>，我们有</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf f^T\mathbf L\mathbf f&amp;=\sum\limits_{i=1}^Ng_if_i^2-\sum\limits_{i=1}^N\sum\limits_{i'=1}^Nf_if_{i'}w_{ii'}\\
&amp; = \frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{i'=1}^Nw_{ii'}(f_i-f_{i'})^2\tag{14.64}
\end{align*}
\end{split}\]</div>
<p>公式 式（ 14.64 ） 表明如果有较大邻接 <span class="math notranslate nohighlight">\(w_{ii'}\)</span> 的数据对的坐标 <span class="math notranslate nohighlight">\(f_i\)</span> 和 <span class="math notranslate nohighlight">\(f_{i'}\)</span> 很接近，则 <span class="math notranslate nohighlight">\(\mathbf f^T\mathbf L\mathbf f\)</span> 将达到较小的值。</p>
<p>因为对于任意的图 <span class="math notranslate nohighlight">\(\boldsymbol 1^T\mathbf L\boldsymbol 1=0\)</span>，常值向量是特征值为 0 的平凡解。如果图是连接的，这是唯一的 0 特征向量，这个结论并不是很显然（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/183">练习 14.21</a>）。推广这个结论，可以很简单地证明对于有 <span class="math notranslate nohighlight">\(m\)</span> 个连接组分的图，能重新排列结点使得 <span class="math notranslate nohighlight">\(\mathbf L\)</span> 是成块对角的，其中每个块是连接的组分。于是 <span class="math notranslate nohighlight">\(\mathbf L\)</span> 有 <span class="math notranslate nohighlight">\(m\)</span> 个特征值为 0 的特征向量，并且特征值为 0 的特征空间由连接组分的指示向量张成。实际上，连接有强有弱，则零特征值也可以用较小的特征值代替。</p>
<p>!!! info “weiya 注：Ex. 14.21”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/183">Issue 183: Ex. 14.21</a>.</p>
<p>谱聚类是寻找非凸簇的一种很有趣的方法。当采用标准化后的 graph laplacian 定义，有另外一种方式来看这种方法。定义 <span class="math notranslate nohighlight">\(\mathbf P=\mathbf G^{-1}\mathbf W\)</span>，我们考虑在图上以转移概率矩阵 <span class="math notranslate nohighlight">\(\mathbf P\)</span> 进行随机游走。则谱聚类得到随机游走中类与类之间不发生转移的点集。</p>
<p>在实际中应用谱聚类时必须要处理一系列的问题。我们必须选择相似图的类型——比如，全连接或者最近邻，以及相关的参数比如最近邻的个数 <span class="math notranslate nohighlight">\(k\)</span> 或者核的缩放参数 <span class="math notranslate nohighlight">\(c\)</span>。我们也必须选择从 <span class="math notranslate nohighlight">\(\mathbf L\)</span> 中提取的特征向量的个数，以及最后和所有聚类方法一样，选择簇的个数。在图 14.29 这一简单例子中，我们得到 <span class="math notranslate nohighlight">\(k\in [5, 200]\)</span> 中良好的结果，值为 200 的对应全连接图。当 <span class="math notranslate nohighlight">\(k &lt; 5\)</span>，结果变坏。观测图 14.29 的右上图，我们看到最小的三个特征值与剩余部分没有强烈的分离。因此选择多少个特征向量并不清楚。</p>
</div>
<div class="section" id="id9">
<h2>核主成分<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<p>谱聚类与 <strong>核主成分 (kernel principal components)</strong> 有关联，这是线性主成分的非线性版本。标准的线性主成分 (PCA) 可以通过协方差矩阵的特征向量得到，并且给出了数据有最大方差的方向。核主成分 (KPCA) 扩充了 PCA 的范围，模仿扩充特征时采用的非线性变换方法，然后在变换后的特征空间中应用 PCA。</p>
<p>在 <span class="xref myst">18.5.2 节</span>，我们展示了数据矩阵 <span class="math notranslate nohighlight">\(X\)</span> 的主成分变量 <span class="math notranslate nohighlight">\(Z\)</span> 可以通过内积矩阵 (gram 矩阵) <span class="math notranslate nohighlight">\(K=XX^T\)</span> 得到。具体地，对双重中心化的 gram 矩阵进行特征分解</p>
<div class="math notranslate nohighlight">
\[
\tilde K = (\I-\M)K(\I-\M) = \U\D^2\U^T\,,
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\M=11^T/N\)</span>, 然后我们有 <span class="math notranslate nohighlight">\(Z=\U\D\)</span>。<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/184">练习 18.15</a> 展示了怎么计算这个空间中新观测的投影。</p>
<p>!!! info “weiya 注：Ex. 18.15”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/184">Issue 184: Ex. 18.15</a>.</p>
<p>核主成分简单地模仿了这个过程，将核矩阵 <span class="math notranslate nohighlight">\(K=\\{K(x_i,x_{i'})\\}\)</span> 看成隐含特征 <span class="math notranslate nohighlight">\(\langle \phi(x_i),\phi(x_i')\rangle\)</span> 的内积矩阵，然后寻找其特征向量。第 <span class="math notranslate nohighlight">\(m\)</span> 个组分 <span class="math notranslate nohighlight">\(\z_m\)</span>（<span class="math notranslate nohighlight">\(Z\)</span> 的第 <span class="math notranslate nohighlight">\(m\)</span> 列）的元素（在忽略中心化的情况下）可以写成 <span class="math notranslate nohighlight">\(z_{im}=\sum_{j=1}^N\alpha_{jm}K(x_i,x_j)\)</span>，其中 <span class="math notranslate nohighlight">\(\alpha_{jm} = u_{jm}/d_m\)</span>（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/185">练习 14.16</a>）。</p>
<p>!!! info “weiya 注：Ex. 14.16”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/185">Issue 185: Ex. 14.16</a>。</p>
<p>将 <span class="math notranslate nohighlight">\(\z_m\)</span> 看成主成分函数 <span class="math notranslate nohighlight">\(g_m\in\cH_K\)</span> 在样本处的取值，其中 <span class="math notranslate nohighlight">\(\cH_K\)</span> 是由 <span class="math notranslate nohighlight">\(K\)</span> 生成的再生核希尔伯特空间（<span class="xref myst">5.8.1 节</span>），这可以帮助我们进一步理解核主成分。第一主成分函数 <span class="math notranslate nohighlight">\(g_1\)</span> 求解了</p>
<div class="math notranslate nohighlight">
\[
\max_{g_1\in\cH_K}\mathrm{Var}_{\cT}g_1(X) \text{ subject to } \Vert g_1\Vert_{\cH_K}=1\tag{14.66}.
\]</div>
<p>这里 <span class="math notranslate nohighlight">\(\mathrm{Var}_\cT\)</span> 表示在训练数据 <span class="math notranslate nohighlight">\(\cT\)</span> 上的样本方差。范数约束 <span class="math notranslate nohighlight">\(\Vert g_1\Vert_{\cH_K}=1\)</span> 控制了函数 <span class="math notranslate nohighlight">\(g_1\)</span> 的大小及光滑度，这由核 <span class="math notranslate nohighlight">\(K\)</span> 控制。在回归情形中，可以证明 式（ 14.66 ） 的解是有限维的，并且可以表示为 <span class="math notranslate nohighlight">\(g_1(x)=\sum_{j=1}^Nc_jK(x,x_j)\)</span>. <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/202">练习 14.17</a> 证明了这个解为 <span class="math notranslate nohighlight">\(\hat c_j=\alpha_{j1},j=1,\ldots,N\)</span>. 第二主成分函数也是类似定义的，但多了额外的限制 <span class="math notranslate nohighlight">\(\langle g_1,g_2\rangle_{\cH_K}=0\)</span>，以此类推。</p>
<p>!!! note “weiya 注：Ex. 14.17”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/202">Issue 202: Ex. 14.17</a>。</p>
<p>Schölkopf et al. (1999)<a class="footnote-reference brackets" href="#id19" id="id10">5</a> 演示了核主成分作为手写数字分类中的特征的应用，并且说明了相对于线性主成分，分类器的表现有改善。</p>
<p>注意到如果我们采用径向核</p>
<div class="math notranslate nohighlight">
\[
K(x,x')=\exp(-\Vert x-x'\Vert^2/c)\,\tag{14.67}
\]</div>
<p>则核矩阵 <span class="math notranslate nohighlight">\(K\)</span> 与谱聚类中的相似度矩阵 <span class="math notranslate nohighlight">\(\S\)</span> 有着相同的形式。边的权重矩阵 <span class="math notranslate nohighlight">\(W\)</span> 是 <span class="math notranslate nohighlight">\(K\)</span> 的局部化版本，将不是最近邻的成对点的相似度设为 0。</p>
<p>核主成分寻找 <span class="math notranslate nohighlight">\(\tilde K\)</span> 最大特征值对应的特征向量；这等价于寻找</p>
<div class="math notranslate nohighlight">
\[
\I - \tilde K\tag{14.68}
\]</div>
<p>的最小特征值对应的特征向量。这几乎与 Laplacian 式（ 14.63 ） 一样，区别在于 <span class="math notranslate nohighlight">\(\tilde K\)</span> 的中心化和 <span class="math notranslate nohighlight">\(\G\)</span> 对角元有结点的度。</p>
<p><img alt="" src="../_images/fig14.30.png" /></p>
<p>图 14.30 检验了在图 14.29 的小例子中，核主成分的表现效果。左上角我们使用 <span class="math notranslate nohighlight">\(c=2\)</span> 的径向核，跟谱聚类中使用的值一样。这并没有将类别分开，但是当 <span class="math notranslate nohighlight">\(c=10\)</span>（右上图），第一主成分能很好地将类别分开。在左下图采用谱聚类中的最近邻径向核 <span class="math notranslate nohighlight">\(W\)</span> 来应用核 PCA。在右下角我们用核矩阵作为谱聚类中构造 式（ 14.63 ） 的相似度矩阵。这两种情形都不能很好地把两个类别分开。调节 <span class="math notranslate nohighlight">\(c\)</span> 也没有帮助。</p>
<p>在这个小例子中，我们看到核主成分对于核的尺度以及本性很敏感。我们也看到核的最近邻截断对于谱聚类能否成功很重要。</p>
<p>!!! note “weiya 注：”
粗略地说，谱聚类要求最近邻截断，而核主成分要求中心化的核矩阵。</p>
</div>
<div class="section" id="id11">
<h2>稀疏主成分<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<p>我们经常通过查看方向向量 <span class="math notranslate nohighlight">\(v_j\)</span>，或者称 <strong>载荷 (loadings)</strong>，来判断哪个变量在起作用，进而对主成分进行解释。我们在 <span class="math notranslate nohighlight">\((14.55)\)</span> 中对图象载荷采用了这种方法。如果载荷是稀疏的，这种解释通常会很简单。这一节我们简要讨论能导出具有稀疏载荷的主成分方法。它们都是基于 lasso (<span class="math notranslate nohighlight">\(L_1\)</span>) 惩罚。</p>
<p>首先以 <span class="math notranslate nohighlight">\(N\times p\)</span> 的数据矩阵 <span class="math notranslate nohighlight">\(X\)</span> 开始，其中列进行了中心化。这些方法要么关注主成分的最大方差性质，要么最小重构误差。Joliffe et al. (2003)<a class="footnote-reference brackets" href="#id20" id="id12">6</a> 的 SCoTLASS procedure 采用第一种方法，求解</p>
<div class="math notranslate nohighlight">
\[
\max\; v^T(X^TX)v\,, \text{ subject to }\sum_{j=1}^p\vert v_j\vert\le t, v^Tv=1\,.\tag{14.69}
\]</div>
<p>绝对值约束促使某些载荷为 0，因此 <span class="math notranslate nohighlight">\(v\)</span> 是稀疏的。进一步，通过限制第 <span class="math notranslate nohighlight">\(k\)</span> 个主成分与前 <span class="math notranslate nohighlight">\(k-1\)</span> 个主成分正交来寻找稀疏的主成分。不幸的是，这个问题是非凸的，并且计算很困难。</p>
<p>Zou et al. (2006)<a class="footnote-reference brackets" href="#id21" id="id13">7</a> 而是以主成分的回归/重构性质开始，类似 <span class="xref myst">14.5.1</span> 的方法。令 <span class="math notranslate nohighlight">\(x_i\)</span> 为 <span class="math notranslate nohighlight">\(X\)</span> 的第 <span class="math notranslate nohighlight">\(i\)</span> 行。对于单个主成分，他们的稀疏主成分技巧解决了</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min_{\theta,v}\sum_{i=1}^N\Vert x_i-\theta v^Tx_i\Vert_2^2&amp;+\lambda \Vert v\Vert_2^2 + \lambda_1\Vert v\Vert_1\tag{14.70}\\
&amp;\text{subject to }\Vert \theta\Vert_2=1
\end{align*}
\end{split}\]</div>
<p>下面更具体地观察它的组成。</p>
<ul class="simple">
<li><p>如果 <span class="math notranslate nohighlight">\(\lambda\)</span> 和 <span class="math notranslate nohighlight">\(\lambda_1\)</span> 都是 0，并且 <span class="math notranslate nohighlight">\(N&gt;p\)</span>，易证 <span class="math notranslate nohighlight">\(v=\theta\)</span>，并且这是最大主成分方向。</p></li>
<li><p>当 <span class="math notranslate nohighlight">\(p&gt;&gt;N\)</span>，解不一定是唯一的，除了 <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>。对于任何 <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span> 以及 <span class="math notranslate nohighlight">\(\lambda_1=0\)</span>，<span class="math notranslate nohighlight">\(v\)</span> 的解与最大主成分方向成比例。</p></li>
<li><p><span class="math notranslate nohighlight">\(v\)</span> 的第二个惩罚鼓励载荷的稀疏性。</p></li>
</ul>
<p>对于多重组分，稀疏主成分过程最小化</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^N\Vert x_i-\bTheta\V^Tx_i\Vert^2+\lambda \sum_{k=1}^K\vert v_k\vert^2_2 + \sum_{k=1}^K\lambda_{1k}\Vert v_k\Vert_1\,,\tag{14.71}
\]</div>
<p>约束为 <span class="math notranslate nohighlight">\(\bTheta^T\bTheta=\I_K\)</span>。这里 <span class="math notranslate nohighlight">\(\V\)</span> 是 <span class="math notranslate nohighlight">\(p\times K\)</span> 的矩阵，其列向量为 <span class="math notranslate nohighlight">\(v_k\)</span>，<span class="math notranslate nohighlight">\(\bTheta\)</span> 也是 <span class="math notranslate nohighlight">\(p\times K\)</span>.</p>
<p>准则 式（ 14.71 ） 关于 <span class="math notranslate nohighlight">\(\V\)</span> 和 <span class="math notranslate nohighlight">\(\bTheta\)</span> 不是联合凸的，但是当固定一个参数，这关于另一参数是凸的。固定 <span class="math notranslate nohighlight">\(\bTheta\)</span> 然后对 <span class="math notranslate nohighlight">\(\V\)</span> 最小化等价于 <span class="math notranslate nohighlight">\(K\)</span> 个 elastic net 问题（<span class="xref myst">18.4 节</span>），并且可以有效地解决。另一方面，固定 <span class="math notranslate nohighlight">\(\V\)</span> 然后对 <span class="math notranslate nohighlight">\(\Theta\)</span> 最小化是 Procrustes 问题 式（ 14.56 ） 的一个版本，并且可以通过简单的 SVD 进行求解（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/186">练习 14.12</a>）。这些步骤交替进行直至收敛。</p>
<p>!!! info “weiya 注：Ex. 14.12”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/186">Issue 186: Ex. 14.12</a>。</p>
<p>图 14.31 展示了采用 式（ 14.71 ） 进行系数主成分分析的例子，取自 Sjöstrand et al. (2007)<a class="footnote-reference brackets" href="#id22" id="id14">8</a>。在一项涉及 569 名老人的研究中，<strong>胼胝体 (corpus callosum， CC)</strong> 的 <strong>矢状面横截面 (mid-sagittal cross-section)</strong> 的形状与不同的临床参数有关。这个例子中，对形状数据应用 PCA，这在形态学中是很流行的工具。对于这样的应用，沿着形状的外围，识别出一系列的标记 (landmarks)，图 14.32 展示了一个例子。</p>
<p><img alt="" src="../_images/fig14.31.png" /></p>
<p><img alt="" src="../_images/fig14.32.png" /></p>
<p>它们是通过允许旋转的 Procrustes 分析后继续对齐得到的，这里 Procrustes 还允许放缩。PCA 使用的特征是每个标记 (landmarks) 的坐标对的序列，放到单个向量中。</p>
<p>在这个分析中，标准主成分和稀疏主成分都进行了计算，并且识别出了显著与不同临床参数有关的组分。这张图像中，对应显著主成分（红色曲线）的形状变体画在了均值 CC 的上面。与 CC 有关的慢速步行在连接行动控制和大脑的感知中心的区域中更细（表现出萎缩症）。与 CC 有关的低语言流利度在连接听觉、视觉、认知中心的区域中更细。稀疏主成分过程对这一重要差异提供了一个更简洁，并且可能更有信息量的描绘。</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p>Golub, G. and Van Loan, C. (1983). Matrix Computations, Johns Hopkins University Press, Baltimore.</p>
</dd>
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id5">2</a></span></dt>
<dd><p>Hastie, T., Kishon, E., Clark, M. and Fan, J. (1992). A model for signature verification, Technical report, AT&amp;T Bell Laboratories. <a class="reference external" href="http://www-stat.stanford.edu/~hastie/Papers/signature.pdf">http://www-stat.stanford.edu/~hastie/Papers/signature.pdf</a> .</p>
</dd>
<dt class="label" id="id17"><span class="brackets"><a class="fn-backref" href="#id6">3</a></span></dt>
<dd><p>Mardia, K., Kent, J. and Bibby, J. (1979). Multivariate Analysis, Academic Press.</p>
</dd>
<dt class="label" id="id18"><span class="brackets"><a class="fn-backref" href="#id8">4</a></span></dt>
<dd><p>Duchamp, T. and Stuetzle, W. (1996). Extremal properties of principal curves in the plane, Annals of Statistics 24: 1511–1520.</p>
</dd>
<dt class="label" id="id19"><span class="brackets"><a class="fn-backref" href="#id10">5</a></span></dt>
<dd><p>Schölkopf, B., Smola, A. and M¨uller, K.-R. (1999). Kernel principal component analysis, in B. Sch¨olkopf, C. Burges and A. Smola (eds), Advances in Kernel Methods—Support Vector Learning, MIT Press, Cambridge, MA, USA, pp. 327–352.</p>
</dd>
<dt class="label" id="id20"><span class="brackets"><a class="fn-backref" href="#id12">6</a></span></dt>
<dd><p>Joliffe, I. T., Trendafilov, N. T. and Uddin, M. (2003). A modified principal component technique based on the lasso, Journal of Computational and Graphical Statistics 12: 531–547.</p>
</dd>
<dt class="label" id="id21"><span class="brackets"><a class="fn-backref" href="#id13">7</a></span></dt>
<dd><p>Zou, H., Hastie, T. and Tibshirani, R. (2006). Sparse principal component analysis, Journal of Computational and Graphical Statistics 15(2): 265–28.</p>
</dd>
<dt class="label" id="id22"><span class="brackets"><a class="fn-backref" href="#id14">8</a></span></dt>
<dd><p>Sjöstrand, K., Rostrup, E., Ryberg, C., Larsen, R., Studholme, C., Baezner, H., Ferro, J., Fazekas, F., Pantoni, L., Inzitari, D. and Waldemar, G. (2007). Sparse decomposition and modeling of anatomical shape variation, IEEE Transactions on Medical Imaging 26(12): 1625–1635.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./14-Unsupervised-Learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="14.4-Self-Organizing-Maps.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">14.4 自组织图</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="14.6-Non-negative-Matrix-Factorization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">14.6 非负矩阵分解</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>