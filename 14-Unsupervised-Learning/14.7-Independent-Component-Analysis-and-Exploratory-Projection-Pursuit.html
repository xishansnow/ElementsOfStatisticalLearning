
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>14.7 独立成分分析和探索投影寻踪 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="14.8 多维缩放" href="14.8-Multidimensional-Scaling.html" />
    <link rel="prev" title="14.6 非负矩阵分解" href="14.6-Non-negative-Matrix-Factorization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.3-Subset-Selection.html">
     3.3 子集的选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多重输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 光滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 光滑参数的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波光滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核光滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差，方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的 optimism
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.11-Bootstrap-Methods.html">
     7.11 自助法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 条件测试误差或期望测试误差？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.2-The-Bootstrap-and-Maximum-Likelihood-Methods.html">
     8.2 自助法和最大似然法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.4-Relationship-Between-the-Bootstrap-and-Bayesian-Inference.html">
     8.4 自助法和贝叶斯推断之间的关系
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5 EM 算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆栈
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   潜变量和因子分析
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   独立成分分析
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     例子: 手写数字
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     例子: 时序脑电图数据
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id12">
   探索投影寻踪
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id18">
   独立分量分析的一种直接方法
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id23">
     例子：模拟
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1>14.7 独立成分分析和探索投影寻踪<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>多元数据经常被看成是从未知来源多次间接测量的数据，一般不能被直接测量。例子有：</p>
<ul class="simple">
<li><p>在教育或心理学测试中，采用问卷调查来衡量潜在的智商以及其他的心理特征。</p></li>
<li><p>EEG 脑扫描通过放置在头部不同位置的感受器的电子信号来间接衡量脑的不同部分的神经元活性。</p></li>
<li><p>股票交易价格随着时间持续变化，并且反映了各种各样的未测量的因素，如 <strong>市场信心 (market confidence)</strong>，外部影响以及其他很难识别和测量的推动力。</p></li>
</ul>
<p><strong>因子分析(Factor Analysis)</strong> 是统计学领域为了识别潜在因素的一个经典方法。因子分析模型通常是用在高斯分布中，所以某种程度上阻碍了它的适用性。最近，<strong>独立成分分析(Independent Component Analysis)</strong> 成为了因子分析强劲的对手，我们将会看到，它的成功归功于潜在源信号的非高斯特征。</p>
<div class="section" id="id2">
<h2>潜变量和因子分析<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>!!! note “weiya 注：Recall”
$<span class="math notranslate nohighlight">\(
	\mathbf{X=UDV^T}\tag{14.54}\label{14.54}
	\)</span>$</p>
<p>奇异值分解 \eqref{14.54} <span class="math notranslate nohighlight">\(\mathbf X=\mathbf{UDV}^T\)</span> 可以表示成潜变量的形式。记 <span class="math notranslate nohighlight">\(\mathbf S=\sqrt{N}\mathbf U\)</span>，以及 <span class="math notranslate nohighlight">\(\mathbf A^T=\mathbf{DV}^T/\sqrt{N}\)</span>，我们有 <span class="math notranslate nohighlight">\(\mathbf{X=SA}^T\)</span>，因此 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 的每一列是 <span class="math notranslate nohighlight">\(\mathbf S\)</span> 的列的线性组合。现在因为 <span class="math notranslate nohighlight">\(\mathbf U\)</span> 是正交的，并且和之前一样假设 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 的列均值为 0（因此 <span class="math notranslate nohighlight">\(\mathbf U\)</span> 也是），这意味着 <span class="math notranslate nohighlight">\(\mathbf S\)</span> 列均值为 0，而且不相关，有单位方差。</p>
<p>!!! note “weiya 注：”
对 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 进行列中心化相当于左乘 <span class="math notranslate nohighlight">\(\mathbf{I - M}\)</span>，其中 <span class="math notranslate nohighlight">\(\mathbf M = \1\1^T/N\)</span>，进行 SVD 分解
$<span class="math notranslate nohighlight">\(
	\mathbf{(I-M)X=UDV}^T\,,
	\)</span><span class="math notranslate nohighlight">\(
	再左乘 \)</span>\mathbf{I-M}<span class="math notranslate nohighlight">\(，
	\)</span><span class="math notranslate nohighlight">\(
	\mathbf{(I-M)^2X=(I-M)UDV}^T\,,
	\)</span><span class="math notranslate nohighlight">\(
	注意到 \)</span>\mathbf{I-M}<span class="math notranslate nohighlight">\( 为幂等矩阵，即 \)</span>\mathbf{(I-M)^2 = (I-M)}<span class="math notranslate nohighlight">\(，则
	\)</span><span class="math notranslate nohighlight">\(
	\mathbf{(I-M)X=UDV}^T = \mathbf{(I-M)UDV}^T
	\)</span><span class="math notranslate nohighlight">\(
	右乘 \)</span>\mathbf{VD}^{-1}<span class="math notranslate nohighlight">\( 可以得到 \)</span>\mathbf{U = (I-M)U}<span class="math notranslate nohighlight">\(，即 \)</span>\mathbf U$ 列均值为 0.</p>
<p>用随机变量表示，我们可以把 SVD 或者对应的主成分分析看成是下列潜变量模型的一个估计</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
X_1&amp;=a_{11}S_1+a_{12}S_2+\cdots+a_{1p}S_p\\
X_2&amp;=a_{21}S_1+a_{22}S_2+\cdots+a_{2p}S_p\\
\vdots &amp;\qquad\vdots\\
X_p&amp;=a_{p1}S_1+a_{p2}S_2+\cdots+a_{pp}S_p
\end{align*}
\tag{14.78}
\label{14.78}
\end{split}\]</div>
<p>或者简单地写成 <span class="math notranslate nohighlight">\(X=\A S\)</span>。<strong>相关的 (correlated)</strong> <span class="math notranslate nohighlight">\(X_j\)</span> 都表示成 <strong>不相关的 (uncorrected)</strong>、单位方差的变量 <span class="math notranslate nohighlight">\(S_\ell\)</span> 的线性展开。尽管这不是太满意，因为对于任意给定的 <span class="math notranslate nohighlight">\(p\times p\)</span> 的正交矩阵 <span class="math notranslate nohighlight">\(\R\)</span>，我们可以写出</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
X&amp;=\A S\\
&amp;=\A\R^T\R S\\
&amp;=\A^*S^*
\end{align*}
\tag{14.79}
\label{14.79}
\end{split}\]</div>
<p>并且 <span class="math notranslate nohighlight">\(\Cov (S^*)=\R\Cov(S)\R^T=\I\)</span>。因此存在许多这样的分解，也因此不可能将任意特定的潜变量作为唯一的潜在来源。SVD 分解确实能以最优的方式得到任意秩为 <span class="math notranslate nohighlight">\(q &lt; p\)</span> 的截断分解。</p>
<p>经典的因子分析模型，主要由 <strong>心理测量学 (psychometrics)</strong> 的研究者发展而来。某种程度上缓解了这个问题；举个例子，Mardia et al. (1979)<a class="footnote-reference brackets" href="#id26" id="id3">1</a>。当 <span class="math notranslate nohighlight">\(q &lt; p\)</span>，因子分析模型有如下形式</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
X_1&amp;=a_{11}S_1+a_{12}S_2+\cdots+a_{1q}S_q+\varepsilon_1\\
X_2&amp;=a_{21}S_1+a_{22}S_2+\cdots+a_{2q}S_q+\varepsilon_2\\
\vdots &amp;\qquad\vdots\\
X_p&amp;=a_{p1}S_1+a_{p2}S_2+\cdots+a_{pq}S_q+\varepsilon_p
\end{align*}
\tag{14.80}
\label{14.80}
\end{split}\]</div>
<p>或者写成 <span class="math notranslate nohighlight">\(X=\A S+\epsilon\)</span>。这里 <span class="math notranslate nohighlight">\(S\)</span> 是 <span class="math notranslate nohighlight">\(q &lt; p\)</span>个揭示潜变量或者因子的向量，<span class="math notranslate nohighlight">\(\A\)</span> 是 <span class="math notranslate nohighlight">\(p\times q\)</span> 的 <strong>载荷 (loadings)</strong> 矩阵， <span class="math notranslate nohighlight">\(\varepsilon_j\)</span> 是零均值不相关的扰动。想法是潜变量 <span class="math notranslate nohighlight">\(S_\ell\)</span> 是 <span class="math notranslate nohighlight">\(X_j\)</span> 公共方差的来源，表明了它们之间的相关性结构，而 <span class="math notranslate nohighlight">\(\varepsilon_j\)</span> 对每个 <span class="math notranslate nohighlight">\(X_j\)</span> 是唯一的，并且解释了剩下的方差。一般地，<span class="math notranslate nohighlight">\(S_\ell\)</span> 和 <span class="math notranslate nohighlight">\(\varepsilon_j\)</span> 假设为高斯随机变量，并且采用极大似然法来拟合模型。参数都存在于下面的协方差阵中</p>
<div class="math notranslate nohighlight">
\[
\bSigma=\A\A^T+\D_\varepsilon \tag{14.81}\label{14.81}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\D_\varepsilon = \diag[\Var(\varepsilon_1),\ldots, \Var(\varepsilon_p)]\)</span>。<span class="math notranslate nohighlight">\(S_\ell\)</span> 为高斯并且不相关，这使得它们在统计上是独立随机变量。因此一系列的教育考试成绩可以认为是有潜在独立的因子，比如 <em>智力 (intelligence)</em>，<em>动机 (drive)</em> 等等所决定的。 <span class="math notranslate nohighlight">\(\A\)</span> 的列被称为<strong>因子载荷(factor loadings)</strong>，而且用来命名因子和解释因子。</p>
<p>不幸的是, 唯一性问题 <span class="math notranslate nohighlight">\(\eqref{14.79}\)</span> 仍然存在, 因为在式 <span class="math notranslate nohighlight">\(\eqref{14.81}\)</span> 中, 对于任意 <span class="math notranslate nohighlight">\(q\times q\)</span> 的正交矩阵 <span class="math notranslate nohighlight">\(\R\)</span>, <span class="math notranslate nohighlight">\(\A\)</span> 和 <span class="math notranslate nohighlight">\(\A\R^T\)</span> 是等价的。这导致了因子分析中的主观性, 因为用户可以寻找因子更易解释的旋转版本. 这点使得许多分析学家对因子分析表示怀疑, 而且这可能是它在当代统计中不受欢迎的原因。尽管我们这里不继续讨论细节，但是 SVD 分解在 \eqref{14.81} 的估计上发挥重要作用。举个例子，假设 <span class="math notranslate nohighlight">\(\Var(\varepsilon_j)\)</span> 相等，SVD 的前 <span class="math notranslate nohighlight">\(q\)</span> 个成分确定了由 <span class="math notranslate nohighlight">\(\A\)</span> 张成的子空间。</p>
<p>因为每个 <span class="math notranslate nohighlight">\(X_j\)</span> 独立的扰动 <span class="math notranslate nohighlight">\(\varepsilon_j\)</span>，因子分析可以看成是对 <span class="math notranslate nohighlight">\(X_j\)</span> 的相关性结构进行建模，而非对协方差结构建模。这个可以通过对 <span class="math notranslate nohighlight">\(\eqref{14.81}\)</span> 的协方差结构进行标准化后得到（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/51">练习 14.14</a>）。</p>
<p>!!! note “weiya 注: Ex. 14.14”
练习 14.14 表明，因子分析实质上是对相关矩阵进行分解。详见解答见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/51">Issue 51: Ex. 14.14</a></p>
<p>这是因子分析与 PCA 的重要区别，尽管这不是我们讨论的重点。<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/50">练习 14.15</a> 讨论了一个简单的例子，因为这个差异，因子分析和主成分的解完全不同。</p>
<p>!!! note “weiya 注: Ex. 14.15”
练习 14.15 用一个实际例子说明了，因子分析和主成分得到的第一因子和第一主成分完全不同。详细解答见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/50">Issue 50: Ex. 14.15</a></p>
</div>
<div class="section" id="id4">
<h2>独立成分分析<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>**独立成分分析 (ICA)**模型与 <span class="math notranslate nohighlight">\(\eqref{14.78}\)</span> 有相同的形式, 除了假设 <span class="math notranslate nohighlight">\(S_\ell\)</span> 是统计上独立而非不相关。</p>
<p>!!! note “weiya 注: 独立与不相关”
统计上, 连续型随机变量 <span class="math notranslate nohighlight">\(X\)</span> 与 <span class="math notranslate nohighlight">\(Y\)</span> 独立的定义为
$<span class="math notranslate nohighlight">\(
    p(x, y)=p_X(x)p_Y(y)\;\forall x,y
    \)</span><span class="math notranslate nohighlight">\(
    而不相关的定义为
    \)</span><span class="math notranslate nohighlight">\(
	\Cov(X, Y)=0
    \)</span>$
独立意味着不相关，但反之不对。对于二元正态随机变量，两者等价。</p>
<p>直观上, 不相关确定了多元变量分布的二阶交叉矩（协方差）, 而一般地, 统计独立确定了所有的交叉矩。 这些额外的矩条件能让我们找到唯一的 <span class="math notranslate nohighlight">\(\A\)</span>。 因为多元正态分布只要二阶矩就可以确定, 所以这是一个特例, 在忽略一个旋转的情况下任意高斯独立成分可以像之前被确定。 因此如果假设 <span class="math notranslate nohighlight">\(S_\ell\)</span> 是独立且非高斯的, 则可以避免 <span class="math notranslate nohighlight">\(\eqref{14.78}\)</span> 和 <span class="math notranslate nohighlight">\(\eqref{14.80}\)</span> 的唯一性问题.</p>
<p>这里我们将要讨论 <span class="math notranslate nohighlight">\(\eqref{14.78}\)</span> 中全 <span class="math notranslate nohighlight">\(p\)</span> 个成分的模型, 其中 <span class="math notranslate nohighlight">\(S_\ell\)</span> 是独立的且有单位方差; 因子分析模型 <span class="math notranslate nohighlight">\(\eqref{14.80}\)</span> 的 ICA 版本也同样存在。 我们的处理是基于 Hyvärinen and Oja (2000)<a class="footnote-reference brackets" href="#id27" id="id5">2</a> 的综述文章.</p>
<p>我们希望恢复 <span class="math notranslate nohighlight">\(X=\A S\)</span> 中的混合矩阵 <span class="math notranslate nohighlight">\(\A\)</span>。 不失一般性, 我们假设 <span class="math notranslate nohighlight">\(X\)</span> 已经 <strong>白化 (whitened)</strong> 使得 <span class="math notranslate nohighlight">\(\Cov(X)=\I\)</span>; 这一般可以通过前面描述的 SVD 实现。</p>
<p>!!! note “weiya 注：白化 (whitened)”
根据<a class="reference external" href="https://en.wikipedia.org/wiki/Whitening_transformation">维基百科</a>，因为白噪声的协方差矩阵为单位阵，所以称这一过程为白化。白化矩阵 <span class="math notranslate nohighlight">\(W\)</span> 使得 <span class="math notranslate nohighlight">\(WX\)</span> 协方差为单位阵（此处 <span class="math notranslate nohighlight">\(X\)</span> 表示为随机向量，若作用在矩阵上，则右乘），则 <span class="math notranslate nohighlight">\(W^TW = \Sigma^{-1}\)</span>. 根据求解 <span class="math notranslate nohighlight">\(W\)</span> 的算法，可以分类为</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>- Mahalanobis or ZCA whitening: $W = \Sigma^{-1/2}$
- Cholesky whitening: 取 Cholesky 分解 $\Sigma^{-1} = LL^T$ 中的 $L^T$，则 $\Sigma = (L^{T})^{-1}L^{-1}$，因而 $W\Sigma W^T=L^T (L^{T})^{-1}L^{-1} L = \I$.
- PCA whitening: 特征分解 

而这里提到 SVD，大概是通过 
$$
\Sigma = V^TD^2V\,,
$$
得到
$$
\Sigma^{-1} = V^TD^{-2}V\,,
$$
然后取 $W = D^{-1}V$.
</pre></div>
</div>
<p>反过来, 因为 <span class="math notranslate nohighlight">\(S\)</span> 的协方差也为 <span class="math notranslate nohighlight">\(\I\)</span>, 这意味着 <span class="math notranslate nohighlight">\(\A\)</span> 是正交的。 所以求解 ICA 问题等价于寻找正交的 <span class="math notranslate nohighlight">\(\A\)</span> 使得随机变量向量 <span class="math notranslate nohighlight">\(S=\A^T X\)</span> 的组分是独立(且是非高斯的)。</p>
<p><img alt="" src="../_images/fig14.37.png" /></p>
<p>图 14.37 显示了在分离两个混合信号例子中 ICA 的能力。这也是经典的 cocktail party problem 的一个例子，不同的麦克风 <span class="math notranslate nohighlight">\(X_j\)</span> 接受来自不同独立源 <span class="math notranslate nohighlight">\(S_\ell\)</span>（音乐、不同人说的话等等）的混合信号。 ICA 通过利用原始信号源的独立性和非高斯性，能够进行 <strong>盲信号分离 (blind source separation)</strong>。</p>
<p>许多流行的 ICA 方法是基于熵。 密度为 <span class="math notranslate nohighlight">\(g(y)\)</span> 的随机变量 <span class="math notranslate nohighlight">\(Y\)</span> 的 <strong>微分熵 (differential entropy)</strong> <span class="math notranslate nohighlight">\(H\)</span> 由下式给出</p>
<div class="math notranslate nohighlight">
\[
H(Y)=-\int g(y)\log g(y)dy\tag{14.82}\label{14.82}
\]</div>
<p>信息理论中一个著名的结论是在所有同方差的随机变量中，高斯随机变量有最大的熵。最后，随机向量 <span class="math notranslate nohighlight">\(Y\)</span> 的组分之间的 <strong>互信息量(mutual information)</strong> <span class="math notranslate nohighlight">\(I(Y)\)</span> 是独立性的一个自然度量:</p>
<div class="math notranslate nohighlight">
\[
I(Y)=\sum\limits_{j=1}^pH(Y_j)-H(Y)\tag{14.83}
\]</div>
<p>值 <span class="math notranslate nohighlight">\(I(Y)\)</span> 称为 <span class="math notranslate nohighlight">\(Y\)</span> 的密度 <span class="math notranslate nohighlight">\(g(y)\)</span> 与其独立版本 <span class="math notranslate nohighlight">\(\prod\limits_{j=1}^pg_j(y_j)\)</span> 之间的 Kullback-Leibler 距离, 其中 <span class="math notranslate nohighlight">\(g_j(y_j)\)</span> 是 <span class="math notranslate nohighlight">\(Y_j\)</span> 的边缘密度. 如果 <span class="math notranslate nohighlight">\(X\)</span> 有协方差 <span class="math notranslate nohighlight">\(\I\)</span>, 且 <span class="math notranslate nohighlight">\(Y=\A^TX\)</span>, 其中 <span class="math notranslate nohighlight">\(\A\)</span> 是正交, 则易证</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
I(Y)&amp;= \sum\limits_{j=1}^pH(Y_j)-H(X)-\LOG\vert \det\A\vert\tag{14.84}\label{14.84}\\
&amp;=\sum\limits_{j=1}^pH(Y_j)-H(X)\tag{14.85}
\end{align}
\end{split}\]</div>
<p>!!! note “weiya 注: (14.85) 的证明”
只要证
$<span class="math notranslate nohighlight">\(
	H(Y)=H(X)+\LOG \vert \det \A \vert
	\)</span><span class="math notranslate nohighlight">\(
	对 \eqref{14.82} 进行变量替换有
	\)</span><span class="math notranslate nohighlight">\(
	H(Y)=-\int g(\A'x)\LOG g(\A'x)\cdot \vert \det \J\vert dx\qquad (*)
	\)</span><span class="math notranslate nohighlight">\(
	其中\)</span>\J_{ij}=\frac{\partial y_i}{\partial x_j}<span class="math notranslate nohighlight">\(.
	又
	\)</span><span class="math notranslate nohighlight">\(
	y_i = \sum\limits_{j=1}^p (\A')_{ij}x_j
	\)</span><span class="math notranslate nohighlight">\(
	故
	\)</span><span class="math notranslate nohighlight">\(
	\J_{ij}=(\A')_{ij}
	\)</span><span class="math notranslate nohighlight">\(
	所以 \)</span>\det\J = \det \A’=\det \A<span class="math notranslate nohighlight">\(.
	另外, \)</span>X<span class="math notranslate nohighlight">\( 的密度函数为
	\)</span><span class="math notranslate nohighlight">\(
	f(x)=g(\A'x)\cdot\vert \det\J\vert 
	\)</span><span class="math notranslate nohighlight">\(
	于是 \)</span>(*)<span class="math notranslate nohighlight">\( 式可以写成
	\)</span><span class="math notranslate nohighlight">\(
	\begin{align}
	H(Y) &amp;= -\int \frac{f(x)}{\vert\det\A\vert} \LOG\frac{f(x)}{\vert\det\A\vert}\cdot\vert\det\A\vert dx\\
	&amp;=-\int f(x)[\LOG f(x)-\LOG\vert\det\A\vert]dx\\
	&amp;=H(X)+\LOG\vert\det\A\vert
	\end{align}
	\)</span>$
证毕。</p>
<p>寻找 <span class="math notranslate nohighlight">\(\A\)</span> 最小化 <span class="math notranslate nohighlight">\(I(Y)=I(\A^TX)\)</span> 也就是寻找使得组分间的独立性最强的正交变换。考虑到式 <span class="math notranslate nohighlight">\(\eqref{14.84}\)</span>, 这等价于最小化 <span class="math notranslate nohighlight">\(Y\)</span> 的各组分的熵的和, 反过来意味着最大化它们与高斯分布的距离。</p>
<p>!!! note “weiya 注”
因为高斯随机变量的熵最大, 则让各组分熵之和最小, 意味着各组分远离高斯分布。</p>
<p>为了简便, 与其采用熵 <span class="math notranslate nohighlight">\(H(Y_j)\)</span>, Hyvärinen and Oja (2000)<a class="footnote-reference brackets" href="#id27" id="id6">2</a> 采用 <strong>负熵 (negentropy)</strong> <span class="math notranslate nohighlight">\(J(Y_j)\)</span></p>
<div class="math notranslate nohighlight">
\[
J(Y_j) = H(Z_j)-H(Y_j)\tag{14.86}\label{14.86}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(Z_j\)</span> 是与 <span class="math notranslate nohighlight">\(Y_j\)</span> 同方差的高斯随机变量。负熵是非负的, 它度量了 <span class="math notranslate nohighlight">\(Y_j\)</span> 与高斯随机变量之间的距离。他们提出负熵的一个简单近似, 这个近似可以用来计算和优化数据. 图 14.37 至图 14.39 中的 ICA 都采用下面的近似</p>
<div class="math notranslate nohighlight">
\[
J(Y_j)\approx [\E G(Y_j) - \E G(Z_j)]^2\tag{14.87}\label{14.87}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(G(u)=\frac 1a\log\cosh(au), 1\le a\le 2\)</span>。当应用到实际数据时, 期望用数据的平均值代替。这是这些作者们提供的<code class="docutils literal notranslate"><span class="pre">FastICA</span></code>软件中的一个选项. 更经典（但不太鲁棒）的度量是基于四阶矩, 也因此可以通过 <strong>峰度 (kurtosis)</strong> 来衡量与高斯分布的距离。更多细节参见 Hyvärinen and Oja (2000)<a class="footnote-reference brackets" href="#id27" id="id7">2</a>. 在 <span class="xref myst">14.7.4 节</span>我们讨论他们寻找最优方向的近似牛顿算法.</p>
<p>!!! note “weiya 注:”
峰度是四阶标准矩
$<span class="math notranslate nohighlight">\(
	Kurt(X)=\E\Big[
	\Big(
	\frac{X-\mu}{\sigma}
	\Big)^4
	\Big]
	\)</span>$</p>
<p>总结一下，ICA 应用到多元数据中，来寻找一系列的正交投影，使得投影数据尽可能远离高斯分布。采用白化后的数据（协方差为 <span class="math notranslate nohighlight">\(\I\)</span>），这意味着寻找尽可能独立的组分。</p>
<p>ICA 本质上从因子分析的一个解出发，并且寻找一个旋转得到独立组分。从这点看, ICA 与在心理测量学中采用的传统方法 “varimax” 和 “quartimax” 一样, 仅仅是因子旋转的一种方式.</p>
<div class="section" id="id8">
<h3>例子: 手写数字<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>我们再次讨论 <span class="xref myst">14.5.1 节</span>用 PCA 分析的手写数字“3”。</p>
<p><img alt="" src="../_images/fig14.39.png" /></p>
<p>图 14.39 比较了前五个主成分（标准化）和前五个 ICA 成分，都显示在标准化后的同一单位尺度下。注意到每张图都是 256 维空间的二维投影。所有 PCA 组分看上去都服从联合高斯分布，而 ICA 组分都服从长尾分布。这并不是很奇怪，因为 PCA 主要考虑方差，而 ICA 特地寻找非高斯的分布。所有的组分都已经标准化，所以我们看不出主成分的方差降低。</p>
<p><img alt="" src="../_images/fig14.40.png" /></p>
<p>图 14.40 展示了每个 ICA 成分的两个极端的数据点，以及均值的两个极端点。这解释了每个组分的实际意义。举个例子，第五个 ICA 成分捕捉具有长扫尾 (long sweeping tailed) 的“3”。</p>
</div>
<div class="section" id="id9">
<h3>例子: 时序脑电图数据<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>ICA 已经成为 <strong>脑动力学 (brain dynamics)</strong> 中的重要工具， 这里介绍的例子采用 ICA 来理清多频脑电图 (EEG) 数据中信号的组分 (Onton and Makeig, 2006<a class="footnote-reference brackets" href="#id28" id="id10">12</a>)。</p>
<p>被试者戴上装有 100 个 EEG 电极的帽子， 这些用来记录头皮上不同部位的脑活动。</p>
<p><img alt="" src="../_images/fig14.41.png" /></p>
<p>图 14.41(上图)显示了被试者在 30 分钟的周期内进行标准的”two-back”学习过程时，这些电极中的 9 个电极在 15 秒内的输出结果。 大约间隔 1500ms 依次给被试者呈现一个字母 (B, H, J, C, F 或者 K), 然后被试者通过按“是”或“否”的按钮来判断当前的字母与前两步出现的字母是否一致。</p>
<p>!!! note “weiya 注: n-back”
参考<a class="reference external" href="https://en.wikipedia.org/wiki/N-back">wiki: <span class="math notranslate nohighlight">\(n\)</span>-back</a>, <span class="math notranslate nohighlight">\(n\)</span>-back是指给被试者连续的刺激, 要求其判断当前刺激与前<span class="math notranslate nohighlight">\(n\)</span>步的刺激是否一致. 举个例子, 在3-back测试中, 若给被试者如下刺激
<img alt="" src="../_images/nback.png" />
则当前刺激为高亮部分时, 被试者应当判断”是”, 因为在前3步出现了高亮部分相同的刺激.</p>
<p>根据被试者的回答，他们得分或者失分，并且偶尔赚取 bonus 或者额外惩罚。脑电图信号中这个时序的数据表现出空间上的相关性———邻近的信号感受器看起来非常相似。</p>
<p>这里重要假设是每个头皮电极上记录的信号是从不同表皮活动以及非表皮区域的人工活动（如下文中提到的眨眼）中产生的独立的电势的混合。 更多ICA在这个领域的细节参见参考文献。</p>
<p>图 14.41 的下半部分展示了 ICA 组分的选择. 彩色图象用（画在头皮上的）热图表示未混合的系数向量的估计值 <span class="math notranslate nohighlight">\(\hat a_j\)</span>, 它表明了活性的位置. 对应的时序数据展示了 ICA 组分的活动.</p>
<p>举个例子, 每次反馈信号之后被试者的眨眼（彩色的垂直直线）, 它解释了 IC1 和 IC3 中的位置和人工信号. IC12 是与 <strong>心脏脉冲 (cardiac pulse)</strong> 有关的人工信号. IC4 和 IC7 对应额骨 (frontal) theta-band 的活动, 而且这出现在回答正确后被试者身体的舒展. 更多细节参见 Onton and Makeig (2006)<a class="footnote-reference brackets" href="#id28" id="id11">12</a>中对这个例子的讨论, 以及ICA在脑电图模型中的应用.</p>
</div>
</div>
<div class="section" id="id12">
<h2>探索投影寻踪<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h2>
<p>Friedman and Tukey (1974)<a class="footnote-reference brackets" href="#id29" id="id13">3</a> 提出了 <strong>探索投影寻踪 (exploratory projection pursuit)</strong>，这是可视化高维数据的图象探索技巧。他们的观点是高维数据的大多数低维（一维或二维）投影看起来是高斯分布的。 他们提出一系列 projection indices 的方法用于优化, 每个集中在与高斯分布的不同距离。自从他们最先提出该方法, 陆续有各种改进的建议 (Huber, 1985<a class="footnote-reference brackets" href="#id30" id="id14">4</a>; Friedman, 1987<a class="footnote-reference brackets" href="#id31" id="id15">5</a>)，以及交互式图形软件包 Xgobi（Swayne et al., 1991<a class="footnote-reference brackets" href="#id32" id="id16">6</a>, 现在叫做 GGobi）中实现的各种指标，包括熵。 这些投影指标与上文介绍的 <span class="math notranslate nohighlight">\(J(Y_j)\)</span> 形式一样， 其中 <span class="math notranslate nohighlight">\(Y_j=a_j^TX\)</span> 是 <span class="math notranslate nohighlight">\(X\)</span> 组分的标准化的线性组合。实际上，交叉熵的一些近似和替代与投影寻踪中提出的指标重合。一般在投影寻踪中, 方向 <span class="math notranslate nohighlight">\(a_j\)</span> 不需要限制为正交。Friedman (1987)<a class="footnote-reference brackets" href="#id31" id="id17">5</a>将数据在选定的投影上转换使之看起来像高斯分布，然后搜索接下来的方向。尽管他们的出发点不一样，但是 ICA 和探索投影寻踪非常相似，至少对于这里描述的表示形式。</p>
</div>
<div class="section" id="id18">
<h2>独立分量分析的一种直接方法<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h2>
<p>由定义知独立组分有如下联合乘积密度</p>
<div class="math notranslate nohighlight">
\[
f_S(s)=\prod\limits_{j=1}^pf_j(s_j)\tag{14.88}
\]</div>
<p>所以这里展示一种采用广义可加模型（<span class="xref myst">9.1 节</span>）来直接估计这个密度的方式。全部细节可以在 Hastie and Tibshirani (2003)<a class="footnote-reference brackets" href="#id33" id="id19">7</a>中找到, 并且这个方法已经在<code class="docutils literal notranslate"><span class="pre">ProDenICA</span></code>的 R 包中实现了, 这可以在 CRAN 上下载.</p>
<p>根据与高斯分布的距离的表示, 我们将每个 <span class="math notranslate nohighlight">\(f_j\)</span> 写成</p>
<div class="math notranslate nohighlight">
\[
f_j(s_j)=\phi(s_j)e^{g_j(s_j)}\tag{14.89}\label{14.89}
\]</div>
<p>这是<strong>倾斜的 (tilted)</strong> 高斯密度。这里 <span class="math notranslate nohighlight">\(\phi\)</span> 是标准高斯分布密度, 并且 <span class="math notranslate nohighlight">\(g_j\)</span> 满足密度函数所要求的标准化条件。和之前一样假设 <span class="math notranslate nohighlight">\(X\)</span> 已经预处理，观测数据 <span class="math notranslate nohighlight">\(X=\A S\)</span> 的对数似然为</p>
<div class="math notranslate nohighlight">
\[
\ell(\A, \{g_j\}_1^p;\X)=\sum\limits_{i=1}^N\sum\limits_{j=1}^p[\LOG \phi(a_j^Tx_i)+g_j(a_j^Tx_i)]\tag{14.90}\label{14.90}
\]</div>
<p>!!! note “weiya 注：”
\eqref{14.90}原书中 <span class="math notranslate nohighlight">\(\phi\)</span> 有下标 <span class="math notranslate nohighlight">\(j\)</span>， 但是因为 <span class="math notranslate nohighlight">\(\phi\)</span> 为标准正态， 所以应该直接忽略下标， 原书中后面的 \eqref{14.91} 也确实没有下标。</p>
<p>我们希望在 <span class="math notranslate nohighlight">\(\A\)</span> 为正交且 <span class="math notranslate nohighlight">\(\eqref{14.89}\)</span> 定义的 <span class="math notranslate nohighlight">\(g_j\)</span> 的条件下, 最大化上式。对 <span class="math notranslate nohighlight">\(g_j\)</span> 不添加额外的约束, 模型 \eqref{14.90} 是过参数化的, 所以我们最大化下面的正则化版本</p>
<div class="math notranslate nohighlight">
\[
\sum\limits_{j=1}^p\Big[
\frac 1N \sum_{i=1}^N[\LOG \phi(a_j^Tx_i) + g_j(a_j^Tx_i)] - 
\int \phi(t)e^{g_j(t)}dt - 
\lambda_j\int\{g_j^{(3)}\}^2(t)dt
\Big]\tag{14.91}\label{14.91}
\]</div>
<p>受 Silverman (1986)<a class="footnote-reference brackets" href="#id34" id="id20">8</a>的启发，在 <span class="math notranslate nohighlight">\(\eqref{14.91}\)</span> 中, 我们（对每个 <span class="math notranslate nohighlight">\(j\)</span>）减去了两个惩罚项:</p>
<ul class="simple">
<li><p>第一个在任意解 <span class="math notranslate nohighlight">\(\hat g_j\)</span> 上强制要求密度约束 <span class="math notranslate nohighlight">\(\int \phi(t)e^{\hat g_j(t)}dt=1\)</span></p></li>
<li><p>第二个是鲁棒性惩罚, 保证了解 <span class="math notranslate nohighlight">\(\hat g_j\)</span> 是结点在观测值 <span class="math notranslate nohighlight">\(s_{ij}=a_j^Tx_i\)</span> 的四次样条.</p></li>
</ul>
<p>!!! note “weiya 注：”
下面说明为什么第一个惩罚项能够对 <span class="math notranslate nohighlight">\(\phi(t)e^{\hat g_j(t)}\)</span> 的密度进行约束。假设随机变量 <span class="math notranslate nohighlight">\(X\)</span> 的密度函数为 <span class="math notranslate nohighlight">\(f(x)\)</span>，其对数似然函数为 <span class="math notranslate nohighlight">\(g(x)\propto \log f(x)\)</span>，这个问题归结为说明
$<span class="math notranslate nohighlight">\(
	A_0(g)=\frac 1n\sum g(x_i)
	\)</span><span class="math notranslate nohighlight">\(
	在 \)</span>\int e^g=1<span class="math notranslate nohighlight">\( 的约束下的 \)</span>\arg\max<span class="math notranslate nohighlight">\( 等于
	\)</span><span class="math notranslate nohighlight">\(
	A(g)=\frac 1n\sum g(x_i)-\int \exp(g(x))dx
	\)</span><span class="math notranslate nohighlight">\(
	的 \)</span>\arg\max<span class="math notranslate nohighlight">\(。
	考虑\)</span>g^*=g-\LOG \int e^g<span class="math notranslate nohighlight">\(
	则
	\)</span><span class="math notranslate nohighlight">\(
	\int e^{g^*}=\int e^{g(x)}\cdot \frac{1}{\int e^{g(t)}dt}dx=1
	\)</span><span class="math notranslate nohighlight">\(
	于是
	\)</span><span class="math notranslate nohighlight">\(
	A(g^*)=A(g)+\int e^{g(x)}dx-\LOG(\int e^{g(x)}dx)-1
	\)</span><span class="math notranslate nohighlight">\(
	因 \)</span>t-\log t\ge 1, \forall t&gt;0<span class="math notranslate nohighlight">\(，当且仅当 \)</span>t=1<span class="math notranslate nohighlight">\( 取等号，
	则 \)</span>A(g^*)\ge A(g)<span class="math notranslate nohighlight">\(，也就是当且仅当 \)</span>\int e^g = 1<span class="math notranslate nohighlight">\( 时，\)</span>A(g)<span class="math notranslate nohighlight">\(会取得最大值，而这时候\)</span>A(g)=A_0(g)-1<span class="math notranslate nohighlight">\(，所以\)</span>A_0(g)<span class="math notranslate nohighlight">\(也会取得最大值。由此看出，\)</span>A_0(g)<span class="math notranslate nohighlight">\(在\)</span>\int e^g=1<span class="math notranslate nohighlight">\(的约束下的\)</span>\arg\max<span class="math notranslate nohighlight">\(等于（无约束的）\)</span>A(g)<span class="math notranslate nohighlight">\(的\)</span>\arg\max<span class="math notranslate nohighlight">\(。
	关于第二个点说得到四次样条，我想可以这样类比理解（但不严谨），我们知道四次样条有三阶连续的导函数，且对于[光滑样条（三次）](../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines/index.html)有如下形式
	\)</span><span class="math notranslate nohighlight">\(
	RSS(f,\lambda)=\sum\limits_{i=1}^N\{y_i-f(x_i)\}^2+\lambda\int \{f''(t)\}^2dt\tag{5.9}
	\)</span>$
可以类比看出第二个惩罚正是针对四次样条。</p>
<p>可以进一步证明每个解密度 <span class="math notranslate nohighlight">\(\hat{f_j}=\phi e^{\hat g_j}\)</span> 的均值为 0, 方差为 1（练习 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/53">14.18</a>）。当我们增大 <span class="math notranslate nohighlight">\(\lambda_j\)</span>，这些解近似标准高斯密度 <span class="math notranslate nohighlight">\(\phi\)</span>。</p>
<p><img alt="" src="../_images/alg14.3.png" /></p>
<p>如在算法 14.3 中描述, 以一种轮换的方式优化 <span class="math notranslate nohighlight">\(\eqref{14.91}\)</span> 来拟合函数 <span class="math notranslate nohighlight">\(g_i\)</span> 和方向 <span class="math notranslate nohighlight">\(a_j\)</span>。</p>
<p>第 2(a) 步等价于半参密度估计，可以采用广义可加模型来求解。为了方便，我们取 <span class="math notranslate nohighlight">\(p\)</span> 个独立问题中的一个</p>
<div class="math notranslate nohighlight">
\[
\frac 1N \sum_{i=1}^N[\LOG \phi(a_j^Tx_i) + g_j(a_j^Tx_i)] - 
\int \phi(t)e^{g_j(t)}dt - 
\lambda_j\int\{g_j^{(3)}\}^2(t)dt\tag{14.92}\label{14.92}
\]</div>
<p>尽管 <span class="math notranslate nohighlight">\(\eqref{14.92}\)</span> 的第二个积分导出光滑样条， 但第一个积分比较困难，需要近似。我们构造一个 <span class="math notranslate nohighlight">\(L\)</span> 个值为 <span class="math notranslate nohighlight">\(s_\ell^*\)</span> 的细网格，这些值间距为 <span class="math notranslate nohighlight">\(\Delta\)</span>、覆盖观测值 <span class="math notranslate nohighlight">\(s_i\)</span> 的取值范围， 并且统计得到的小块中 <span class="math notranslate nohighlight">\(s_i\)</span> 的个数：</p>
<div class="math notranslate nohighlight">
\[
y_\ell^*=\frac{\#s_i\in (s_\ell^*-\Delta/2, s_\ell^*+\Delta/2)}{N}\tag{14.93}
\]</div>
<p>一般我们取 <span class="math notranslate nohighlight">\(L\)</span> 为 1000 就已经足够了。 则 <span class="math notranslate nohighlight">\(\eqref{14.92}\)</span> 可以近似为</p>
<div class="math notranslate nohighlight">
\[
\sum\limits_{\ell=1}^L\Big\{
y_\ell^*[\log(\phi(s_\ell^*)) + g(s_\ell^*)]-\Delta\phi(s_\ell^*)e^{g(s_\ell^*)}-\lambda\int {g^{(3)}}^2(s)ds\tag{14.94}
\Big\}
\]</div>
<p>!!! note “weiya 注：翻译相关”
原文 <span class="math notranslate nohighlight">\(y\)</span> 的脚标是 <span class="math notranslate nohighlight">\(i\)</span>，但应该是个 typo，应为 <span class="math notranslate nohighlight">\(\ell\)</span>.</p>
<p>这个表达是可以看成是与惩罚的 Poisson 对数似然成比例，其中响应变量为 <span class="math notranslate nohighlight">\(y_\ell^\*/\Delta\)</span>，惩罚参数为 <span class="math notranslate nohighlight">\(\lambda/\Delta\)</span>，均值为 <span class="math notranslate nohighlight">\(\mu(s)=\phi(s)e^{g(s)}\)</span>。</p>
<p>!!! note “weiya 注：Poisson 对数似然”
有关讨论详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/54">Issue 54</a></p>
<p>这是一个 <strong>广义可加样条模型 (generalized additive spline model)</strong>(Hastie and Tibshirani, 1990<a class="footnote-reference brackets" href="#id35" id="id21">9</a>; Efron and Tibshirani, 1996<a class="footnote-reference brackets" href="#id36" id="id22">10</a>)，其中的 offset 项为 <span class="math notranslate nohighlight">\(\log \phi(s)\)</span>， 并且可以以 <span class="math notranslate nohighlight">\(O(L)\)</span> 的复杂度用牛顿算法进行拟合。尽管要求四次样条，但是实际中我们发现三次样条就足够了。我们有 <span class="math notranslate nohighlight">\(p\)</span> 个调整参数 <span class="math notranslate nohighlight">\(\lambda_j\)</span> 需要设定，在实际中我们使得它们相等，然后通过有效自由度 <span class="math notranslate nohighlight">\(\df(\lambda)\)</span> 来确定光滑的程度。我们的软件采用 <span class="math notranslate nohighlight">\(5\df\)</span> 作为默认值。</p>
<p>算法 14.3 的 2(b) 要求在给定 <span class="math notranslate nohighlight">\(\hat g_j\)</span>，对 <span class="math notranslate nohighlight">\(\A\)</span> 进行优化。和式中仅仅第一项有涉及 <span class="math notranslate nohighlight">\(\A\)</span>，并且因为 <span class="math notranslate nohighlight">\(\A\)</span> 是正交的，则涉及 <span class="math notranslate nohighlight">\(\phi\)</span> 的所有项的集合不依赖 <span class="math notranslate nohighlight">\(\A\)</span>（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/55">练习 14.19</a>）。</p>
<p>!!! note “weiya 注：Ex. 14.19”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/55">Issue 55: 练习 14.19</a>.</p>
<p>因此我们只需要最大化</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
C(\A) &amp;= \frac 1N\sum\limits_{j=1}^p\sum\limits_{i=1}^N\hat g_j(a_j^Tx_i)\tag{14.95}\\
&amp;= \sum\limits_{j=1}^pC_j(a_j)\notag
\end{align}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(C(\A)\)</span> 是拟合的密度与高斯密度间的对数似然比，而且可以看成是负熵 \eqref{14.86} 的一个估计， 其中每个 <span class="math notranslate nohighlight">\(\hat g_j\)</span> 就像 \eqref{14.87} 中的差异函数。第 2(b) 步中的固定点更新是修改后的牛顿法（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/56">练习 14.20</a>）：</p>
<ol class="simple">
<li><p>对于每个 <span class="math notranslate nohighlight">\(j\)</span>，更新
$<span class="math notranslate nohighlight">\(
a_j\leftarrow \E\{X\hat g_j'(a_j^TX)-\E[\hat g_j^{(2)}(a_j^TX)]a_j\}\tag{14.96}
\)</span><span class="math notranslate nohighlight">\(
其中 \)</span>\E<span class="math notranslate nohighlight">\( 表示关于 \)</span>x_i<span class="math notranslate nohighlight">\( 的期望。因为 \)</span>\hat g_j$ 是拟合的四次（或者三次）样条，则式中第一项和第二项已经计算好了。</p></li>
<li><p>采用对称平方根变换将 <span class="math notranslate nohighlight">\(\A\)</span> 化为正交矩阵：<span class="math notranslate nohighlight">\((\A\A^T)^{-\frac 12}\A\)</span>。如果 <span class="math notranslate nohighlight">\(\A=\U\D\V^T\)</span> 是 <span class="math notranslate nohighlight">\(\A\)</span> 的 SVD 分解，则易证 <span class="math notranslate nohighlight">\(\A\leftarrow \U\V^T\)</span>。</p></li>
</ol>
<p>!!! note “weiya 注：Ex. 14.20”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/56">Issue 56: Ex. 14.20</a>.</p>
<p>我们的 <code class="docutils literal notranslate"><span class="pre">ProDenICA</span></code> 算法和 <code class="docutils literal notranslate"><span class="pre">FastICA</span></code> 算法在图 14.37 中的模拟时间序列数据、图 14.38 的均匀分布数据的混合以及图 14.39 中的数字数据都表现得一样好。</p>
<div class="section" id="id23">
<h3>例子：模拟<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h3>
<p><img alt="" src="../_images/fig14.42.png" /></p>
<p>图 14.42 展示了比较 <code class="docutils literal notranslate"><span class="pre">ProDenICA</span></code> 与 <code class="docutils literal notranslate"><span class="pre">FastICA</span></code> 以及另外一个半参方法 <code class="docutils literal notranslate"><span class="pre">KernelICA</span></code>(Bach and Jordan, 2002<a class="footnote-reference brackets" href="#id37" id="id24">11</a>)的模拟的结果。左图展示了作为比较的基础的 18 个分布。对于每个分布，我们产生成对的独立组分(<span class="math notranslate nohighlight">\(N=1024\)</span>)，以及随机的混合矩阵 <span class="math notranslate nohighlight">\(\IR^2\)</span>，条件数为 1 到 2 之间。</p>
<p>!!! note “weiya 注：条件数”
矩阵<span class="math notranslate nohighlight">\(A\)</span>的条件数为
$<span class="math notranslate nohighlight">\(
	\kappa(A) = \Vert A^{-1}\Vert\cdot \Vert A\Vert\ge \Vert A^{-1}\cdot A\Vert=1
	\)</span><span class="math notranslate nohighlight">\(
	若\)</span>\Vert\cdot \Vert<span class="math notranslate nohighlight">\(为二范数
	\)</span><span class="math notranslate nohighlight">\(
	\kappa(A) = \frac{\sigma_{\max}(A)}{\sigma_{\min}(A)}
	\)</span><span class="math notranslate nohighlight">\(
	其中\)</span>\sigma_\max(A)<span class="math notranslate nohighlight">\(和\)</span>\sigma_\min(A)<span class="math notranslate nohighlight">\(分别为\)</span>A<span class="math notranslate nohighlight">\(的最大和最小的奇异值。
	对于正规矩阵，即满足
	\)</span><span class="math notranslate nohighlight">\(
	A^TA=AA^T
	\)</span><span class="math notranslate nohighlight">\(
	有
	\)</span><span class="math notranslate nohighlight">\(
	\kappa(A)=\frac{\vert\lambda_\max(A)\vert}{\vert \lambda_\min(A)\vert}
	\)</span><span class="math notranslate nohighlight">\(
	其中\)</span>\lambda<span class="math notranslate nohighlight">\(为特征值。
	当\)</span>A<span class="math notranslate nohighlight">\(为单位阵时，\)</span>\kappa(A)=1$。</p>
<p>我们采用<code class="docutils literal notranslate"><span class="pre">FastICA</span></code>的 R 语言实现，采用 <span class="math notranslate nohighlight">\(\eqref{14.87}\)</span> 的负熵准则，<code class="docutils literal notranslate"><span class="pre">ProDenICA</span></code>也是这样。对于<code class="docutils literal notranslate"><span class="pre">KernelICA</span></code>，我们采用作者的MATLAB代码。因为搜索准则是非凸的，我们对每种方法采用 5 个随机初始值。每个算法都传入正交的混合矩阵 <span class="math notranslate nohighlight">\(\A\)</span>（数据已经进行了白化），这可以用来比较产生的正交混合矩阵 <span class="math notranslate nohighlight">\(\A_0\)</span>。我们采用 Amari 距离(Bach and Jordan, 2002<a class="footnote-reference brackets" href="#id37" id="id25">11</a>)来度量两个矩阵接近程度：</p>
<div class="math notranslate nohighlight">
\[
d(\A_0, \A)=\frac{1}{2p}\sum_{i=1}^p\Big(\frac{\sum_{j=1}^p\vert r_{ij}\vert}{\max_j\vert r_{ij}\vert}-1\Big)+
\frac{1}{2p}\sum_{j=1}^p\Big(\frac{\sum_{j=1}^p\vert r_{ij}\vert}{\max_j\vert r_{ij}\vert}-1\Big)\tag{14.97}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(r_{ij}=(\A_o\A^{-1})_{ij}\)</span>。图 14.42 的右图比较了真实矩阵与估计的矩阵的 Amari 距离（对数尺度下）。<code class="docutils literal notranslate"><span class="pre">ProDenICA</span></code>在所有的情形都可以与<code class="docutils literal notranslate"><span class="pre">FastICA</span></code>和<code class="docutils literal notranslate"><span class="pre">KernelICA</span></code>进行比较，并且在大多数混合情形下表现最好。</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id26"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p>Mardia, K., Kent, J. and Bibby, J. (1979). Multivariate Analysis, Academic Press.</p>
</dd>
<dt class="label" id="id27"><span class="brackets">2</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id6">2</a>,<a href="#id7">3</a>)</span></dt>
<dd><p>Hyvärinen, A., Karhunen, J. and Oja, E. (2001). Independent Component Analysis, Wiley, New York.</p>
</dd>
<dt class="label" id="id28"><span class="brackets">12</span><span class="fn-backref">(<a href="#id10">1</a>,<a href="#id11">2</a>)</span></dt>
<dd><p>Onton, J. and Makeig, S. (2006). Information-based modeling of event-related brain dynamics, in Neuper and Klimesch (eds), Progress in Brain Research, Vol. 159, Elsevier, pp. 99–120.</p>
</dd>
<dt class="label" id="id29"><span class="brackets"><a class="fn-backref" href="#id13">3</a></span></dt>
<dd><p>Friedman, J. and Tukey, J. (1974). A projection pursuit algorithm for exploratory data analysis, IEEE Transactions on Computers, Series C 23: 881–889.</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id14">4</a></span></dt>
<dd><p>Huber, P. (1985). Projection pursuit, Annals of Statistics 13: 435–475.</p>
</dd>
<dt class="label" id="id31"><span class="brackets">5</span><span class="fn-backref">(<a href="#id15">1</a>,<a href="#id17">2</a>)</span></dt>
<dd><p>Friedman, J. (1987). Exploratory projection pursuit, Journal of the American Statistical Association 82: 249–266.</p>
</dd>
<dt class="label" id="id32"><span class="brackets"><a class="fn-backref" href="#id16">6</a></span></dt>
<dd><p>Swayne, D., Cook, D. and Buja, A. (1991). Xgobi: Interactive dynamic graphics in the X window system with a link to S, ASA Proceedings of Section on Statistical Graphics, pp. 1–8.</p>
</dd>
<dt class="label" id="id33"><span class="brackets"><a class="fn-backref" href="#id19">7</a></span></dt>
<dd><p>Hastie, T. and Tibshirani, R. (2003). Independent components analysis through product density estimation, in S. T. S. Becker and K. Obermayer (eds), Advances in Neural Information Processing Systems 15, MIT Press, Cambridge, MA, pp. 649–656.</p>
</dd>
<dt class="label" id="id34"><span class="brackets"><a class="fn-backref" href="#id20">8</a></span></dt>
<dd><p>Silverman, B. (1986). Density Estimation for Statistics and Data Analysis, Chapman and Hall, London. <span class="xref myst">下载</span></p>
</dd>
<dt class="label" id="id35"><span class="brackets"><a class="fn-backref" href="#id21">9</a></span></dt>
<dd><p>Hastie, T. and Tibshirani, R. (1990). Generalized Additive Models, Chapman and Hall, London.</p>
</dd>
<dt class="label" id="id36"><span class="brackets"><a class="fn-backref" href="#id22">10</a></span></dt>
<dd><p>Efron, B. and Tibshirani, R. (1996). Using specially designed exponential families for density estimation, Annals of Statistics 24(6): 2431–2461.</p>
</dd>
<dt class="label" id="id37"><span class="brackets">11</span><span class="fn-backref">(<a href="#id24">1</a>,<a href="#id25">2</a>)</span></dt>
<dd><p>Bach, F. and Jordan, M. (2002). Kernel independent component analysis, Journal of Machine Learning Research 3: 1–48. <a class="reference external" href="http://www.di.ens.fr/~fbach/kernel-ica/index.htm">从作者主页下载代码和文章</a>，<span class="xref myst">备用下载</span></p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./14-Unsupervised-Learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="14.6-Non-negative-Matrix-Factorization.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">14.6 非负矩阵分解</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="14.8-Multidimensional-Scaling.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">14.8 多维缩放</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>