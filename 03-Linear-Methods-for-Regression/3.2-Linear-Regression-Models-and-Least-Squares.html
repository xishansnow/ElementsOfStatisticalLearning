
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3.2 线性回归模型和最小二乘法 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.3 选择预测变量的子集" href="3.3-Subset-Selection.html" />
    <link rel="prev" title="3.1 导言" href="3.1-Introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.3-Subset-Selection.html">
     3.3 选择预测变量的子集
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 光滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 光滑参数的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波光滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核光滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差、方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的乐观估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 “条件测试误差”还是“测试误差的期望”？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5
     <code class="docutils literal notranslate">
      <span class="pre">
       EM
      </span>
      <span class="pre">
       算法
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆叠
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14-Unsupervised-Learning/14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   （ 1 ）例子：前列腺癌
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#guass-markov">
   （ 2 ）Guass-Markov 定理
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   （ 3 ）从简单单变量回归到多重回归
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   （）多重输出
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>3.2 线性回归模型和最小二乘法</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   （ 1 ）例子：前列腺癌
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#guass-markov">
   （ 2 ）Guass-Markov 定理
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   （ 3 ）从简单单变量回归到多重回归
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   （）多重输出
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>3.2 线性回归模型和最小二乘法<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>正如第二章介绍的那样，我们有输入向量 <span class="math notranslate nohighlight">\(X^T=(X\_1,X\_2,\ldots,X\_p)\)</span>，而且想要预测实值输出变量 <span class="math notranslate nohighlight">\(Y\)</span>。线性回归模型有如下形式</p>
<div class="math notranslate nohighlight">
\[
f(X)=\beta_0+\sum\limits_{j=1}^pX_j\beta_j \tag{3.1}
\]</div>
<p>线性模型要么假设回归函数 <span class="math notranslate nohighlight">\(\mathbb{E}(Y\mid X)\)</span> 是线性的，要么假设它本身是一个合理的近似。这里 <span class="math notranslate nohighlight">\(\beta_j\)</span> 是未知的参数或系数，变量 <span class="math notranslate nohighlight">\(X_j\)</span> 可以有下列不同的来源：</p>
<ul class="simple">
<li><p>定量的输入变量</p></li>
<li><p>定量输入变量的变换，比如对数变换，平方根变换或者平方变换</p></li>
<li><p>基函数展开，比如 <span class="math notranslate nohighlight">\(X_2=X_1^2,X_3=X_1^3\)</span>，产生一个多项式的表示</p></li>
<li><p>定性输入变量水平 (level) 的数值或“虚拟”编码。举个例子，如果 <span class="math notranslate nohighlight">\(G\)</span> 是有 5 个水平的输入变量，我们可能构造 <span class="math notranslate nohighlight">\(X_j,j=1,\ldots,5\)</span> 使得 <span class="math notranslate nohighlight">\(X_j=I(G=j)\)</span>。通过一系列独立于水平的常数，整个 <span class="math notranslate nohighlight">\(X_j\)</span> 可以用来表示 <span class="math notranslate nohighlight">\(G\)</span> 的效应，因为在 <span class="math notranslate nohighlight">\(\sum_{j=1}^5X_j\beta_j\)</span> 中，其中一个 <span class="math notranslate nohighlight">\(X_j\)</span> 的系数为 1，其它的都是 0。</p></li>
<li><p>变量的交叉影响，举个例子，<span class="math notranslate nohighlight">\(X_3=X_1\cdot X_2\)</span>。</p></li>
</ul>
<p>无论 <span class="math notranslate nohighlight">\(X_j\)</span> 是哪个来源，模型关于参数都是线性的。</p>
<p>一般地，我们有一个用来估计参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的训练数据集合 <span class="math notranslate nohighlight">\((x_1,y_1),\ldots,(x_N,y_N)\)</span>。每个 <span class="math notranslate nohighlight">\(x_i=(x_{i1},x_{i2},\ldots,x_{ip})^T\)</span> 是第 <span class="math notranslate nohighlight">\(i\)</span> 种情形的特征测量值的向量。最受欢迎的估计方法是 <strong>最小二乘 (least squares)</strong>，我们取参数 <span class="math notranslate nohighlight">\(\beta=(\beta_0,\beta_1,\ldots,\beta_p)^T\)</span> 使得下式的残差平方和最小</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathrm{RSS}(\beta)&amp;=\sum\limits_{i=1}^N(y_i-f(x_i))^2\\
&amp;=\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2\tag{3.2}
\end{align*}
\end{split}\]</div>
<p>从统计学的观点来看，如果训练观测值 <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> 是从总体独立随机抽取的，则该标准是很合理的。即使 <span class="math notranslate nohighlight">\(x_i\)</span>’s 不是随机选取的，如果在给定输入 <span class="math notranslate nohighlight">\(x_i\)</span> 的条件下 <span class="math notranslate nohighlight">\(y_i\)</span> 条件独立，这个准则也是有效的。图 3.1 展示了在充满数据对 <span class="math notranslate nohighlight">\((X,Y)\)</span> 的 <span class="math notranslate nohighlight">\(\mathbb{R}^{p+1}\)</span> 维空间的最小二乘拟合的几何意义。注意到 式（ 3.2 ） 对模型 (3.1) 的有效性没有作假设，而是简单地找到对数据最好的线性拟合。无论数据是怎样产生的，最小二乘拟合直观上看是满意的，这个准则衡量了平均的拟合误差。</p>
<p><img alt="" src="../_images/fig3.1.png" /></p>
<blockquote>
<div><p>图 3.1 <span class="math notranslate nohighlight">\(X\in R^2\)</span> 中的最小二乘拟合。我们寻找关于 <span class="math notranslate nohighlight">\(X\)</span> 并且使 <span class="math notranslate nohighlight">\(Y\)</span> 的残差最小的线性函数。</p>
</div></blockquote>
<p>那我们应该怎样最小化 式（ 3.2 ）？记 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 为 <span class="math notranslate nohighlight">\(N\times (p+1)\)</span> 的矩阵，矩阵每一行为一个输入向量（在第一个位置为 1），类似地令 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 为训练集里的 <span class="math notranslate nohighlight">\(N\)</span> 维输出向量。然后我们可以将残差平方和写成如下形式</p>
<div class="math notranslate nohighlight">
\[
\mathrm{RSS}(\beta)=(\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta)\tag{3.3}
\]</div>
<p>这是含 <span class="math notranslate nohighlight">\(p+1\)</span> 个参数的二次函数。关于 <span class="math notranslate nohighlight">\(\beta\)</span> 求导有</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{ll}
\dfrac{\partial \mathrm{RSS}}{\partial \beta}&amp;=-2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta)\\
\dfrac{\partial^2 \mathrm{RSS}}{\partial \beta\partial \beta^T}&amp;=2\mathbf{X}^T\mathbf{X}
\end{array}
\tag{3.4}
\end{split}\]</div>
<p>假设 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 列满秩，因此 <span class="math notranslate nohighlight">\(\mathbf{X}^T\mathbf{X}\)</span> 是正定的，我们令一阶微分为 0，即</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta)=0\tag{3.5}
\]</div>
<p>得到唯一解</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\tag{3.6}
\]</div>
<p>在输入向量 <span class="math notranslate nohighlight">\(x_0\)</span> 下的预测值由 <span class="math notranslate nohighlight">\(\hat{f}(x_0)=(1:x_0)^T\hat{\beta}\)</span> 给出；在训练输入值下的拟合值为</p>
<div class="math notranslate nohighlight">
\[
\hat{y}=\mathbf{X}\hat{\beta}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\tag{3.7}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\hat y_i=\hat f(x_i)\)</span>。在式 式（ 3.7 ） 中出现的矩阵 <span class="math notranslate nohighlight">\(\mathbf{H=X(X^TX)^{-1}X^T}\)</span> 有时称之为“帽子”矩阵，因为其为 <span class="math notranslate nohighlight">\(\mathbf y\)</span> 戴了一顶帽子。</p>
<p><img alt="" src="../_images/fig3.2.png" /></p>
<blockquote>
<div><p>图 3.2 含两个预测的最小二乘回归的 <span class="math notranslate nohighlight">\(N\)</span> 维几何图形。输出向量 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 正交投影在输入向量 <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span> 和<span class="math notranslate nohighlight">\(\mathbf{x}_2\)</span> 张成的超平面中。投影向量 <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span> 表示最小二乘法的预测值。</p>
</div></blockquote>
<p>图 3.2 展示了在 <span class="math notranslate nohighlight">\(\mathbb{R}^N\)</span> 中最小二乘估计的另一种几何表示。我们记 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的列向量为 <span class="math notranslate nohighlight">\(x_0,x_1,\ldots,x_p\)</span>，其中 <span class="math notranslate nohighlight">\(x_0 \equiv 1\)</span>。下文中第一列都是这样假设。这些向量张成了 <span class="math notranslate nohighlight">\(\mathbb{R}^N\)</span> 的子空间，也被称作 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的列空间。我们通过选择 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 来最小化 <span class="math notranslate nohighlight">\(\mathrm{RSS}(\beta)=\lVert(\mathbf{y}-\mathbf{X}\beta)\rVert^2\)</span>，则残差向量 <span class="math notranslate nohighlight">\(\mathbf{y-\hat{y}}\)</span> 正交于子空间。式（ 3.5 ） 式描述了这种正交，然后估计值 <span class="math notranslate nohighlight">\(\hat{y}\)</span> 因此可以看成是 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 在子空间中的正交投影。帽子矩阵 <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> 计算了正交投影，因此也被称作投影矩阵。</p>
<p>可能会出现 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的列不是线性独立的，则 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 不是满秩的。举个例子，如果两个输入是完全相关的，（比如，<span class="math notranslate nohighlight">\(x_2=3x_1\)</span>）。则矩阵 <span class="math notranslate nohighlight">\(\mathbf{X^TX}\)</span> 是奇异的，并且最小二乘的系数 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 不是唯一的。然而，拟合值 <span class="math notranslate nohighlight">\(\mathbf{\hat{y}}=\mathbf{X}\hat{\beta}\)</span> 仍然是 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 在列空间 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的投影；用 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的列向量来表示这种投射的方式不止一种。当一个或多个定性输入用一种冗余的方式编码时经常出现非满秩的情形。通过重编码或去除 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 中冗余的列等方式可以解决非唯一表示的问题。大多数回归软件包会监测这些冗余并且自动采用一些策略去除冗余项。信号和图像分析中经常发生秩缺失，其中输入的个数 <span class="math notranslate nohighlight">\(p\)</span> 可以大于训练的情形个数 <span class="math notranslate nohighlight">\(N\)</span>。在这种情形下，特征经常通过滤波来降维或者对拟合进行正则化。（<a class="reference external" href="https://esl.hohoweiya.xyz/05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines/index.html#_4">5.2.3节</a>和第 18 章）</p>
<p>截至目前我们已经对数据的真实分布做的假设非常少。为了约束 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 的采样特点，我们现在假设观测值 <span class="math notranslate nohighlight">\(y\_i\)</span> 不相关，且有固定的方差 <span class="math notranslate nohighlight">\(\sigma^2\)</span>，并且 <span class="math notranslate nohighlight">\(x\_i\)</span> 是固定的（非随机）。最小二乘法的参数估计的 <strong>协方差矩阵 (variance-covariance matrix)</strong> 可以很容易从式 式（ 3.6 ） 导出：</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(\hat{\beta})=(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2\tag{3.8}
\]</div>
<blockquote>
<div><p>note “weiya 注：”
根据<a class="reference external" href="https://en.wikipedia.org/wiki/Covariance_matrix">维基百科</a>，协方差矩阵的英文表达有，covariance matrix, variance-covariance matrix, dispersion matrix 等等. 经 <a class="reference external" href="http://disq.us/p/27ffai6">&#64;fengxiang guo</a> 提醒，为避免误解，统一翻译成协方差矩阵.</p>
</div></blockquote>
<p>一般通过下式来估计方差 <span class="math notranslate nohighlight">\(\sigma^2\)</span></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}
\hat{\sigma}^2=\frac{1}{N-p-1}\sum\limits_{i=1}^N(y_i-\hat{y}_i)^2
$\\之所以分母是 $N-p-1$ 而不是 $N$，是因为 $N-p-1$ 使得 $\hat{\sigma}^2$ 为无偏估计：$E(\hat{\sigma}^2)=\sigma^2$\\为了对参数和模型进行推断，需要一些额外的假设。我们现在假设式 式（ 3.1 ） 是关于均值的正确模型；则 $Y$ 的条件期望关于 $X_1,X_2,\ldots,X_p$ 是线性的。我们也假设 $Y$ 与其期望的偏差是可加的且是正态的。因此\end{aligned}\end{align} \]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Y&amp;=\mathbb{E}(Y\mid X_1,\ldots,X_p)+\varepsilon\\
&amp;=\beta_0+\sum\limits_{j=1}^pX_j\beta_j+\varepsilon\tag{3.9}
\end{align*}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}其中误差 $\epsilon$ 是期望值为 0 方差为 $\sigma^2$ 的高斯随机变量，记作 $\varepsilon\sim N(0,\sigma^2)$\\由式 式（ 3.9 ），可以很简单地证明\end{aligned}\end{align} \]</div>
<p>\hat{\beta}\sim N(\beta,(\mathbf{X}^T\mathbf{X})^{-1}\sigma^2) \tag{3.10}
$$</p>
<p>这是一个有上述均值向量和协方差矩阵的多元正态分布。同时有</p>
<div class="math notranslate nohighlight">
\[
(N-p-1)\hat \sigma^2\sim \sigma^2\chi^2_{N-p-1}\tag{3.11}
\]</div>
<p>是一个自由度为 <span class="math notranslate nohighlight">\(N-p-1\)</span> 的卡方分布。另外，<span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 和 <span class="math notranslate nohighlight">\(\hat\sigma^2\)</span> 是统计独立的。我们利用这些分布性质得到假设检验以及对于参数 <span class="math notranslate nohighlight">\(\beta_j\)</span> 的置信区间</p>
<p><img alt="" src="../_images/fig3.3.png" /></p>
<blockquote>
<div><p>图 3.3 <span class="math notranslate nohighlight">\(t_{30}\)</span>,<span class="math notranslate nohighlight">\(t_{100}\)</span>以及标准正态三种分布的尾概率 <span class="math notranslate nohighlight">\(\mathrm{Pr}(|Z|&gt;z)\)</span>。图中显示了在 <span class="math notranslate nohighlight">\(p=0.05\)</span> 和 <span class="math notranslate nohighlight">\(0.01\)</span> 水平下检验显著性的合适分位数。当 <span class="math notranslate nohighlight">\(N\)</span> 大于 100 时 <span class="math notranslate nohighlight">\(t\)</span> 分布与标准正态分布之间的差异很小。</p>
</div></blockquote>
<p>为了检验系数 <span class="math notranslate nohighlight">\(\beta_j\)</span> 的这一假设，我们构造标准化因数或者 Z-分数。</p>
<div class="math notranslate nohighlight">
\[
z_j=\dfrac{\hat{\beta}_j}{\hat{\sigma}\sqrt{v_j}}\tag{3.12}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(v_j\)</span> 是 <span class="math notranslate nohighlight">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span> 的第 <span class="math notranslate nohighlight">\(j\)</span> 个对角元。零假设为 <span class="math notranslate nohighlight">\(\beta_j=0\)</span>，<span class="math notranslate nohighlight">\(z_j\)</span> 分布为 <span class="math notranslate nohighlight">\(t_{N-p-1}\)</span>（自由度为 <span class="math notranslate nohighlight">\(N-p-1\)</span> 的 <span class="math notranslate nohighlight">\(t\)</span> 分布），因此当 <span class="math notranslate nohighlight">\(z_j\)</span> 的绝对值较大时会拒绝零假设。如果用已知的 <span class="math notranslate nohighlight">\(\sigma\)</span> 值替换 <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>，则 <span class="math notranslate nohighlight">\(z_j\)</span> 服从标准正态分布。<span class="math notranslate nohighlight">\(t\)</span> 分布和标准正态分布在尾概率之间的差异随着样本规模增大可以忽略，因此我们一般使用正态的分位数（图 3.3）。</p>
<p>我们经常需要同时检验多个系数 (groups of coefficients) 的显著性。举个例子，检验有 <span class="math notranslate nohighlight">\(k\)</span> 个水平的分类变量是否要从模型中排除，我们需要检验用来表示水平的虚拟变量的系数是否可以全部设为 0。这里我们利用 <span class="math notranslate nohighlight">\(F\)</span> 统计量</p>
<div class="math notranslate nohighlight">
\[
F=\dfrac{(\mathrm{RSS}_0-\mathrm{RSS}_1)/(p_1-p_0)}{\mathrm{RSS}_1/(N-p_1-1)}\tag{3.13}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mathrm{RSS}_1\)</span> 是有 <span class="math notranslate nohighlight">\(p_1+1\)</span> 个参数的大模型的最小二乘法拟合的残差平方和，<span class="math notranslate nohighlight">\(\mathrm{RSS}_0\)</span> 是有 <span class="math notranslate nohighlight">\(p_0+1\)</span> 参数的小模型的最小二乘法拟合的残差平方和，其有 <span class="math notranslate nohighlight">\(p_1-p_0\)</span> 个参数约束为 0。<span class="math notranslate nohighlight">\(F\)</span> 统计量衡量了在大模型中每个增加的系数对残差平方和的改变，而且用 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 的估计值进行标准化。在高斯分布以及小模型的零假设为正确的情况下，<span class="math notranslate nohighlight">\(F\)</span> 统计量服从 <span class="math notranslate nohighlight">\(F_{p_1-p_0,N-p_1-1}\)</span> 分布。可以证明 式（ 3.12 ） 式中的 <span class="math notranslate nohighlight">\(z_j\)</span> 等于从模型中去除单系数 <span class="math notranslate nohighlight">\(\beta_j\)</span> 的 <span class="math notranslate nohighlight">\(F\)</span> 统计量（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/69">习题 3.1</a>），当 <span class="math notranslate nohighlight">\(N\)</span> 足够大时，<span class="math notranslate nohighlight">\(F_{p_1-p_0,N-p_1-1}\)</span> 近似 <span class="math notranslate nohighlight">\(\chi^2_{p_1-p_0}\)</span>。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.1”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/69">Issue 69: Ex. 3.1</a>。</p>
</div></blockquote>
<blockquote>
<div><p><strong>注解：</strong></p>
<p>在 R 语言中，可以采用 <code class="docutils literal notranslate"><span class="pre">pwr</span></code> 包来进行 power analysis。对于线性模型（比如多重回归），<code class="docutils literal notranslate"><span class="pre">pwr.f2.test(u=,</span> <span class="pre">v=,</span> <span class="pre">f2=,</span> <span class="pre">sig.level=,</span> <span class="pre">power=)</span></code> 可以用来进行 power analysis，其中 <code class="docutils literal notranslate"><span class="pre">u</span></code> 和 <code class="docutils literal notranslate"><span class="pre">v</span></code> 分别是分子和分母的自由度，<code class="docutils literal notranslate"><span class="pre">f2</span></code> 是 <strong>有效大小 (effect size)</strong>。</p>
<p>当需要衡量一个变量集对输出变量的影响，<code class="docutils literal notranslate"><span class="pre">f2</span></code> 计算公式为，</p>
<div class="math notranslate nohighlight">
\[
&gt;  f^2 = \frac{R^2}{1-R^2}\,,
&gt;\]</div>
<p>当需要衡量一个变量集优于另一个变量集的影响时，</p>
<div class="math notranslate nohighlight">
\[
&gt;  f^2 = \frac{R^2_{AB}-R^2_A}{1-R_{AB}^2}\,,
&gt;\]</div>
<p>其中 <span class="math notranslate nohighlight">\(R_A^2\)</span> 是变量集 <span class="math notranslate nohighlight">\(A\)</span> 解释的方差，<span class="math notranslate nohighlight">\(R_{AB}^2\)</span> 是变量集 <span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span> 共同解释的方差。</p>
</div></blockquote>
<blockquote>
<div><div class="highlight-none notranslate"><div class="highlight"><pre><span></span>这里举一个实际例子来说明如何在线性回归模型中进行 power analysis。
</pre></div>
</div>
<p>假设我们要研究老板的领导风格对公司员工满意度的影响，同时员工满意度还与薪酬有关，其中薪酬用 3 个变量来衡量，而老板领导风格用 4 个变量来衡量。根据以往经验，薪酬能够解释员工满意度方差的 30%，现在问题是确定多少的调查个体，使得显著度为 .05，而且 power 为 .90。求解代码为<code class="docutils literal notranslate"><span class="pre">pwr.f2.test(u=3,</span> <span class="pre">f2=(.35-.30)/(1-.35),</span> <span class="pre">sig.level=0.05,</span> <span class="pre">power=0.90)</span></code>，输出<code class="docutils literal notranslate"><span class="pre">v=184.2426</span></code>。</p>
<p>其中<code class="docutils literal notranslate"><span class="pre">u</span></code>和<code class="docutils literal notranslate"><span class="pre">v</span></code>分别被称为分子、分母自由度。结合 式（ 3.13 ） 式，我们可以看出 <span class="math notranslate nohighlight">\(p_1-p_0=u, N-p_1-1=v\)</span>，并且这里 <span class="math notranslate nohighlight">\(p_0=4, p_1=7\)</span>。换个角度看，其实式 式（ 3.13 ） 就是用于 power analysis 的检验统计量，原假设为可以将领导力水平的四个变量系数设为 0，若 <span class="math notranslate nohighlight">\(F\)</span> 足够大，则会拒绝原假设。</p>
</div></blockquote>
<p>类似地，我们可以分离式 式（ 3.10 ） 中的 <span class="math notranslate nohighlight">\(\beta_j\)</span> 得到 <span class="math notranslate nohighlight">\(\beta_j\)</span> 的置信水平为 <span class="math notranslate nohighlight">\(1-2\alpha\)</span> 的置信区间</p>
<div class="math notranslate nohighlight">
\[
(\hat{\beta}_j-z^{1-\alpha}v_j^{1/2}\hat{\sigma},\hat{\beta}_j+z^{1-\alpha}v_j^{1/2}\hat{\sigma})\tag{3.14}
\]</div>
<p>这里，<span class="math notranslate nohighlight">\(z^{1-\alpha}\)</span> 是正态分布的 <span class="math notranslate nohighlight">\(1-\alpha\)</span> 分位数。</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
z^{1-0.025}&amp;=1.96,\\
z^{1-0.05}&amp;=1.645,etc.
\end{align*}
\end{split}\]</div>
<p>因此报告 <span class="math notranslate nohighlight">\(\hat{\beta}\pm 2\cdot se(\hat{\beta})\)</span> 的标准做法是约 <span class="math notranslate nohighlight">\(95\%\)</span> 的置信区间。即便没有高斯误差的假设，区间也近似正确，因为当样本规模 <span class="math notranslate nohighlight">\(N\sim \infty\)</span> 时，覆盖范围近似为 <span class="math notranslate nohighlight">\(1-2\alpha\)</span>。</p>
<p>运用类似的方法我们可以得到全体系数向量 <span class="math notranslate nohighlight">\(\beta\)</span> 的近似置信集，记作</p>
<div class="math notranslate nohighlight">
\[
F_{\beta} = \{\beta \mid ((\hat{\beta}-\beta)^T\mathbf{X^TX}(\hat{\beta}-\beta)\le \hat{\sigma}^2\chi^{2^{(1-\alpha)}}_{p+1} \}\tag{3.15}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\chi_\ell^{2^{(1-\alpha)}}\)</span> 是 <span class="math notranslate nohighlight">\(\ell\)</span> 个自由度的卡方分布的 <span class="math notranslate nohighlight">\(1-\alpha\)</span> 的分位数：举个例子，<span class="math notranslate nohighlight">\(\chi^{2^{(1-0.05)}}\_5=11.1,\chi^{2^{(1-0.1)}}\_5=9.2\)</span>。这个关于 <span class="math notranslate nohighlight">\(\beta\)</span> 的置信集导出关于真函数 <span class="math notranslate nohighlight">\(f(x)=x^T\beta\)</span> 对应的置信集，记作 <span class="math notranslate nohighlight">\(\\{x^T\beta\mid \beta \in C_\beta\\}\)</span>（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/120">练习 3.2</a>；并见 5.2.2 节中函数置信带的例子图 5.4）</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.2”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/120">Issue 120: Ex. 3.2</a>。</p>
</div></blockquote>
<div class="section" id="id2">
<h2>（ 1 ）例子：前列腺癌<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>这个例子的数据来自 Stamey et al. (1989)<a class="footnote-reference brackets" href="#id6" id="id3">1</a>。他们检验即将接受彻底前列腺切除术的男性的前列腺特定抗原水平和一系列临床措施之间的相关性。变量有癌体积的对数值 (lcavol)、前列腺重量的对数值 (lweight)、良性前列腺增生数量 (lbph)、精囊浸润 (svi)、包膜浸透的对数值 (lcp)、Gleason 得分 (gleason)、Gleason 得分为 4 或 5 的比例 (pgg45)。表 3.1 中给出的预测变量的相关性矩阵显示了很多强相关性。第 1 章的图 1.1 是散点图矩阵，显示了变量两两间的图象。我们看到 svi 是二进制变量，gleason 是有序分类变量。举个例子，我们看到 lcavol 和 lcp 都与响应变量 lpsa 有强的相关性。我们需要联合拟合这些影响来解开预测变量和响应变量之间的关系。</p>
<p><img alt="" src="../_images/tab3.1.png" /></p>
<blockquote>
<div><p>表 3.1 前列腺癌数据中预测变量的相关系数；表 3.2 前列腺癌数据的线性模型拟合。<span class="math notranslate nohighlight">\(Z\)</span> 分数为系数除以标准误差 式（ 3.12 ）。大体上， <span class="math notranslate nohighlight">\(Z\)</span> 分数绝对值大于 2 表示在 <span class="math notranslate nohighlight">\(p=0.05\)</span> 的水平下显著不为零。</p>
</div></blockquote>
<p>我们对前列腺特定抗原的对数值 lpsa 和经过一次标准化有单位方差的预测变量进行线性模型拟合。我们随机将数据集分离得到规模为 67 的训练集以及规模为 30 的测试集。我们对训练集应用最小二乘估计得到表 3.2 中的估计值、标准误差以及 <span class="math notranslate nohighlight">\(Z\)</span> 分数。<span class="math notranslate nohighlight">\(Z\)</span> 分数定义式为 式（ 3.12 ），并且衡量了从模型中去掉该变量的影响。在 <span class="math notranslate nohighlight">\(5\%\)</span> 的水平下，<span class="math notranslate nohighlight">\(Z\)</span> 分数的绝对值大于 2 近似显著。（在我们的例子中，我们有 9 个参数，<span class="math notranslate nohighlight">\(t_{67-9}\)</span> 的分布的 0.025 尾分位数是 <span class="math notranslate nohighlight">\(\pm 2.002\)</span>！）预测变量 <code class="docutils literal notranslate"><span class="pre">lcavol</span></code> 显示了最强的影响，lweight 和 svi 的影响同样显著。注意到一旦 <code class="docutils literal notranslate"><span class="pre">lcavol</span></code> 在模型中，<code class="docutils literal notranslate"><span class="pre">lcp</span></code> 便不显著（当使用一个没有 <code class="docutils literal notranslate"><span class="pre">lcavol</span></code> 的模型，<code class="docutils literal notranslate"><span class="pre">lcp</span></code> 是强显著的）。我们也可以用 <span class="math notranslate nohighlight">\(F\)</span> 统计量 式（ 3.13 ） 一次除去一些项。举个例子，我们考虑除掉表 3.2 中所有不显著的项，<code class="docutils literal notranslate"><span class="pre">age</span></code>，<code class="docutils literal notranslate"><span class="pre">lcp</span></code>，<code class="docutils literal notranslate"><span class="pre">gleason</span></code> 以及 <code class="docutils literal notranslate"><span class="pre">pgg45</span></code>。我们得到</p>
<div class="math notranslate nohighlight">
\[
F=\dfrac{(32.81-29.43)/(9-5)}{29.43/(67-9)}=1.67\tag{3.16}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(p\)</span> 值为 0.17（<span class="math notranslate nohighlight">\(\mathrm{Pr}(F_{4,58}&gt;1.67)=0.17\)</span>），因此不是显著的。</p>
<p>测试数据的平均预测误差为 0.521。相反地，使用 <code class="docutils literal notranslate"><span class="pre">lpsa</span></code> 训练数据的平均值预测的测试误差为 1.057，称作“基本误差阶”。因此线性模型将基本误差阶降低了大概 <span class="math notranslate nohighlight">\(50\%\)</span>。下文中我们将要回到这个例子来比较不同的选择和收缩方法。</p>
</div>
<div class="section" id="guass-markov">
<h2>（ 2 ）Guass-Markov 定理<a class="headerlink" href="#guass-markov" title="Permalink to this headline">¶</a></h2>
<p>统计学中一个很有名的结论称参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的最小二乘估计在所有的线性无偏估计中有最小的方差。我们将要严谨地说明这一点，并且说明约束为无偏估计不是明智的选择。这个结论导致我们考虑本章中像岭回归的有偏估计。我们考虑任意参数 <span class="math notranslate nohighlight">\(\theta = a^T\beta\)</span> 的线性组合，举个例子，预测值 <span class="math notranslate nohighlight">\(f(x_0)=x_0^T\beta\)</span> 是这种形式。<span class="math notranslate nohighlight">\(a^T\beta\)</span> 的最小二乘估计为</p>
<div class="math notranslate nohighlight">
\[
\hat{\theta}=a^T\hat{\beta}=a^T\mathbf{(X^TX)^{-1}X^Ty}\tag{3.17}
\]</div>
<p>考虑固定 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>，则上式是关于响应变量 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 的线性函数 <span class="math notranslate nohighlight">\(\mathbf{c}_0^T\mathbf{y}\)</span>。如果我们假设线性模型是正确的，则 <span class="math notranslate nohighlight">\(a^T\hat{\beta}\)</span> 是无偏的，因为</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{E}(a^T\hat{\beta})&amp;=E(a^T\mathbf{(X^TX)^{-1}X^Ty})\\
&amp;=a^T\mathbf{(X^TX)^{-1}X^TX}\beta\\
&amp;=a^T\beta\tag{3.18}
\end{align*}
\end{split}\]</div>
<p>依据 Gauss-Markov 定理说如果 <span class="math notranslate nohighlight">\(a^T\beta\)</span> 存在其它无偏估计 <span class="math notranslate nohighlight">\(\tilde{\theta}=\mathbf{c}^T\mathbf{y}\)</span>，即 <span class="math notranslate nohighlight">\(\mathbb{E}(\mathbf{c}^T\mathbf{y})=a^T\beta\)</span>，则</p>
<div class="math notranslate nohighlight">
\[
Far(a^T\hat{\beta})\le Var(\mathbf{c}^T\mathbf{y})\tag{3.19}
\]</div>
<p>该证明（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/70">习题3.3a</a>）运用三角不等式。为了简化我们已经用单个参数 <span class="math notranslate nohighlight">\(a^T\beta\)</span> 估计来叙述结果，但若额外增加<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/71">习题3.3b</a>中的定义，则可以用整个参数向量 <span class="math notranslate nohighlight">\(\beta\)</span> 来说明Gauss-Markov 定理。</p>
<blockquote>
<div><p>note “weiya注：Ex. 3.3”
已完成，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/70">Issue 70: Ex. 3.3</a></p>
</div></blockquote>
<p>考虑 <span class="math notranslate nohighlight">\(\theta\)</span> 的估计值 <span class="math notranslate nohighlight">\(\tilde{\theta}\)</span> 的均方误差</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathrm{MSE}(\tilde{\theta})&amp;=\mathbb{E}(\tilde{\theta}-\theta)^2\\
&amp;=\mathrm{Var}(\tilde{\theta})+[\mathbb{E}(\tilde{\theta})-\theta]^2\tag{3.20}
\end{align*}
\end{split}\]</div>
<p>第一项为方差，第二项为平方偏差。Gauss-Markov 定理表明最小二乘估计在所有无偏线性估计中有最小的均方误差。然而，或许存在有较小均方误差的有偏估计。这样的估计用小的偏差来换取方差大幅度的降低。实际中也会经常使用有偏估计。任何收缩或者将最小二乘的一些参数设为 0 的方法都可能导致有偏估计。我们将在这章的后半部分讨论许多例子，包括 <strong>变量子集选择</strong> 和 <strong>岭回归</strong>。从一个更加实际的观点来看，许多模型是对事实的曲解，因此是有偏的；挑选一个合适的模型意味着要在偏差和方差之间创造某种良好的平衡。我们将在第 7 章中详细讨论这些问题。</p>
<p>正如在第 2 章中讨论的那样，均方误差与预测的正确性紧密相关。考虑在输入 <span class="math notranslate nohighlight">\(x_0\)</span> 处的新的响应变量</p>
<div class="math notranslate nohighlight">
\[
Y_0=f(x_0)+\varepsilon_0 \tag{3.21}
\]</div>
<p>其估计量 <span class="math notranslate nohighlight">\(\tilde{f}(x_0)=x_0^T\tilde{\beta}\)</span> 的期望预测误差为</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbb{E}(Y_0-\tilde{f}(x_0))^2 &amp;=\sigma^2+E(x_0^T\tilde{\beta}-f(x_0))^2\\
&amp;= \sigma^2+MSE(\tilde{f}(x_0))\tag{3.22}
\end{align*}
\end{split}\]</div>
<p>因此，预测误差的估计值和均方误差只有常数值 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 的差别，<span class="math notranslate nohighlight">\(\sigma^2\)</span> 表示了新观测 <span class="math notranslate nohighlight">\(y_0\)</span> 的方差。</p>
</div>
<div class="section" id="id4">
<h2>（ 3 ）从简单单变量回归到多重回归<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>有 <span class="math notranslate nohighlight">\(p &gt; 1\)</span> 个输入的线性模型 (3.1) 称作 <strong>多重线性回归模型</strong>。用单 (<span class="math notranslate nohighlight">\(p=1\)</span>) 变量线性模型的估计能更好理解模型 式（ 3.6 ） 的最小二乘估计，我们将在这节中指出。</p>
<p>首先假设我们有一个没有截距的单变量模型，也就是</p>
<div class="math notranslate nohighlight">
\[
Y=X\beta + \varepsilon \tag{3.23}
\]</div>
<p>最小二乘估计和残差为</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\beta}&amp;=\dfrac{\sum_1^Nx_iy_i}{\sum_1^Nx_i^2}\\
r_i &amp;= y_i -x_i\hat{\beta}
\end{align*}
\tag{3.24}
\end{split}\]</div>
<p>为了简便用向量表示，我们令 <span class="math notranslate nohighlight">\(\mathbf{y}=(y_1,\ldots,y_N)^T\)</span>，<span class="math notranslate nohighlight">\(\mathbf{x}=(x_1,\ldots,x_N)^T\)</span>，并且定义</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\langle\mathbf{x},\mathbf{y}\rangle &amp;= \sum\limits_{i=1}^Nx_iy_i\\
&amp;=\mathbf{x^Ty}\tag{3.25}
\end{align*}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{x}\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 之间的内积，于是我们可以写成</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\beta}&amp;=\dfrac{\langle \mathbf{x,y}\rangle}{\langle\mathbf{x,x} \rangle}\\
\mathbf{r}&amp;=\mathbf{y}-\mathbf{x}\hat{\beta}
\end{align*}
\tag{3.26}
\end{split}\]</div>
<blockquote>
<div><p>note “weiya 注：原书脚注”
The inner-product notation is suggestive of generalizations of linear regression to different metric spaces, as well as to probability spaces. 内积表示是线性回归模型一般化到不同度量空间（包括概率空间）建议的方式。</p>
</div></blockquote>
<p>正如我们所看到的，这个简单的单变量回归提供了多重线性回归的框架 (building block)。进一步假设输入变量 <span class="math notranslate nohighlight">\(\mathbf{x}_1,\mathbf{x_2,\ldots,x_p}\)</span>（数据矩阵 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的列）是正交的；也就是对于所有的 <span class="math notranslate nohighlight">\(j\neq k\)</span> 有<span class="math notranslate nohighlight">\(\langle x_j,x_k\rangle=0\)</span>。于是很容易得到多重最小二乘估计 <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> 等于 <span class="math notranslate nohighlight">\(\langle \mathbf{x}_j,\mathbf{y}\rangle/\langle\mathbf{x}_j,\mathbf{x}_j\rangle \)</span> ——单变量估计。换句话说，当输入变量为正交的，它们对模型中其它的参数估计没有影响。</p>
<p>正交输入变量经常发生于平衡的、设定好的实验（强制了正交），但是对于实验数据几乎不会发生。因此为了后面实施这一想法我们将要对它们进行正交化。进一步假设我们有一个截距和单输入 <span class="math notranslate nohighlight">\(\bf{x}\)</span>。则 <span class="math notranslate nohighlight">\(\bf{x}\)</span> 的最小二乘系数有如下形式</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1=\dfrac{\langle \mathbf{x}-\bar{x}\mathbf{1},\mathbf{y}\rangle}{\langle \mathbf{x}-\bar{x}\mathbf{1},\mathbf{x}-\bar{x}\mathbf{1}\rangle}\tag{3.27}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\bar{x}=\sum_ix_i/N\)</span>,且 <span class="math notranslate nohighlight">\(N\)</span> 个元素全为 1 的向量 <span class="math notranslate nohighlight">\(\mathbf{1}=\mathbf{x_0}\)</span>。我们可以将式 式（ 3.27 ） 的估计看成简单回归 式（ 3.26 ） 的两次应用。这两步是：</p>
<ol class="simple">
<li><p>在 <span class="math notranslate nohighlight">\(\bf{1}\)</span> 上回归 <span class="math notranslate nohighlight">\(\bf{x}\)</span> 产生残差 <span class="math notranslate nohighlight">\(\mathbf{z}=\mathbf{x}-\bar{x}\mathbf{1}\)</span>;</p></li>
<li><p>在残差 <span class="math notranslate nohighlight">\(\bf{z}\)</span> 上回归 <span class="math notranslate nohighlight">\(\bf{y}\)</span> 得到系数 <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>
在这个过程中，“在 <span class="math notranslate nohighlight">\(\bf{a}\)</span> 上回归 <span class="math notranslate nohighlight">\(\bf{b}\)</span>”意思是 <span class="math notranslate nohighlight">\(\bf{b}\)</span> 在 <span class="math notranslate nohighlight">\(\bf{a}\)</span> 上的无截距的简单单变量回归，产生系数 <span class="math notranslate nohighlight">\(\hat{\gamma}=\langle\mathbf{a,b}\rangle/\langle\mathbf{a,a}\rangle\)</span> 以及残差向量 <span class="math notranslate nohighlight">\(\mathbf{b}-\hat{\gamma}\mathbf{a}\)</span>。我们称 <span class="math notranslate nohighlight">\(\bf{b}\)</span> 由 <span class="math notranslate nohighlight">\(\bf{a}\)</span> 校正(adjusted)，或者关于 <span class="math notranslate nohighlight">\(\bf{a}\)</span> 正交化。</p></li>
</ol>
<p>第一步对 <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> 作关于 <span class="math notranslate nohighlight">\(\mathbf{x}_0=\mathbf{1}\)</span> 的正交化。第二步是一个利用正交预测变量 <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 简单的单变量回归。图 3.4 展示了两个一般输入 <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf{x}_2\)</span> 的过程。正交化不会改变由 <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf{x}_2\)</span> 张成的子空间，它简单地产生一个正交基来表示子空间。</p>
<p><img alt="" src="../_images/fig3.4.png" /></p>
<blockquote>
<div><p>正交输入的最小二乘回归。向量 <span class="math notranslate nohighlight">\(\mathbf{x}_2\)</span> 在向量 <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span> 上回归，得到残差向量 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>。<span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 在<span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 上的回归给出 <span class="math notranslate nohighlight">\(\mathbf{x}_2\)</span> 的系数。把 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 在 <span class="math notranslate nohighlight">\(\mathbf{x}_1\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 上的投影加起来给出了最小二乘拟合 <span class="math notranslate nohighlight">\(\mathbf{\hat{y}}\)</span>。</p>
</div></blockquote>
<p>这个方法可以推广到 <span class="math notranslate nohighlight">\(p\)</span> 个输入的情形，如算法 3.1 所示。注意到第二步的输入 <span class="math notranslate nohighlight">\(\mathbf{z}\_0,\ldots,\mathbf{z}_{j-1}\)</span> 是正交的，因此这里计算得到的简单回归的系数实际上是多重回归的系数。</p>
<p><img alt="" src="../_images/Alg3.1.png" /></p>
<!--
****
**算法 3.1** 依次正交的回归
****
1. 初始化 $\mathbf{z}_0=\mathbf{x}_0=\mathbf{1}$
2. 对于 $j=1,2,\ldots,p$
    在$\mathbf{z}_0,\mathbf{z}_1,\ldots,\mathbf{z}_{j-1}​$上对$\mathbf{x}_j​$回归得到系数$\hat{\gamma}_{\ell j}=\langle \mathbf{z}_{\ell},\mathbf{x}_j \rangle/\langle\mathbf{z}_{\ell},\mathbf{z}_{\ell}\rangle,\ell=0,\ldots,j-1$以及残差向量$\mathbf{z}_j=\mathbf{x}_j-\sum_{k=0}^{j-1}\hat{\gamma}_{kj}\mathbf{z}_k$
3. 在残差$\mathbf{z}_p$上回归$\mathbf{y}$得到系数$\hat{\beta}_p$
****
-->
<p>算法的结果为</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_p=\dfrac{\langle \mathbf{z}_p,\mathbf{y} \rangle}{\langle \mathbf{z}_p,\mathbf{z}_p \rangle}\tag{3.28}
\]</div>
<p>对第二步中的残差进行重排列，我们可以看到每个 <span class="math notranslate nohighlight">\(\mathbf x_j​\)</span> 是 <span class="math notranslate nohighlight">\(\mathbf z_k,k\le j​\)</span> 的线性组合。因为 <span class="math notranslate nohighlight">\(\mathbf z_j​\)</span> 是正交的，它们形成了 <span class="math notranslate nohighlight">\(\mathbf{X}​\)</span> 列空间的基，从而得到最小二乘在该子空间上投影为 <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}​\)</span>。因为 <span class="math notranslate nohighlight">\(\mathbf z_p​\)</span>仅与 <span class="math notranslate nohighlight">\(\mathbf x_p​\)</span> 有关（系数为 1），则式 式（ 3.28 ） 确实是 <span class="math notranslate nohighlight">\(\mathbf{y}​\)</span> 在 <span class="math notranslate nohighlight">\(\mathbf x_p​\)</span> 上多重回归的系数。</p>
<p>这一重要结果揭示了在多重回归中输入变量相关性的影响。注意到通过对 <span class="math notranslate nohighlight">\(\mathbf x_j​\)</span> 重排列，其中的任何一个都有可能成为最后一个，然后得到一个类似的结果。因此一般来说，我们已经展示了多重回归的第 <span class="math notranslate nohighlight">\(j​\)</span> 个系数是 <span class="math notranslate nohighlight">\(\mathbf{y}​\)</span> 在 <span class="math notranslate nohighlight">\(\mathbf x_{j\cdot 012...(j-1)(j+1)...,p}​\)</span> 的单变量回归，<span class="math notranslate nohighlight">\(\mathbf x_{j\cdot 012...(j-1)(j+1)...,p}​\)</span> 是 <span class="math notranslate nohighlight">\(\mathbf x_j​\)</span> 在<span class="math notranslate nohighlight">\(\mathbf x_0,\mathbf x_1,...,\mathbf x_{j-1},\mathbf{j+1},...,\mathbf{x}_p​\)</span> 回归后的残差向量：</p>
<blockquote>
<div><p>多重回归系数 <span class="math notranslate nohighlight">\(\hat\beta_j​\)</span> 表示 <span class="math notranslate nohighlight">\(\mathbf x_j​\)</span> 经过 <span class="math notranslate nohighlight">\(\mathbf x_0,\mathbf x_1,\ldots,\mathbf x_{j-1},\mathbf x_{j+1},\ldots,\mathbf x_p​\)</span> 调整后 <span class="math notranslate nohighlight">\(\mathbf x_j​\)</span> 对 <span class="math notranslate nohighlight">\(\mathbf y​\)</span> 额外的贡献</p>
</div></blockquote>
<p>如果 <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> 与某些 <span class="math notranslate nohighlight">\(\mathbf{x}_k\)</span> 高度相关，残差向量 <span class="math notranslate nohighlight">\(\mathbf{z}_p\)</span> 会近似等于 0，而且由 式（ 3.28 ） 得到的系数 <span class="math notranslate nohighlight">\(\hat{\beta}_p\)</span> 会非常不稳定。对于相关变量集中所有的变量都是正确的。在这些情形下我们可能得到所有的 Z 分数（如表 3.2 所示）都很小——相关变量集中的任何一个变量都可以删掉——然而我们不可能删掉所有的变量。由 式（ 3.28 ） 我们也得到估计方差 式（ 3.8 ） 的另一种方法</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(\hat{\beta}_p)=\dfrac{\sigma^2}{\langle \mathbf{z}_p,\mathbf{z}_p \rangle}=\dfrac{\sigma^2}{\Vert\mathbf{z}_p\Vert ^2}\tag{3.29}
\]</div>
<p>换句话说，我们估计 <span class="math notranslate nohighlight">\(\hat{\beta}_p\)</span> 的精度取决于残差向量 <span class="math notranslate nohighlight">\(\mathbf{z}_p\)</span> 的长度；它表示 <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> 不能被其他 <span class="math notranslate nohighlight">\(\mathbf{x}_k\)</span> 解释的程度。</p>
<p>算法 3.1 被称作多重回归的 Gram-Schmidt 正交化，也是一个计算估计的有用的数值策略。我们从中不仅可以得到 <span class="math notranslate nohighlight">\(\hat{\beta}_p\)</span>，而且可以得到整个多重最小二乘拟合，如<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/72">练习 3.4</a> 所示。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.4”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/72">Issue 72: Ex. 3.4</a>。</p>
</div></blockquote>
<p>我们可以用矩阵形式来表示算法 3.1 的第二步：</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X=Z\Gamma}\tag{3.30}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> 是 <span class="math notranslate nohighlight">\(\mathbf{z_j}\)</span>（按顺序）作为列向量的矩阵，<span class="math notranslate nohighlight">\(\mathbf{\Gamma}\)</span> 是值为 <span class="math notranslate nohighlight">\(\hat\gamma_{kj}\)</span> 的上三角矩阵。引入第 <span class="math notranslate nohighlight">\(j\)</span> 个对角元 <span class="math notranslate nohighlight">\(D_{jj}=\Vert \mathbf{z}_j \Vert\)</span> 的对角矩阵 <span class="math notranslate nohighlight">\(\mathbf{D}\)</span>，我们有</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{X}&amp;=\mathbf{ZD^{-1}D\Gamma}\\
&amp;=\mathbf{QR}\tag{3.31}
\end{align*}
\end{split}\]</div>
<p>称为矩阵 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的 QR 分解。这里 <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> 为 <span class="math notranslate nohighlight">\(N\times (p+1)\)</span> 正交矩阵，<span class="math notranslate nohighlight">\(\mathbf{Q^TQ=I}\)</span>，并且 <span class="math notranslate nohighlight">\(\mathbf{R}\)</span> 为 <span class="math notranslate nohighlight">\((p+1)\times (p+1)\)</span> 上三角矩阵。</p>
<p><span class="math notranslate nohighlight">\(\mathbf{QR}\)</span> 分解表示了 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 列空间的一组方便的正交基。举个例子，很容易可以看到，最小二乘的解由下式给出</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}=\mathbf{R^{-1}Q^Ty}\tag{3.32} 
\]</div>
<div class="math notranslate nohighlight">
\[
\hat{\mathbf{y}}=\mathbf{QQ^Ty}\tag{3.33}
\]</div>
<p>等式 式（ 3.32 ） 很容易求解，因为 <span class="math notranslate nohighlight">\(\mathbf{R}\)</span> 为上三角（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/72">练习 3.4</a>）</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.4”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/72">Issue 72: Ex. 3.4</a>。</p>
</div></blockquote>
</div>
<div class="section" id="id5">
<h2>（）多重输出<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>假设有多重输出 <span class="math notranslate nohighlight">\(Y_1,Y_2,\ldots,Y_K\)</span>，我们希望通过输入变量 <span class="math notranslate nohighlight">\(X_0,X_1,X_2,\ldots,X_p\)</span> 去预测。我们假设对于每一个输出变量有线性模型</p>
<div class="math notranslate nohighlight">
\[
Y_k=\beta_{0k}+\sum\limits_{j=1}^pX_j\beta_{jk}+\varepsilon_k \tag{3.34}
\]</div>
<div class="math notranslate nohighlight">
\[
= f_k(X)+\varepsilon_k \tag{3.35}
\]</div>
<p>有 <span class="math notranslate nohighlight">\(N\)</span> 个训练情形时我们可以将模型写成矩阵形式</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Y=XB+E}\tag{3.36}
\]</div>
<p>这里 <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> 为 <span class="math notranslate nohighlight">\(N\times K\)</span> 的响应矩阵，<span class="math notranslate nohighlight">\(ik\)</span> 处值为 <span class="math notranslate nohighlight">\(y_{ik}\)</span>，<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 为 <span class="math notranslate nohighlight">\(N\times (p+1)\)</span> 的输入矩阵，<span class="math notranslate nohighlight">\(\mathbf{B}\)</span> 为 <span class="math notranslate nohighlight">\((p+1)\times K\)</span> 的系数矩阵 <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> 为 <span class="math notranslate nohighlight">\(N\times K\)</span> 的误差矩阵。单变量损失函数 式（ 3.2 ） 的直接推广为</p>
<div class="math notranslate nohighlight">
\[
\mathrm{RSS}(\mathbf{B})=\sum\limits_{k=1}^K\sum\limits_{i=1}^N(y_{ik}-f_k(x_i))^2\tag{3.37}
\]</div>
<div class="math notranslate nohighlight">
\[
= \mathrm{tr}[\mathbf{(Y-XB)^T(Y-XB)}]\tag{3.38}
\]</div>
<p>最小二乘估计有和前面一样的形式</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\hat{B}=(X^TX)^{-1}X^TY}\tag{3.39}
\]</div>
<p>因此第 <span class="math notranslate nohighlight">\(k\)</span> 个输出的系数恰恰是 <span class="math notranslate nohighlight">\(\mathbf{y}_k\)</span> 在 <span class="math notranslate nohighlight">\(\mathbf{x}_0,\mathbf{x}_1,\ldots,\mathbf{x}_p\)</span> 上回归的最小二乘估计。多重输出不影响其他的最小二乘估计。</p>
<p>如果式 式（ 3.34 ） 中的误差 <span class="math notranslate nohighlight">\(\varepsilon = (\varepsilon_1,\ldots,\varepsilon_K)\)</span> 相关，则似乎更恰当的方式是修正 式（ 3.37 ） 来改成多重变量版本。特别地，假设 <span class="math notranslate nohighlight">\(\mathrm{Cov}(\varepsilon)=\Sigma\)</span>，则 <strong>多重变量加权准则</strong> 为</p>
<div class="math notranslate nohighlight">
\[
\mathrm{RSS}(\mathbf{B;\Sigma})=\sum\limits_{i=1}^N(y_i-f(x_i))^T\Sigma^{-1}(y_i-f(x_i))\tag{3.40}
\]</div>
<p>这可以由 <strong>多变量高斯定理</strong> 自然得出。这里 <span class="math notranslate nohighlight">\(f(x)\)</span> 为向量函数 <span class="math notranslate nohighlight">\((f_1(x),\ldots,f_K(x))\)</span>，而且 <span class="math notranslate nohighlight">\(y_i\)</span> 为观测值 <span class="math notranslate nohighlight">\(i\)</span> 的 <span class="math notranslate nohighlight">\(K\)</span> 维响应向量。然而，可以证明解也是由式 式（ 3.39 ） 给出；<span class="math notranslate nohighlight">\(K\)</span> 个单独的回归忽略了相关性（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/73">练习 3.11</a>）。如果 <span class="math notranslate nohighlight">\(\Sigma_i\)</span> 对于不同观测取值不同，则不再是这种情形，而且关于 <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> 的解不再退化 (decouple).</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.11”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/73">Issue 73: Ex. 3.11</a>。</p>
</div></blockquote>
<p>在 3.7 节我们继续讨论多重输出的问题，并且考虑需要合并回归的情形。</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id6"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p>Stamey, T., Kabalin, J., McNeal, J., Johnstone, I., Freiha, F., Redwine, E. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate II radical prostatectomy treated patients, Journal of Urology 16: 1076–1083.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./03-Linear-Methods-for-Regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="3.1-Introduction.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">3.1 导言</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3.3-Subset-Selection.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">3.3 选择预测变量的子集</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>