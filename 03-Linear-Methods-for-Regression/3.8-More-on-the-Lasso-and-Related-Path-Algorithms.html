
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3.8 Lasso 和相关路径算法的补充 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.9 计算上的考虑" href="3.9-Computational-Considerations.html" />
    <link rel="prev" title="3.7 多输出的收缩和选择" href="3.7-Multiple-Outcome-Shrinkage-and-Selection.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.3-Subset-Selection.html">
     3.3 选择预测变量的子集
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 平滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 平滑参数
     <span class="math notranslate nohighlight">
      \(\lambda\)
     </span>
     的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波平滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核平滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     维空间中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     维空间中的结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差、方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的乐观估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 “条件测试误差”还是“测试误差的期望”？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5
     <code class="docutils literal notranslate">
      <span class="pre">
       EM
      </span>
      <span class="pre">
       算法
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆叠
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14-Unsupervised-Learning/14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#incremental-forward-stagewise-regression">
   （1 ）增长的向前逐渐回归 (Incremental Forward Stagewise Regression)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   （ 2 ）分段线性路径算法
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dantzig">
   （ 3 ）Dantzig 选择器
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-grouped-lasso">
   （ 4 ）The Grouped Lasso
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id20">
   （ 5 ）lasso 的更多性质
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pathwise-coordinate-optimization">
   （ 6 ）Pathwise Coordinate Optimization
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>3.8 Lasso 和相关路径算法的补充</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#incremental-forward-stagewise-regression">
   （1 ）增长的向前逐渐回归 (Incremental Forward Stagewise Regression)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   （ 2 ）分段线性路径算法
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dantzig">
   （ 3 ）Dantzig 选择器
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-grouped-lasso">
   （ 4 ）The Grouped Lasso
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id20">
   （ 5 ）lasso 的更多性质
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pathwise-coordinate-optimization">
   （ 6 ）Pathwise Coordinate Optimization
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="lasso">
<h1>3.8 Lasso 和相关路径算法的补充<a class="headerlink" href="#lasso" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>自从 LAR 算法（Efron et al., 2004<a class="footnote-reference brackets" href="#id41" id="id1">1</a>）的提出，许多研究都在发展对于不同问题的正则化拟合算法。另外，<span class="math notranslate nohighlight">\(L_1\)</span> 正则有它自己的用处，它促进了信号处理领域的 <strong>压缩传感 (compressed sensing)</strong> 的发展（Donoho, 2006a<a class="footnote-reference brackets" href="#id42" id="id2">2</a>; Candes, 2006<a class="footnote-reference brackets" href="#id43" id="id3">3</a>）。在这部分我们讨论一些相关的想法和以 LAR 算法为先驱的其它路径算法。</p>
<div class="section" id="incremental-forward-stagewise-regression">
<h2>（1 ）增长的向前逐渐回归 (Incremental Forward Stagewise Regression)<a class="headerlink" href="#incremental-forward-stagewise-regression" title="Permalink to this headline">¶</a></h2>
<p>这里我们提出另外一种类似 LAR 的算法，这次集中在 <strong>向前逐渐回归 (Forward Stagewise Regression)</strong>。有趣地是，理解一个灵活的非线性回归过程 (boosting) 的努力导出了线性模型的一个新算法（LAR）。在阅读本书的第一版时，<span class="xref myst">第 16 章</span>的向前逐渐算法 16.1，Brad Efron 意识到对于线性模型，可以明确地构造出如图 3.10 所示的分段线性的 lasso 路径。这促使他提出 <a class="reference external" href="3.4-Shrinkage-Methods/index.html#_2">3.4.4 节</a>介绍的 LAR 过程，以及这里提到的 <strong>向前逐渐回归 (forward-stagewise regression)</strong> 的增长版本。</p>
<p><img alt="" src="../_images/alg3.4.png" /></p>
<!--
****
**算法 3.4** 增长的向前逐渐回归——$FS_\epsilon$
****
1. 从残差向量$\mathbf r$等于$\mathbf y$开始，$\beta_1,\beta_2,\ldots,\beta_p=0$. 所有的预测变量进行标准化使得均值为0、方差为1.
2. 寻找与残差向量$\mathbf r$最相关的预测变量$\mathbf x_j$
3. 更新$\beta_j\leftarrow\beta_j+\delta_j$, 其中$\delta_j=\epsilon\cdot sign[\langle \mathbf x_j,\mathbf r\rangle]$并且$\epsilon>0$是一个很小的步长，然后令$\mathbf r=\mathbf r-\delta_j\mathbf x_j$
4. 重复步骤2和步骤3，直到所有的残差向量与所有的预测变量都不相关。

****
-->
<p>考虑 <span class="xref myst">16.2 节</span> 提出的 forward-stagewise boosting 算法 16.1 的线性版本。它通过重复更新与当前残差最相关的变量的系数（乘以一个小量 <span class="math notranslate nohighlight">\(\epsilon\)</span>）得到系数曲线。算法 3.4 给出了具体的细节。图 3.19（左边）展示了前列腺癌数据中步长 <span class="math notranslate nohighlight">\(\epsilon=0.01\)</span> 的过程。如果 <span class="math notranslate nohighlight">\(\delta_j=\langle \mathbf x_j,\mathbf r\rangle\)</span>（残差在第 <span class="math notranslate nohighlight">\(j\)</span> 个预测变量的最小二乘系数），则这恰恰是 <a class="reference external" href="3.3-Subset-Selection/index.html#forward-stagewis">3.3 节</a> 中介绍的一般 <strong>向前逐渐过程 (FS)</strong>。</p>
<p><img alt="" src="../_images/fig3.19.png" /></p>
<p>这里我们主要对小的 <span class="math notranslate nohighlight">\(\epsilon\)</span> 值感兴趣。令 <span class="math notranslate nohighlight">\(\epsilon\rightarrow 0\)</span> 则得到图 3.19 的右图，在这种情形下与图 3.10 的 lasso 路径相同。我们称这个极限过程为 <strong>无穷小的向前逐渐回归 (infinitesimal forward stagewise regression)</strong> 或者 <span class="math notranslate nohighlight">\(FS_0\)</span> 。这个过程在非线性、自适应方法中有着很重要的作用，比如 boosting（<span class="xref myst">第 10</span> 和 <span class="xref myst">16 章</span>），并且是增长的向前逐渐回归的，这是最能经得起理论分析的版本。由于它与 boosting 的关系，Buhlmann and Hothorn (2007)<a class="footnote-reference brackets" href="#id44" id="id4">4</a>称这个过程为”L2boost”。</p>
<p>Efron 最初认为 LAR 算法 3.2 是 <span class="math notranslate nohighlight">\(FS_0\)</span> 的一个实现，允许每个连结变量 (tied predictor) 以一种平衡的方式更新他们的系数，并且在相关性方面保持连结。然而，他接着意识到 LAR 在这些连结预测变量中的最小二乘拟合可以导致系数向相反的方向移动到它们的相关系数，这在算法 3.4 中是不可能发生的。下面对 LAR 算法的修正实现了 <span class="math notranslate nohighlight">\(FS_0\)</span>：</p>
<blockquote>
<div><p><strong>注解：</strong>
直观上看，在算法 3.2 的第 4 步中，系数朝着联合最小二乘方向移动，注意此时方向与最小二乘方向可能一致或者相反。然而算法 3.4 中的移动方向始终与最小二乘方向保持一致。因此需要算法 3.2b 的修改。</p>
</div></blockquote>
<p><img alt="" src="../_images/alg3.2b.png" /></p>
<!--
****
**算法 3.2b** 最小角回归：$FS_0$修正
****
4.通过求解下面的约束最小二乘问题找到新的方向

$$
\underset{b}{min}\Vert\mathbf r-\mathbf X_{\mathcal A}b\Vert^2_2 \;s.t.\; b_js_j\ge 0,\;j\in\mathcal A

$$

其中，$s_j$是$\langle\mathbf x_j,\mathbf r \rangle$的方向。
****
-->
<p>这个修正相当于一个非负的最小二乘拟合, 保持系数的符号与相关系数的符号一致。可以证明它实现了对于最大相关性的连结变量的无限小”更新”的最优平衡。（Hastie et al.，2007<a class="footnote-reference brackets" href="#id45" id="id5">5</a>）。类似 lasso，全 <span class="math notranslate nohighlight">\(FS_0\)</span> 路径可以通过 LAR 算法非常有效地计算出来。</p>
<p>作为这些事实的结果，如果 LAR 图象是单调不减或者单调不增，如 3.19 所示，则 LAR，lasso，以及 <span class="math notranslate nohighlight">\(FS_0\)</span> 这三种算法给出了相同的图象。如果图象不是单调的但是不穿过 0，则 LAR 和 lasso 是一样的。</p>
<p>因为 <span class="math notranslate nohighlight">\(FS_0\)</span> 与 lasso 不同，很自然地问它是否优化了准则。答案比 lasso 更加的复杂；<span class="math notranslate nohighlight">\(FS_0\)</span> 系数曲线是微分方程的一个解。尽管 lasso 在降低系数向量 <span class="math notranslate nohighlight">\(\beta\)</span> 的 <span class="math notranslate nohighlight">\(L_1\)</span> 范数的单位残差平方和增长方面实现了最优化，但 <span class="math notranslate nohighlight">\(FS_0\)</span> 在沿着系数路径的 <span class="math notranslate nohighlight">\(L_1\)</span> 弧长的单位增长是最优的。因此它的系数曲线不会经常改变方向。</p>
<p><span class="math notranslate nohighlight">\(FS_0\)</span> 比 lasso的约束更强，事实上也可以看成是 lasso 的单调版本；见图 16.3 生动的例子。<span class="math notranslate nohighlight">\(FS_0\)</span> 可能在 <span class="math notranslate nohighlight">\(p&gt;&gt;N\)</span> 情形下很有用，它的系数曲线会更加的光滑，因此比 lasso 有更小的方差。更多关于 <span class="math notranslate nohighlight">\(FS_0\)</span> 的细节将在 16.2.3 节给出以及 Hastie et al. (2007)<a class="footnote-reference brackets" href="#id45" id="id6">5</a>。图 3.16 包含了<span class="math notranslate nohighlight">\(FS_0\)</span>, 它的表现非常类似于 lasso。</p>
</div>
<div class="section" id="id7">
<h2>（ 2 ）分段线性路径算法<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>最小角回归过程探索了 lasso 解的路径分段线性的本质。这导出了其他正则化问题类似的“路径算法”。假设我们求解</p>
<div class="math notranslate nohighlight">
\[
\hat \beta(\lambda)=\mathrm{argmin}_\beta[R(\beta)+\lambda J(\beta)]\tag{3.76}
\]</div>
<div class="math notranslate nohighlight">
\[
F(\beta)=\sum\limits_{i=1}^NL(y_i,\beta_0+\sum_{j=1}^px_{ij}\beta_j)\tag{3.77}
\]</div>
<p>其中损失函数 <span class="math notranslate nohighlight">\(L\)</span> 和惩罚函数 <span class="math notranslate nohighlight">\(J\)</span> 都是凸函数。则下面是解的路径 <span class="math notranslate nohighlight">\(\hat\beta(\lambda)\)</span> 为分段线性的充分条件(Rosset and Zhu, 2007)<a class="footnote-reference brackets" href="#id46" id="id8">6</a></p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(R\)</span> 作为 <span class="math notranslate nohighlight">\(\beta\)</span> 的函数是二次的或者是分段二次</p></li>
<li><p><span class="math notranslate nohighlight">\(J\)</span> 关于 <span class="math notranslate nohighlight">\(\beta\)</span> 分段线性</p></li>
</ol>
<p>这也意味着（原则上）解的路径可以有效地计算出来。例子包括平方损失和绝对误差损失，“Huberized”损失，以及关于 <span class="math notranslate nohighlight">\(\beta\)</span> 的 <span class="math notranslate nohighlight">\(L_1, L_\infty\)</span> 惩罚。另一个例子是支持向量机中的“hinge loss”。那里损失是分段线性，惩罚是二次的。有趣的是，这导出了对偶空间的分段线性路径算法；更多的细节在 <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html"><span class="doc std std-doc">12.3.5 节</span></a>给出。</p>
<blockquote>
<div><p>note “weiya 注：Huber Loss &amp; Hinge Loss”
Huber loss 损失函数为：</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\begin{split}
\rho(t;\lambda) = \begin{cases}
    \lambda \vert t\vert - \lambda^2/2 &amp; \text{ if }\vert t\vert &gt;\lambda\\
    t^2/2 &amp; \text{ if }\vert t\vert \le \lambda
    \end{cases}    
\end{split}\]</div>
<p>其图象为
<img alt="" src="../_images/huber.png" /></p>
<p><a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/178">SLS 的 Ex. 2.11 <strong>(已解决！)</strong></a> 讨论了 Huber 损失函数与 <span class="math notranslate nohighlight">\(\ell_1\)</span>-norm 的等价性。</p>
<p>Hinge Loss 是用于分类器的损失函数，定义为</p>
<div class="math notranslate nohighlight">
\[
L(y, f) = [1-yf]_+\,,    
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(f\in\{-1,+1\}\)</span>.</p>
</div>
<div class="section" id="dantzig">
<h2>（ 3 ）Dantzig 选择器<a class="headerlink" href="#dantzig" title="Permalink to this headline">¶</a></h2>
<p>Candes and Tao (2007)<a class="footnote-reference brackets" href="#id47" id="id9">7</a> 提出下面的准则：</p>
<div class="math notranslate nohighlight">
\[
\mathrm{min}_\beta\Vert\beta\Vert_1\text{ subject to }\Vert \mathbf X^T(\mathbf y-\mathbf X\beta)\Vert_\infty\le s\tag{3.78}
\]</div>
<p>他们称这个解为 <strong>Dantzig selector (DS)</strong>。可以等价地写成</p>
<div class="math notranslate nohighlight">
\[
\min_\beta \Vert \mathbf X^T(\mathbf y-\mathbf X\beta)\Vert_\infty\text{ subject to } \Vert\beta\Vert_1\le t\tag{3.79}
\]</div>
<p>这里 <span class="math notranslate nohighlight">\(\Vert\cdot\Vert_\infty\)</span> 为 <span class="math notranslate nohighlight">\(L_\infty\)</span> 范数，也就是该向量中绝对值最大的组分。这种形式类似 lasso，用梯度绝对值的最大值替换平方误差损失。注意到当 <span class="math notranslate nohighlight">\(t\)</span> 变大，如果 <span class="math notranslate nohighlight">\(N &lt; p\)</span>，则两个过程都会得到最小二乘解。如果 <span class="math notranslate nohighlight">\(p\ge N\)</span>，它们都得到最小的 <span class="math notranslate nohighlight">\(L_1\)</span> 范数的最小二乘解。然而，对于较小的 <span class="math notranslate nohighlight">\(t\)</span>，DS 过程的解的路径与 lasso 不同。</p>
<p>Candes and Tao (2007)<a class="footnote-reference brackets" href="#id47" id="id10">7</a>证明了求解 DS 是线性规划问题；为了纪念 George Dantzig（线性规划中单纯形法的发明者），因此称为 Dantzig。他们也证明了该方法的一系列有趣的数学问题，这些性质与重建潜在的稀疏系数向量的能力有关。如 Bickel et al. (2008)<a class="footnote-reference brackets" href="#id48" id="id11">13</a> 所证明，这些性质对于 lasso 也适用。</p>
<p>不幸的是 DS 方法的运算性质不够令人满意。这个方法想法上与 lasso 类似，特别是当我们观察 lasso 的平稳条件 式（ 3.58 ）。</p>
<blockquote>
<div><p>note “Recall”</p>
<div class="math notranslate nohighlight">
\[
\mathbf x_j^T(\mathbf y-\mathbf X\beta)=\lambda\cdot \mathrm{sign}(\beta_j),\forall j\in {\mathcal B}\tag{3.58}    
&gt;\]</div>
</div></blockquote>
<p>和 LAR 算法一样，对于活跃集中的所有变量，lasso 保持着与当前残差相同的内积（以及相关系数），并且将它们的系数向残差平方和的最优下降方向变化。在这个过程中，相同的相关系数单调下降（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/100">练习 3.23</a>），并且在任何时刻这个相关性大于非活跃集中的变量。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.23”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/100">Issue 100: Ex. 3.23</a>，欢迎讨论交流。</p>
</div></blockquote>
<p>而 Dantzig 选择器试图最小化当前残差与所有变量之间的最大内积。因此它可以达到比 lasso 更小的最大值，但是在这一过程中会发生奇怪的现象。如果活跃集的大小为 <span class="math notranslate nohighlight">\(m\)</span>，则会有 <span class="math notranslate nohighlight">\(m\)</span> 个变量与最大相关性绑在一起。然而，这些变量不需要与活跃集重合！因此它可以在模型中包含这样一个变量，其与当前残差的相关性小于不在活跃集中的变量与残差的相关性 (Efron et al., 2007<a class="footnote-reference brackets" href="#id49" id="id12">14</a>)。这似乎不合理，而且有时会导致较差的预测误差。Efron et al. (2007)<a class="footnote-reference brackets" href="#id49" id="id13">14</a> 也证明了随着正则化参数 <span class="math notranslate nohighlight">\(s\)</span> 的变化，DS 可能得到非常不稳定的系数曲线。</p>
</div>
<div class="section" id="the-grouped-lasso">
<h2>（ 4 ）The Grouped Lasso<a class="headerlink" href="#the-grouped-lasso" title="Permalink to this headline">¶</a></h2>
<p>在一些问题中，预测变量属于预定义的群体中；举个例子，属于同一个生物路径的基因，或者表示类别型数据层次的指示变量（哑变量）。在这种情形中，或许想要对群体中每个成员一起进行收缩或选择。Grouped lasso 便是一种实现方式。假设 <span class="math notranslate nohighlight">\(p\)</span> 个预测变量被分到 <span class="math notranslate nohighlight">\(L\)</span> 个群中，在第 <span class="math notranslate nohighlight">\(\ell\)</span> 个群中有 <span class="math notranslate nohighlight">\(p_\ell\)</span> 个成员。为了简便，我们采用矩阵 <span class="math notranslate nohighlight">\(\mathbf X_\ell\)</span> 来表示对应第 <span class="math notranslate nohighlight">\(\ell\)</span> 个群的预测变量，对应的系数向量为<span class="math notranslate nohighlight">\(\beta_\ell\)</span>。grouped-lasso 最小化下面的凸准则</p>
<div class="math notranslate nohighlight">
\[
\underset{\beta\in \mathbb{R}^p}{\mathrm{min}}\Big(\Vert \mathbf y-\beta_0\boldsymbol 1-\sum\limits_{\ell=1}^L\mathbf X_\ell\beta_\ell\Vert_2^2+\lambda \sum\limits_{\ell=1}^L\sqrt{p_\ell}\Vert \beta_\ell\Vert_2\Big)\tag{3.80}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\sqrt{p_\ell}\)</span> 项对应不同的群体大小，并且 <span class="math notranslate nohighlight">\(\Vert\cdot\Vert_2\)</span> 是欧几里得范数。因为一个向量 <span class="math notranslate nohighlight">\(\beta_\ell\)</span> 的欧式范数为 0 当且仅当其各组分都为 0，这个过程保证了群体层次和个体水平的稀疏性。也就是，对于某些 <span class="math notranslate nohighlight">\(\lambda\)</span>，预测变量的整个群体都排除在模型之外。这个过程由 Bakin (1999)<a class="footnote-reference brackets" href="#id50" id="id14">8</a> 和 Lin and Zhang (2006)<a class="footnote-reference brackets" href="#id51" id="id15">9</a> 提出，以及 Yuan and Lin (2007)<a class="footnote-reference brackets" href="#id52" id="id16">10</a> 的研究和推广。推广包括更一般的 <span class="math notranslate nohighlight">\(L_2\)</span> 范数 <span class="math notranslate nohighlight">\(\Vert \eta\Vert=(\eta^TK\eta)^{1/2}\)</span>，并且允许重复的预测变量 (Zhao et al., 2008<a class="footnote-reference brackets" href="#id53" id="id17">11</a>)。拟合离散的可加模型的方法之间也有联系（Lin and Zhang, 2006<a class="footnote-reference brackets" href="#id51" id="id18">9</a>; Ravikumar et al., 2008<a class="footnote-reference brackets" href="#id54" id="id19">12</a>）</p>
</div>
<div class="section" id="id20">
<h2>（ 5 ）lasso 的更多性质<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h2>
<p>许多作者已经研究了当 <span class="math notranslate nohighlight">\(N\)</span> 和 <span class="math notranslate nohighlight">\(p\)</span> 增长时，lasso 的能力以及重建模型相关的过程。这个工作的例子有 Knight and Fu (2000)<a class="footnote-reference brackets" href="#id55" id="id21">15</a>, Greenshtein and Ritov (2004)<a class="footnote-reference brackets" href="#id56" id="id22">16</a>, Tropp (2004)<a class="footnote-reference brackets" href="#id57" id="id23">17</a>, Donoho (2006b)<a class="footnote-reference brackets" href="#id58" id="id24">18</a>, Meinshausen (2007)<a class="footnote-reference brackets" href="#id59" id="id25">19</a>, Meinshausen and Bühlmann (2006)<a class="footnote-reference brackets" href="#id60" id="id26">20</a>, Tropp (2006)<a class="footnote-reference brackets" href="#id61" id="id27">21</a>, Zhao and Yu (2006)<a class="footnote-reference brackets" href="#id62" id="id28">22</a>, Wainwright (2006)<a class="footnote-reference brackets" href="#id63" id="id29">23</a>, 以及 Bunea et al. (2007)<a class="footnote-reference brackets" href="#id64" id="id30">24</a>。举个例子，Donoho (2006b)<a class="footnote-reference brackets" href="#id58" id="id31">18</a> 集中在 <span class="math notranslate nohighlight">\(p&gt;N\)</span> 的情形并且当边界 <span class="math notranslate nohighlight">\(t\)</span> 变大时 lasso 的解。极限情形下，这给出了在所有零训练误差的模型中最小的 <span class="math notranslate nohighlight">\(L_1\)</span> 范数解。他证明了对模型矩阵 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 加上具体的假设，如果真实模型为稀疏的，则解能以高概率识别出正确的预测变量。</p>
<p>许多这领域的结果对模型矩阵假设了如下条件</p>
<div class="math notranslate nohighlight">
\[
\underset{j\in \mathcal S^c}{\mathrm{max}}\Vert \mathbf x_j^T\mathbf X_{\mathcal S}(\mathbf X_{\mathcal S}^T\mathbf X_{\mathcal S})^{-1}\Vert_1\le (1-\epsilon)\text{ for some }\epsilon\in (0, 1]\tag{3.81}
\]</div>
<p>这里 <span class="math notranslate nohighlight">\(\mathcal S\)</span> 标记真实的潜在模型中非零系数特征的子集，<span class="math notranslate nohighlight">\(\mathbf X_{\mathcal S}\)</span> 是 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 中对应的这些特征的列。类似地，<span class="math notranslate nohighlight">\(\mathcal S^c\)</span>是真实系数为 0 的特征的集合，<span class="math notranslate nohighlight">\(\mathbf X_{\mathcal S^c}\)</span> 是对应的列。这说明 <span class="math notranslate nohighlight">\(\mathbf X_{\mathcal S^c}\)</span> 的列在 <span class="math notranslate nohighlight">\(\mathbf X_{\mathcal S}\)</span> 上的最小二乘系数不会太大，也就是，信号变量 <span class="math notranslate nohighlight">\(\mathcal S\)</span> 与冗余变量 <span class="math notranslate nohighlight">\(\mathcal S^c\)</span> 之间不是高度相关。</p>
<p>考虑这些系数本身，lasso 收缩导致非零系数的估计偏向 0，并且一般地他们不是一致的。降低这种偏差的一种方式是运行 lasso 来识别非零系数的集合，接着对选出的特征进行无约束线性模型拟合。另外，也可以采用 lasso 来选择非零预测变量，接着再次运用 lasso，但是从第一步开始便只用选择出的变量。这称为 relaxed lasso (Meinshausen, 2007<a class="footnote-reference brackets" href="#id59" id="id32">19</a>)。这个想法是采用交叉验证来估计 lasso 初始的惩罚参数，然后接着对选择出的变量再用一次惩罚参数。因为第二步中的变量与噪声变量之间的竞争变小，所以交叉验证会趋向于选择较小的 <span class="math notranslate nohighlight">\(\lambda\)</span>，因此它们的系数会比初始估计时收缩得要小。</p>
<p>另外，也可以修改 lasso 惩罚函数使得更大的系数收缩得不要太剧烈；Fan and Li (2005)<a class="footnote-reference brackets" href="#id65" id="id33">25</a>的 <strong>平稳削减绝对偏差法 (smoothly clipped absolute deviation, SCAD)</strong> 用 <span class="math notranslate nohighlight">\(J_a(\beta,\lambda)\)</span> 替换 <span class="math notranslate nohighlight">\(\lambda\vert\beta \vert\)</span>，其中对于 <span class="math notranslate nohighlight">\(a\ge 2\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{dJ_a(\beta,\lambda)}{d\beta}=\lambda \cdot \mathrm{sign}(\beta)[I(\vert \beta\vert\le \lambda)+\frac{(a\lambda-\vert \beta\vert)_+}{(a-1)\lambda}I(\vert\beta\vert&gt;\lambda)]\tag{3.82}
\]</div>
<p>方括号中的第二项降低了 lasso 对于较大 <span class="math notranslate nohighlight">\(\beta\)</span> 的收缩程度，在极限状态时，当 <span class="math notranslate nohighlight">\(a\rightarrow \infty\)</span>，没有收缩。图 3.20 显示了 SCAD 惩罚，以及 lasso 和 <span class="math notranslate nohighlight">\(\beta^{1-\nu} \)</span>。然而这个准则不是凸的，这是一个缺陷，因为它会使得计算变得很困难。adaptive lasso (Zou, 2006)<a class="footnote-reference brackets" href="#id66" id="id34">26</a>采用形如 <span class="math notranslate nohighlight">\(\sum_{j=1}^pw_j\vert\beta_j\vert\)</span> 的加权惩罚，其中 <span class="math notranslate nohighlight">\(w_j=1/\vert\hat\beta_j\vert^\nu\)</span>，<span class="math notranslate nohighlight">\(\hat\beta_j\)</span> 是一般最小二乘估计并且 <span class="math notranslate nohighlight">\(\nu &gt;0\)</span>。这是对 <a class="reference external" href="3.4-Shrinkage-Methods/index.html#lasso_1">3.4.3 节</a>中讨论的 <span class="math notranslate nohighlight">\(\vert\beta\vert^q\)</span> 惩罚的实际近似（这里 <span class="math notranslate nohighlight">\(q=1-\nu\)</span>）。adaptive lasso 在保证 lasso 吸引人的凸性基础上还得到了参数的一致估计。</p>
<p><img alt="" src="../_images/fig3.20.png" /></p>
</div>
<div class="section" id="pathwise-coordinate-optimization">
<h2>（ 6 ）Pathwise Coordinate Optimization<a class="headerlink" href="#pathwise-coordinate-optimization" title="Permalink to this headline">¶</a></h2>
<p>一种替代计算 lasso 的 LARS 算法是 <strong>简单坐标下降 (simple coordinate descent)</strong>。这个想法由 Fu (1998)<a class="footnote-reference brackets" href="#id67" id="id35">27</a> 和 Daubechies et al. (2004)<a class="footnote-reference brackets" href="#id68" id="id36">28</a> 提出，后来被 Friedman et al. (2007)<a class="footnote-reference brackets" href="#id69" id="id37">29</a>, Wu and Lange (2008)<a class="footnote-reference brackets" href="#id70" id="id38">30</a> 和其他人研究及推广。想法是固定 Lagrangian 形式 式（ 3.52 ） 中的惩罚参数 <span class="math notranslate nohighlight">\(\lambda\)</span>，在控制其它参数固定不变时，相继地优化每一个参数。</p>
<blockquote>
<div><p>note “Recall”</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}^{lasso}=\underset{\beta}{\arg\min}\Big\{\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2+\lambda\sum\limits_{j=1}^p\vert\beta_j\vert\Big\}\tag{3.52}    
&gt; \]</div>
<p>假设预测变量都经过标准化得到 0 均值和单位范数。用 <span class="math notranslate nohighlight">\(\tilde\beta_k(\lambda)\)</span> 表示惩罚参数为 <span class="math notranslate nohighlight">\(\lambda\)</span> 时对 <span class="math notranslate nohighlight">\(\beta_k\)</span> 的当前估计。我们可以分离出 式（ 3.52 ） 的 <span class="math notranslate nohighlight">\(\beta_j\)</span>,</p>
<div class="math notranslate nohighlight">
\[
F(\tilde\beta(\lambda),\beta_j)=\frac{1}{2}\sum\limits_{i=1}^N(y_i-\sum\limits_{k\neq j}x_{ik}\tilde \beta_k(\lambda)-x_{ij}\beta_j)^2+\lambda \sum\limits_{k\neq j}\vert \tilde \beta_k(\lambda)\vert+\lambda \vert \beta_j\vert
&gt; \]</div>
</div></blockquote>
<p>其中我们压缩了截距并且为了方便引出因子 <span class="math notranslate nohighlight">\(\frac 12\)</span>。这个可以看成是响应变量为部分残差 <span class="math notranslate nohighlight">\(y_i-\tilde y_i^{(j)}=y_i-\sum_{k\neq j}x_{ik}\tilde \beta_k(\lambda)\)</span>。这有显式解，得到下面的更新</p>
<div class="math notranslate nohighlight">
\[
\tilde \beta_j(\lambda)\leftarrow S(\sum_{i=1}^Nx_{ij}(y_i-\tilde y_i^{(j)}),\lambda)\tag{3.84}
\]</div>
<p>这里 <span class="math notranslate nohighlight">\(S(t,\lambda)=\mathrm{sign}(t)(\vert t\vert-\lambda)\_+\)</span> 是表 3.4 中的软阈值算子。<span class="math notranslate nohighlight">\(S(\cdot)\)</span> 中的第一个变量是部分残差在标准化变量 <span class="math notranslate nohighlight">\(x_{ij}\)</span> 上的简单最小二乘系数。式（ 3.84 ） 的重复迭代——轮流考虑每个变量直到收敛——得到 lasso 估计 <span class="math notranslate nohighlight">\(\hat\beta(\lambda)\)</span>。</p>
<p>我们也可以采用这种简单的算法来有效地计算在 <span class="math notranslate nohighlight">\(\lambda\)</span> 的每个网格结点上 lasso 的解。我们从使得 <span class="math notranslate nohighlight">\(\hat\beta(\lambda_{\max})=0\)</span> 的最小 <span class="math notranslate nohighlight">\(\lambda_{\max}\)</span> 开始，每次降低一点点来循环考虑每个变量直到收敛。采用前一个解作为 <span class="math notranslate nohighlight">\(\lambda\)</span> 新值的“warm start”，<span class="math notranslate nohighlight">\(\lambda\)</span> 再一次降低，并且重复该过程。这个可能比 LARS 算法快，特别是在大型问题中。它速度的关键在于 式（ 3.84 ） 中的量随着 <span class="math notranslate nohighlight">\(j\)</span> 变化可以快速更新，并且通常更新后有 <span class="math notranslate nohighlight">\(\tilde \beta_j=0\)</span>。另一方面，它在 <span class="math notranslate nohighlight">\(\lambda\)</span> 的网格处求解，而不是整个解的路径。同样类型的算法可以应用到 elastic net，grouped lasso 以及许多其它惩罚为个体参数的函数之和的模型（Friedman et al., 2010<a class="footnote-reference brackets" href="#id71" id="id39">31</a>）。通过一些修改，这也可以应用到 fused lasso（<a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html"><span class="doc std std-doc">18.4.2 节</span></a>）；细节在 Friedman et al. (2007)<a class="footnote-reference brackets" href="#id69" id="id40">29</a>中给出。</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id41"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Efron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2004). Least angle regression (with discussion), Annals of Statistics 32(2): 407–499.</p>
</dd>
<dt class="label" id="id42"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Donoho, D. (2006a). Compressed sensing, IEEE Transactions on Information Theory 52(4): 1289–1306.</p>
</dd>
<dt class="label" id="id43"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Candes, E. (2006). Compressive sampling, Proceedings of the International Congress of Mathematicians, European Mathematical Society, Madrid, Spain.</p>
</dd>
<dt class="label" id="id44"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Bühlmann, P. and Hothorn, T. (2007). Boosting algorithms: regularization, prediction and model fitting (with discussion), Statistical Science 22(4): 477–505.</p>
</dd>
<dt class="label" id="id45"><span class="brackets">5</span><span class="fn-backref">(<a href="#id5">1</a>,<a href="#id6">2</a>)</span></dt>
<dd><p>Hastie, T., Taylor, J., Tibshirani, R. and Walther, G. (2007). Forward stagewise regression and the monotone lasso, Electronic Journal of Statistics 1: 1–29.</p>
</dd>
<dt class="label" id="id46"><span class="brackets"><a class="fn-backref" href="#id8">6</a></span></dt>
<dd><p>Rosset, S. and Zhu, J. (2007). Piecewise linear regularized solution paths, Annals of Statistics 35(3): 1012–1030.</p>
</dd>
<dt class="label" id="id47"><span class="brackets">7</span><span class="fn-backref">(<a href="#id9">1</a>,<a href="#id10">2</a>)</span></dt>
<dd><p>Candes, E. and Tao, T. (2007). The Dantzig selector: Statistical estimation when p is much larger than n, Annals of Statistics 35(6): 2313–2351.</p>
</dd>
<dt class="label" id="id48"><span class="brackets"><a class="fn-backref" href="#id11">13</a></span></dt>
<dd><p>Bickel, P. J., Ritov, Y. and Tsybakov, A. (2008). Simultaneous analysis of lasso and Dantzig selector, Annals of Statistics. to appear.</p>
</dd>
<dt class="label" id="id49"><span class="brackets">14</span><span class="fn-backref">(<a href="#id12">1</a>,<a href="#id13">2</a>)</span></dt>
<dd><p>Efron, B., Hastie, T. and Tibshirani, R. (2007). Discussion of “Dantzig selector” by Candes and Tao, Annals of Statistics 35(6): 2358–2364.</p>
</dd>
<dt class="label" id="id50"><span class="brackets"><a class="fn-backref" href="#id14">8</a></span></dt>
<dd><p>Bakin, S. (1999). Adaptive regression and model selection in data mining problems, Technical report, PhD. thesis, Australian National University, Canberra.</p>
</dd>
<dt class="label" id="id51"><span class="brackets">9</span><span class="fn-backref">(<a href="#id15">1</a>,<a href="#id18">2</a>)</span></dt>
<dd><p>Lin, Y. and Zhang, H. (2006). Component selection and smoothing in smoothing spline analysis of variance models, Annals of Statistics 34: 2272–2297.</p>
</dd>
<dt class="label" id="id52"><span class="brackets"><a class="fn-backref" href="#id16">10</a></span></dt>
<dd><p>Yuan, M. and Lin, Y. (2007). Model selection and estimation in regression with grouped variables, Journal of the Royal Statistical Society, Series B 68(1): 49–67.</p>
</dd>
<dt class="label" id="id53"><span class="brackets"><a class="fn-backref" href="#id17">11</a></span></dt>
<dd><p>Zhao, P., Rocha, G. and Yu, B. (2008). The composite absolute penalties for grouped and hierarchichal variable selection, Annals of Statistics. (to appear).</p>
</dd>
<dt class="label" id="id54"><span class="brackets"><a class="fn-backref" href="#id19">12</a></span></dt>
<dd><p>Ravikumar, P., Liu, H., Lafferty, J. and Wasserman, L. (2008). Spam: Sparse additive models, in J. Platt, D. Koller, Y. Singer and S. Roweis (eds), Advances in Neural Information Processing Systems 20, MIT Press, Cambridge, MA, pp. 1201–1208.</p>
</dd>
<dt class="label" id="id55"><span class="brackets"><a class="fn-backref" href="#id21">15</a></span></dt>
<dd><p>Knight, K. and Fu, W. (2000). Asymptotics for lasso-type estimators, Annals of Statistics 28(5): 1356–1378.</p>
</dd>
<dt class="label" id="id56"><span class="brackets"><a class="fn-backref" href="#id22">16</a></span></dt>
<dd><p>Greenshtein, E. and Ritov, Y. (2004). Persistence in high-dimensional linear predictor selection and the virtue of overparametrization, Bernoulli 10: 971–988.</p>
</dd>
<dt class="label" id="id57"><span class="brackets"><a class="fn-backref" href="#id23">17</a></span></dt>
<dd><p>Tropp, J. (2004). Greed is good: algorithmic results for sparse approximation, IEEE Transactions on Information Theory 50: 2231– 2242.</p>
</dd>
<dt class="label" id="id58"><span class="brackets">18</span><span class="fn-backref">(<a href="#id24">1</a>,<a href="#id31">2</a>)</span></dt>
<dd><p>Donoho, D. (2006b). For most large underdetermined systems of equations, the minimal l 1 -norm solution is the sparsest solution, Communications on Pure and Applied Mathematics 59: 797–829.</p>
</dd>
<dt class="label" id="id59"><span class="brackets">19</span><span class="fn-backref">(<a href="#id25">1</a>,<a href="#id32">2</a>)</span></dt>
<dd><p>Meinshausen, N. (2007). Relaxed lasso, Computational Statistics and Data Analysis 52(1): 374–393.</p>
</dd>
<dt class="label" id="id60"><span class="brackets"><a class="fn-backref" href="#id26">20</a></span></dt>
<dd><p>Meinshausen, N. and Bühlmann, P. (2006). High-dimensional graphs and variable selection with the lasso, Annals of Statistics 34: 1436–1462.</p>
</dd>
<dt class="label" id="id61"><span class="brackets"><a class="fn-backref" href="#id27">21</a></span></dt>
<dd><p>Tropp, J. (2006). Just relax: convex programming methods for identifying sparse signals in noise, IEEE Transactions on Information Theory 52: 1030–1051.</p>
</dd>
<dt class="label" id="id62"><span class="brackets"><a class="fn-backref" href="#id28">22</a></span></dt>
<dd><p>Zhao, P. and Yu, B. (2006). On model selection consistency of lasso, Journal of Machine Learning Research 7: 2541–2563.</p>
</dd>
<dt class="label" id="id63"><span class="brackets"><a class="fn-backref" href="#id29">23</a></span></dt>
<dd><p>Wainwright, M. (2006). Sharp thresholds for noisy and high-dimensional recovery of sparsity using l 1 -constrained quadratic programming, Technical report, Department of Statistics, University of California, Berkeley.</p>
</dd>
<dt class="label" id="id64"><span class="brackets"><a class="fn-backref" href="#id30">24</a></span></dt>
<dd><p>Bunea, F., Tsybakov, A. and Wegkamp, M. (2007). Sparsity oracle inequalities for the lasso, Electronic Journal of Statistics 1: 169–194.</p>
</dd>
<dt class="label" id="id65"><span class="brackets"><a class="fn-backref" href="#id33">25</a></span></dt>
<dd><p>Fan, J. and Li, R. (2005). Variable selection via nonconcave penalized likelihood and its oracle properties, Journal of the American Statistical Association 96: 1348–1360.</p>
</dd>
<dt class="label" id="id66"><span class="brackets"><a class="fn-backref" href="#id34">26</a></span></dt>
<dd><p>Zou, H. (2006). The adaptive lasso and its oracle properties, Journal of the American Statistical Association 101: 1418–1429.</p>
</dd>
<dt class="label" id="id67"><span class="brackets"><a class="fn-backref" href="#id35">27</a></span></dt>
<dd><p>Fu, W. (1998). Penalized regressions: the bridge vs. the lasso, Journal of Computational and Graphical Statistics 7(3): 397–416.</p>
</dd>
<dt class="label" id="id68"><span class="brackets"><a class="fn-backref" href="#id36">28</a></span></dt>
<dd><p>Daubechies, I., Defrise, M. and De Mol, C. (2004). An iterative thresholding algorithm for linear inverse problems with a sparsity constraint, Communications on Pure and Applied Mathematics 57: 1413–1457.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">29</span><span class="fn-backref">(<a href="#id37">1</a>,<a href="#id40">2</a>)</span></dt>
<dd><p>Friedman, J., Hastie, T., Hoefling, H. and Tibshirani, R. (2007). Pathwise coordinate optimization, Annals of Applied Statistics 2(1): 302–332.</p>
</dd>
<dt class="label" id="id70"><span class="brackets"><a class="fn-backref" href="#id38">30</a></span></dt>
<dd><p>Wu, T. and Lange, K. (2008). Coordinate descent procedures for lasso penalized regression, Annals of Applied Statistics 2(1): 224–244.</p>
</dd>
<dt class="label" id="id71"><span class="brackets"><a class="fn-backref" href="#id39">31</a></span></dt>
<dd><p>Friedman, J., Hastie, T. and Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent, Journal of Statistical Software 33(1): 1–22.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./03-Linear-Methods-for-Regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="3.7-Multiple-Outcome-Shrinkage-and-Selection.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">3.7 多输出的收缩和选择</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3.9-Computational-Considerations.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">3.9 计算上的考虑</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>