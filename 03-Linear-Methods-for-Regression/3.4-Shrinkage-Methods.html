
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3.4 收缩的方法 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.5 运用派生输入方向的方法" href="3.5-Methods-Using-Derived-Input-Directions.html" />
    <link rel="prev" title="3.3 选择预测变量的子集" href="3.3-Subset-Selection.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.3-Subset-Selection.html">
     3.3 选择预测变量的子集
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 光滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 光滑参数的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波光滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核光滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差、方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的乐观估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 “条件测试误差”还是“测试误差的期望”？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5
     <code class="docutils literal notranslate">
      <span class="pre">
       EM
      </span>
      <span class="pre">
       算法
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆叠
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14-Unsupervised-Learning/14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   （ 1 ）岭回归
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lasso">
   （ 2 ）Lasso
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   （ 3 ）讨论：子集的选择，岭回归，Lasso
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   （ 4 ）最小角回归
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lar-lasso">
     LAR 和 Lasso 自由度公式
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>3.4 收缩的方法</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   （ 1 ）岭回归
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lasso">
   （ 2 ）Lasso
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   （ 3 ）讨论：子集的选择，岭回归，Lasso
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   （ 4 ）最小角回归
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lar-lasso">
     LAR 和 Lasso 自由度公式
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>3.4 收缩的方法<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>通过保留一部分预测变量而丢弃剩余的变量，<strong>子集选择 (subset selection)</strong> 可得到一个可解释的、预测误差可能比全模型低的模型。然而，因为这是一个离散的过程（变量不是保留就是丢弃），所以经常表现为高方差，因此不会降低全模型的预测误差。而<strong>收缩方法 (shrinkage methods)</strong> 更加连续，因此不会受 <strong>高易变性 (high variability)</strong> 太大的影响。</p>
<div class="section" id="id2">
<h2>（ 1 ）岭回归<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p><strong>岭回归 (Ridge regression)</strong> 根据回归系数的大小加上惩罚因子对它们进行收缩。岭回归的系数使得带惩罚的残差平方和最小</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}^{ridge}=\underset{\beta}{\arg\min}\Big\{\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2+\lambda\sum\limits_{j=1}^p\beta_j^2\Big\}\tag{3.41}
\]</div>
<p>这里<span class="math notranslate nohighlight">\(\lambda\ge 0 \)</span>是控制收缩程度的参数：<span class="math notranslate nohighlight">\(\lambda\)</span>值越大，收缩的程度越大。每个系数都向零收缩。<!--系数向零收缩（并且彼此收缩到一起）。-->通过参数的平方和来惩罚的想法也用在了神经网络，也被称作 <strong>权重衰减 (weight decay)</strong>（<span class="xref myst">第 11 章</span>）。</p>
<p>岭回归问题可以等价地写成</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\beta}^{ridge}&amp;=\underset{\beta}{\arg\min}\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2\\\
&amp; \text{subject to }\sum\limits_{j=1}^p\beta_j^2 \le t
\end{align*}
\tag{3.42}
\end{split}\]</div>
<p>上式用参数显式表达了对回归参数大小的约束。</p>
<blockquote>
<div><p>note “weiya 注：”
式 式（ 3.41 ） 其实是对式 式（ 3.42 ） 应用 Lagrange 乘子法得到的。</p>
</div></blockquote>
<p>式（ 3.41 ） 中的 <span class="math notranslate nohighlight">\(\lambda\)</span> 和 式（ 3.42 ） 中的 <span class="math notranslate nohighlight">\(t\)</span> 存在一一对应。当在线性回归模型中有许多相关变量，它们的系数可能很难确定且有高方差。某个变量的较大的正系数可以与相关性强的变量的差不多大的负系数相互抵消。通过对系数加入大小限制，如 式（ 3.42 ），这个问题能得以减轻。</p>
<blockquote>
<div><p>note “weiya 注：”
这里说的是，在没有对参数大小进行限制前，会存在一对相关性强的变量，它们系数取值符号相反，但绝对值差不多大，会大大增加方差，这也就是高方差的体现，但其实它们的合作用效果近似为 <span class="math notranslate nohighlight">\(0\)</span>，所以考虑引进对参数大小的惩罚。</p>
</div></blockquote>
<p>对输入按比例进行缩放时，岭回归的解不相等，因此求解 式（ 3.41 ） 前我们需要对输入进行标准化。另外，注意到惩罚项不包含截距 <span class="math notranslate nohighlight">\(\beta_0\)</span>。对截距的惩罚会使得过程依赖于 <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> 的初始选择；也就是，对每个 <span class="math notranslate nohighlight">\(y_i\)</span> 加上常数 <span class="math notranslate nohighlight">\(c\)</span> 不是简单地导致预测值会偏离同样的量 <span class="math notranslate nohighlight">\(c\)</span>。可以证明（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/95">练习 3.5</a>）经过对输入进行中心化（每个 <span class="math notranslate nohighlight">\(x_{ij}\)</span> 替换为 <span class="math notranslate nohighlight">\(x_{ij}-\bar x_j\)</span>）后，式（ 3.41 ） 的解可以分成两部分。我们用 <span class="math notranslate nohighlight">\(\bar y=\frac{1}{N}\sum_1^Ny_i\)</span> 来估计 <span class="math notranslate nohighlight">\(\beta_0\)</span>。剩余的参数利用中心化的 <span class="math notranslate nohighlight">\(x_{ij}\)</span> 通过无截距的岭回归来估计。今后我们假设中心化已经完成，则输入矩阵 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 有 <span class="math notranslate nohighlight">\(p\)</span>（不是 <span class="math notranslate nohighlight">\(p+1\)</span>）列。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.5”
已解答，详细证明过程见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/95">Issue 95: Ex. 3.5</a></p>
</div></blockquote>
<p>将 式（ 3.41 ） 的准则写成矩阵形式</p>
<div class="math notranslate nohighlight">
\[
\mathrm{RSS}(\lambda)=(\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta)+\lambda\beta^T\beta \tag{3.43}
\]</div>
<p>可以简单地看出岭回归的解为</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}^{ridge}=(\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}\tag{3.44}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> 为 <span class="math notranslate nohighlight">\(p\times p\)</span> 的单位矩阵。注意到选择二次函数惩罚 <span class="math notranslate nohighlight">\(\beta^T\beta\)</span>，岭回归的解仍是 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 的线性函数。解在求逆之前向矩阵 <span class="math notranslate nohighlight">\(\mathbf{X^TX}\)</span> 的对角元上加入正的常数值。即使 <span class="math notranslate nohighlight">\(\mathbf{X^TX}\)</span> 不是满秩，这样会使得问题非奇异，而且这是第一次将岭回归引入统计学中 (Hoerl and Kennard, 1970<a class="footnote-reference brackets" href="#id11" id="id3">1</a>）的主要动力。传统的岭回归的描述从定义 式（ 3.44 ） 开始。我们选择通过 式（ 3.41 ） 和 式（ 3.42 ） 来阐述，因为这两式让我们看清楚了它是怎样实现的。</p>
<p>图 3.8 展示了前列腺癌例子的岭回归系数估计，绘制成关于 <span class="math notranslate nohighlight">\(\mathrm{df}(\lambda)\)</span> 的函数图象，<span class="math notranslate nohighlight">\(\mathrm{df}(\lambda)\)</span> 为由惩罚 <span class="math notranslate nohighlight">\(\lambda\)</span> 得到的 <strong>有效自由度 (effective degrees of freedom)</strong>（由式 式（ 3.50 ） 中定义）。在正交输入的情形下，岭回归估计仅仅是最小二乘估计的缩小版本，也就是 <span class="math notranslate nohighlight">\(\hat{\beta}^{ridge}=\hat{\beta}/(1+\lambda)\)</span>。</p>
<p><img alt="" src="../_images/fig3.8.png" /></p>
<blockquote>
<div><p>图 3.8 当惩罚参数 <span class="math notranslate nohighlight">\(\lambda\)</span> 不同时，前列腺癌例子岭回归的变化曲线。画出系数关于有效自由度 <span class="math notranslate nohighlight">\(\mathrm{df}(\lambda)\)</span> 的曲线。垂直直线画在 <span class="math notranslate nohighlight">\(\mathrm{df}=5.0\)</span> 处，这是由交叉验证选择出来的。</p>
</div></blockquote>
<p>当给定一个合适的先验分布，岭回归也可以从后验分布的均值或众数得到。具体地，假设 <span class="math notranslate nohighlight">\(y_i \sim N(\beta_0+x^T_i\beta,\sigma^2)\)</span>，参数 <span class="math notranslate nohighlight">\(\beta_j\)</span> 的分布均为 <span class="math notranslate nohighlight">\(N(0,\tau^2)\)</span>，每个都相互独立。则当 <span class="math notranslate nohighlight">\(\tau^2\)</span> 和 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 值已知时，<span class="math notranslate nohighlight">\(\beta\)</span> 后验分布密度函数的对数值（的负数）与 式（ 3.41 ） 中花括号里面的表达式成比例 <strong>(weiya 注：原文直接说与花括号的表达式相等，但应该是常数倍)</strong>，且 <span class="math notranslate nohighlight">\(\lambda=\sigma^2/\tau^2\)</span>（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/96">练习 3.6</a>)。因此岭回归估计是后验分布的众数；又因分布为高斯分布，则也是后验分布的均值。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.6”
将解答过程移至<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/96">Issue 96: Ex. 3.6</a>。</p>
</div></blockquote>
<!--
> **weiya注：**
>
>
>$$
>\begin{align*}
>f(\beta\mid y)&=\dfrac{f(\beta, y)}{f(y)}\\\
>&=\dfrac{f(\beta,y)}{\int f(y\mid\beta)f(\beta)}\\\
>&\sim f(y\mid\beta)f(\beta)\\\
>&=Cexp\Big\{-\frac{1}{2\sigma^2}\Big[(y-\beta_0-X\beta)'(y-\beta_0-X\beta)+\frac{\sigma^2}{\tau^2}\beta'\beta\Big]\Big\}
>\end{align*}
>$$

>
> $\color{red} 疑问：C是多少？$
> 通过
>$$
>\int Cf(\beta\mid y)d\beta=1
>$$

> 来确定C
> 
$$

> C = \dfrac{1}{(2\pi)^N\tau\sigma \Vert XX'\Vert^{1/2}}
> 
$$

>
>
> 取对数，有
> 
$$

> log(f(\beta\mid y))=-\frac{1}{2\sigma^2}[(y-\beta_0-X\beta)'(y-\beta_0-X\beta)+\frac{\sigma^2}{\tau^2}\beta'\beta]+log(C)
> 
$$

>
> 则$\lambda=\frac{\sigma^2}{\tau^2}$,且岭回归估计是后验分布的众数。
-->
<p>中心化输入矩阵 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的 <strong>奇异值分解 (SVD)</strong> 让我们进一步了解了岭回归的本质。这个分解在许多统计方法分析中非常有用。<span class="math notranslate nohighlight">\(N\times p\)</span> 阶矩阵 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的 SVD 分解有如下形式</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X=UDV^T}\tag{3.45}
\]</div>
<p>这里 <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> 分别是 <span class="math notranslate nohighlight">\(N\times p\)</span> 和 <span class="math notranslate nohighlight">\(p\times p\)</span> 的正交矩阵，<span class="math notranslate nohighlight">\(\mathbf{U}\)</span>的列张成 <span class="math notranslate nohighlight">\(X\)</span> 的列空间，<span class="math notranslate nohighlight">\(\mathbf{V}\)</span> 的列张成 <span class="math notranslate nohighlight">\(X\)</span> 的行空间。<span class="math notranslate nohighlight">\(\mathbf{D}\)</span> 为 <span class="math notranslate nohighlight">\(p\times p\)</span> 的对角矩阵，对角元 <span class="math notranslate nohighlight">\(d_1\ge d_2 \ge \cdots \ge d_p \ge 0\)</span> 称作 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的奇异值。如果一个或多个 <span class="math notranslate nohighlight">\(d_j=0\)</span>，则 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 为奇异的。</p>
<blockquote>
<div><p>note “weiya 注: 奇异值分解（张贤达的《矩阵分析与应用》）”
奇异值分解最早由 Beltrami 在 1873 年对实正方矩阵提出来的。Beltrami 从双线性函数</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[
f(x,y)=x^TAy,\qquad A\in \mathbb{R}^{n\times m}    
\]</div>
<p>出发，通过引入线性变换</p>
<div class="math notranslate nohighlight">
\[
x=U\xi,\qquad y=V\eta    
\]</div>
<p>将双线性函数变为</p>
<div class="math notranslate nohighlight">
\[
f(x,y)=\xi^TS\eta
\]</div>
<p>其中</p>
<div class="math notranslate nohighlight">
\[
S=U^TAV\,
\]</div>
<p>若选择 <span class="math notranslate nohighlight">\(U\)</span> 和 <span class="math notranslate nohighlight">\(V\)</span> 为正交矩阵，则他们的选择各存在 <span class="math notranslate nohighlight">\(n^2-n\)</span> 个自由度。他提出利用这些自由度使矩阵 <span class="math notranslate nohighlight">\(S\)</span> 的非对角元为0，即矩阵<span class="math notranslate nohighlight">\(S=\Sigma=\mathrm{diag}(\sigma_1,\sigma_2,\ldots,\sigma_n)\)</span>为对角矩阵。则</p>
<div class="math notranslate nohighlight">
\[
A=U\Sigma V^T   
\]</div>
<p>这是 Beltrami 于 1873 年得到的实正方矩阵的奇异值分解。后来，Autonne 于 1902 年把奇异值分解推广到复正方矩阵；Eckart 与 Young 于 1939 年又进一步把它推广到一般的长方形矩阵。因此，现在常将任意复长方矩阵奇异值分解定理称为 Autonee-Eckart-Young 定理，即</p>
<p>令 <span class="math notranslate nohighlight">\(A\in \mathbb{R}^{m\times n}\)</span>(或<span class="math notranslate nohighlight">\(C^{m\times n}\)</span>),则存在正交（或酉）矩阵 <span class="math notranslate nohighlight">\(U\in \mathbb{R}^{m\times m}\)</span>(或 <span class="math notranslate nohighlight">\(C^{m\times m}\)</span>)和 <span class="math notranslate nohighlight">\(V\in \mathbb{R}^{n\times n}\)</span>(或<span class="math notranslate nohighlight">\(C^{n\times n}\)</span>)使得</p>
<div class="math notranslate nohighlight">
\[
A=U\Sigma V^T(or\quad U\Sigma V^H)    
\]</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>式中
</pre></div>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma=
\left[
    \begin{array}{cc}
    \Sigma_1&amp;O\\
    O&amp;O
    \end{array}
    \right]    
\end{split}\]</div>
<p>且 <span class="math notranslate nohighlight">\(\Sigma_1=diag(\sigma_1,\sigma_2,\ldots,\sigma_r)\)</span> ，其对角元素按照顺序</p>
<div class="math notranslate nohighlight">
\[
\sigma_1\gt \sigma_2\cdots\ge\sigma_r&gt;0,\qquad r=rank(A)
\]</div>
<p>排列.</p>
<p>下图（来自<a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition#/media/File:Reduced_Singular_Value_Decompositions.svg">维基百科</a>）形象地展示了 SVD 的四种不同形式，</p>
<p><img alt="" src="../_images/Reduced_Singular_Value_Decompositions.svg.png" /></p>
<ul class="simple">
<li><p>Full SVD:</p></li>
<li><p>Thin SVD: 只保留 <span class="math notranslate nohighlight">\(U\)</span> 中对应 <span class="math notranslate nohighlight">\(V^T\)</span> 中行向量的 <span class="math notranslate nohighlight">\(n\)</span> 个列向量</p></li>
<li><p>Compact SVD: 去掉零奇异值对应的行和列</p></li>
<li><p>Truncated SVD: 保留前 <span class="math notranslate nohighlight">\(t\)</span> 个最大奇异值对应的行和列</p></li>
</ul>
<p>利用奇异值分解，通过简化我们可以把最小二乘拟合向量写成</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{X}\hat{\beta}^{ls}&amp;=\mathbf{X(X^TX)^{-1}X^Ty}\\
&amp;=\mathbf{UU^Ty}\tag{3.46}
\end{align*}
\end{split}\]</div>
<p>注意到 <span class="math notranslate nohighlight">\(\mathbf{U}^T\mathbf y\)</span> 是 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 正交基 <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> 下的坐标。同时注意其与 式（ 3.33 ） 的相似性；</p>
<blockquote>
<div><p>note “weiya 注：Recall”</p>
<div class="math notranslate nohighlight">
\[
&gt;\hat{\beta}=\mathbf{R^{-1}Q^Ty}\tag{3.32} 
&gt;\]</div>
<div class="math notranslate nohighlight">
\[
&gt;\hat{\mathbf{y}}=\mathbf{QQ^Ty}\tag{3.33}
&gt;\]</div>
</div></blockquote>
<p><span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> 是 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 列空间的两个不同的正交基（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/97">练习 3.8</a>）。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.8”
已解答，具体证明过程见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/97">Issue 97: Ex. 3.8</a></p>
</div></blockquote>
<p>现在岭回归的解为</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathbf{X}\hat{\beta}^{ridge}&amp;=\mathbf{X}(\mathbf{X^TX}+\lambda \mathbf{I})^{-1}\mathbf{X^Ty}\\
&amp;= \mathbf{UD}(\mathbf{D^2}+\lambda \mathbf{I})^{-1}\mathbf{DU^Ty}\\
&amp;= \sum\limits_{j=1}^p\mathbf{u}_j\dfrac{d_j^2}{d_j^2+\lambda}\mathbf{u_j^Ty}\tag{3.47}
\end{align*}
\end{split}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mathbf{u}_j\)</span> 是 <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> 的列向量。注意到因为 <span class="math notranslate nohighlight">\(\lambda \ge 0\)</span>，我们有 <span class="math notranslate nohighlight">\(d_j^2/(d^2_j+\lambda)\le 1\)</span>。类似线性回归，岭回归计算 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 关于正规基 <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> 的坐标。通过因子 <span class="math notranslate nohighlight">\(d^2_j/(d^2_j+\lambda)\)</span> 来收缩这些坐标。这意味着更小的 <span class="math notranslate nohighlight">\(d_j^2\)</span> 会在更大程度上收缩基向量的坐标。</p>
<p><span class="math notranslate nohighlight">\(d_j^2\)</span> 值小意味着什么？中心化后的矩阵 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的奇异值分解是表示 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 中主成分变量的另一种方式。样本协方差矩阵为 <span class="math notranslate nohighlight">\(\mathbf{S=X^TX}/N\)</span><!--$\mathbf{S={\color{red} E((X-EX)^T(X-EX))=}X^TX}/N$-->，并且从 式（ 3.45 ） 式我们得到</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X^T X = VD^2V^T} \tag{3.48}
\]</div>
<p>上式是 <span class="math notranslate nohighlight">\(\mathbf{X^TX}\)</span>（当忽略因子 <span class="math notranslate nohighlight">\(N\)</span> 时，也是 <span class="math notranslate nohighlight">\(S\)</span>）的 <strong>特征值分解 (eigen decomposition)</strong>。特征向量 <span class="math notranslate nohighlight">\(v_j\)</span>（<span class="math notranslate nohighlight">\(\mathbf{V}\)</span> 的列向量）也称作 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的 <strong>主成分 (principal components)</strong>（或 Karhunen-Loeve）方向。第一主成分方向 <span class="math notranslate nohighlight">\(v_1\)</span> 有下面性质：<span class="math notranslate nohighlight">\(\mathbf{z}_1=\mathbf{X}v_1\)</span> 在所有 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 列的标准化线性组合中有最大的样本方差。样本方差很容易看出来是</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(\mathbf{z}_1)=\mathrm{Var}(\mathbf{X}v_1)=\dfrac{d_1^2}{N}\tag{3.49}
\]</div>
<p>事实上 <span class="math notranslate nohighlight">\(\mathbf{z}_1=\mathbf{X}v_1=\mathbf{u}_1d_1\)</span>。导出变量 <span class="math notranslate nohighlight">\(\mathbf{z_1}\)</span> 称作 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 的第一主成分，因此 <span class="math notranslate nohighlight">\(\mathbf{u_1}\)</span> 是标准化的第一主成分。后面的主成分 <span class="math notranslate nohighlight">\(z_j\)</span> 在与前一个保持正交的前提下有最大的方差 <span class="math notranslate nohighlight">\(d_j^2/N\)</span>。所以，最后一个主成分有最小的方差。因此越小的奇异值 <span class="math notranslate nohighlight">\(d_j\)</span> 对应 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 列空间中方差越小的方向，并且岭回归在这些方向上收缩得最厉害。</p>
<p>图 3.9 展示了两个维度下部分数据点的主成分。如果我们考虑在这个区域（<span class="math notranslate nohighlight">\(Y\)</span> 轴垂直纸面）内拟合线性曲面，数据的结构形态使得确定梯度时长方向会比短方向更精确。岭回归防止在短方向上估计梯度可能存在的高方差。隐含的假设是响应变量往往在高方差的输入方向上变化。这往往是个合理的假设，因为我们所研究的预测变量随响应变量变化而变化，而不需要保持不变。</p>
<p><img alt="" src="../_images/fig3.9.png" /></p>
<blockquote>
<div><p>图 3.9 部分输入数据点的主成分。最大主成分是使得投影数据方差最大的方向，最小主成分是使得方差最小的方向。岭回归将 <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> 投射到这些成分上，然后对低方差成分的系数比高方差收缩得更厉害。</p>
</div></blockquote>
<p>在图 3.7 中我们已经画了预测误差估计值关于 <span class="math notranslate nohighlight">\(\mathrm{df}(\lambda)\)</span> 的曲线</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathrm{df}(\lambda)&amp;=\mathrm{tr}[\mathbf{X}(\mathbf{X^TX}+\lambda\mathbf{I})^{-1}\mathbf{X}^T]\\
&amp;=\mathrm{tr}(\mathbf{H}_{\lambda})\\
&amp;=\sum\limits_{j=1}^p\dfrac{d_j^2}{d_j^2+\lambda}\tag{3.50}
\end{align*}
\end{split}\]</div>
<p>上面 <span class="math notranslate nohighlight">\(\lambda\)</span> 的单调递减函数是岭回归拟合的 <strong>有效自由度 (effective degrees of freedom)</strong>。通常在含 <span class="math notranslate nohighlight">\(p\)</span> 个变量的线性回归拟合中，拟合的自由度为 <span class="math notranslate nohighlight">\(p\)</span>，也就是无约束参数的个数。这里想法是尽管岭回归拟合中所有的 <span class="math notranslate nohighlight">\(p\)</span> 个系数都不为 0，但是它们在由 <span class="math notranslate nohighlight">\(\lambda\)</span> 控制的约束下拟合。注意到当 <span class="math notranslate nohighlight">\(\lambda=0\)</span>（没有正则化）时 <span class="math notranslate nohighlight">\(\mathrm{df}(\lambda)=p\)</span>，并且当 <span class="math notranslate nohighlight">\(\lambda\rightarrow \infty\)</span> 时 <span class="math notranslate nohighlight">\(df(\lambda)\rightarrow 0\)</span>。当然总是对于截距总有一个额外的自由度，事先 (apriori) 已经去掉了。这个定义将在 <a class="reference external" href="#_2">3.4.4 节</a>和 <span class="xref myst">7.4-7.6 节</span>中详细介绍。图 3.7 中最小值在 <span class="math notranslate nohighlight">\(\mathrm{df}(\lambda)=5.0\)</span> 处。表 3.3 表明岭回归将全最小二乘估计的测试误差降低了一小部分。</p>
</div>
<div class="section" id="lasso">
<h2>（ 2 ）Lasso<a class="headerlink" href="#lasso" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>note “weiya 注：”
lasso 是 “Least absolute shrinkage and seleetion operator” 的首字母缩写。</p>
</div></blockquote>
<p>lasso 像岭回归一样是个收缩方法，有微妙但很重要的区别。lasso 估计定义如下</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\beta}^{lasso}&amp;=\underset{\beta}{\arg\min}\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2\\
&amp;\text{subject to }\sum\limits_{j=1}^p\vert\beta_j\vert\le t \tag{3.51}
\end{align*}
\end{split}\]</div>
<p>正如在岭回归中一样，我们可以通过标准化预测变量来对常数 <span class="math notranslate nohighlight">\(\beta_0\)</span> 再参量化；<span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> 的解为 <span class="math notranslate nohighlight">\(\bar{y}\)</span>，并且后面我们拟合无截距的模型（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/95">练习 3.5</a>）。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.5”
已解答，详细证明过程见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/95">Issue 95: Ex. 3.5</a></p>
</div></blockquote>
<p>在信号处理中，lasso 也被称作 basis pursuit (Chen et al., 1998<a class="footnote-reference brackets" href="#id12" id="id4">2</a>)</p>
<p>我们也可以把 lasso 问题等价地写成 <strong>拉格朗日形式 (Lagrangian form)</strong></p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}^{lasso}=\underset{\beta}{\arg\min}\Big\{\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2+\lambda\sum\limits_{j=1}^p\vert\beta_j\vert\Big\}\tag{3.52}
\]</div>
<p>注意到这与岭回归问题 式（ 3.42 ） 或 式（ 3.41 ） 的相似性：<span class="math notranslate nohighlight">\(L_2\)</span> 的岭回归惩罚 <span class="math notranslate nohighlight">\(\sum_1^p\beta^2_j\)</span> 替换为 <span class="math notranslate nohighlight">\(L_1\)</span> 的 lasso 惩罚<span class="math notranslate nohighlight">\(\sum_1^p\vert\beta_j\vert\)</span>。后一约束使得解在 <span class="math notranslate nohighlight">\(y_i\)</span> 处非线性，并且在岭回归中没有相近的表达式。计算 lasso 的解是一个二次规划问题，尽管我们在 3.4.4 节看到当 <span class="math notranslate nohighlight">\(\lambda\)</span> 不同时计算解的整个路径存在与岭回归同样计算量的有效算法。由于该约束的本质，令 <span class="math notranslate nohighlight">\(t\)</span> 充分小会造成一些参数恰恰等于 0。因此 lasso 完成一个温和的连续子集选择。如果所选的 <span class="math notranslate nohighlight">\(t\)</span> 大于<span class="math notranslate nohighlight">\(t_0=\sum_1^p\vert\hat{\beta}_j\vert\)</span>（其中 <span class="math notranslate nohighlight">\(\hat{\beta}_j=\hat{\beta}_j^{ls}\)</span>，<span class="math notranslate nohighlight">\(\hat{\beta}_j^{ls}\)</span> 为最小二乘估计），则 lasso 估计为 <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span>。另一方面，当 <span class="math notranslate nohighlight">\(t=t_0/2\)</span>，最小二乘系数平均收缩 <span class="math notranslate nohighlight">\(50\%\)</span>。然而，收缩的本质不是很显然，我们将在 3.4.4 节进一步研究。类似在变量子集选择中子集的大小，或者岭回归的惩罚参数，应该自适应地选择 <span class="math notranslate nohighlight">\(t\)</span> 使预测误差期望值的估计最小化。</p>
<p>图 3.7 中，为了方便解释，我们已经画出 lasso 的预测误差估计关于标准化参数 <span class="math notranslate nohighlight">\(s=t/\sum^p_1\vert\hat{\beta}_j\vert\)</span> 的曲线。通过 10 折交叉验证选择 <span class="math notranslate nohighlight">\(s\approx 0.36\)</span>；这使得 4 个系数为 0（表 3.3 的第 5 列）。最终模型有第二低的测试误差，比全最小二乘模型略低，但是测试误差估计的标准误差（表 3.3 的最后一行）相当大。</p>
<p>图 3.10 显示了当惩罚参数 <span class="math notranslate nohighlight">\(s=t/\sum_1^p\vert\hat{\beta}_j\vert\)</span> 不同时的 lasso 系数。当 <span class="math notranslate nohighlight">\(s=1.0\)</span> 时为最小二乘估计；当 <span class="math notranslate nohighlight">\(s\rightarrow 0\)</span> 时下降为 0。该下降不总是严格单调的，尽管例子中确实是。在 <span class="math notranslate nohighlight">\(s=0.36\)</span> 处画了垂直直线，该值通过交叉验证来选择。</p>
<p><img alt="" src="../_images/fig3.10.png" /></p>
<blockquote>
<div><p>图 3.10 当惩罚参数 <span class="math notranslate nohighlight">\(t\)</span> 变化时的 lasso 系数曲线。图中画了系数关于 <span class="math notranslate nohighlight">\(s=t/\sum^p_1\vert\hat{\beta}_j\vert\)</span> 的曲线。垂直直线画在 <span class="math notranslate nohighlight">\(s=0.36\)</span> 处，该值通过交叉验证来选择。比较 65 页的图 3.8，lasso 曲线会达到 0，然而岭回归不会。曲线是分段线性的，所以只计算显示点处的值；详见 3.4.4 节。</p>
</div></blockquote>
</div>
<div class="section" id="id5">
<h2>（ 3 ）讨论：子集的选择，岭回归，Lasso<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>这部分我们讨论并且比较至今为止有约束的线性回归模型的三种方法：子集选择、岭回归和 lasso。</p>
<p>在正交输入矩阵的情况下，三种过程都有显式解。每种方法对最小二乘估计 <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> 应用简单的变换，详见表 3.4。</p>
<p><img alt="" src="../_images/tab3.4.png" /></p>
<blockquote>
<div><p>表 3.4 在 <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> 为正规列情形下 <span class="math notranslate nohighlight">\(\beta_j\)</span> 的估计值。<span class="math notranslate nohighlight">\(M\)</span> 和 <span class="math notranslate nohighlight">\(\lambda\)</span> 是通过对应的手段选择的常数；符号标记变量的符号（<span class="math notranslate nohighlight">\(\pm 1\)</span>），而且 <span class="math notranslate nohighlight">\(x_+\)</span> 记 <span class="math notranslate nohighlight">\(x\)</span> 的正数部分。下面的表格中，估计值由红色虚线来显示。灰色的 <span class="math notranslate nohighlight">\(45^{\circ}\)</span> 直线作为参照显示了无约束的估计。</p>
</div></blockquote>
<p>岭回归做等比例的收缩。lasso 通过常数因子 <span class="math notranslate nohighlight">\(\lambda\)</span> 变换每个系数，在 0 处截去。这也称作“软阈限”，而且用在 <span class="xref myst">5.9 节</span>中基于小波光滑的内容中。最优子集选择删掉所有系数小于第 <span class="math notranslate nohighlight">\(M\)</span> 个大系数的变量；这是“硬阈限”的一种形式。</p>
<p>回到非正交的情形，一些图象可以帮助了解它们之间的关系。当只有两个参数时图 3.11 描绘了 lasso（左）和岭回归（右）。残差平方和为椭圆形的等高线，以全最小二乘估计为中心。岭回归的约束区域为圆盘 <span class="math notranslate nohighlight">\(\beta_1^2+\beta_2^2\le t\)</span>，lasso 的约束区域为菱形<span class="math notranslate nohighlight">\(\vert\beta_1\vert+\vert\beta_2\vert\le t\)</span>。两种方式都寻找当椭圆等高线达到约束区域的第一个点。与圆盘不同，<strong>菱形 (diamond)</strong> 有角；如果解出现在角上，则有一个参数 <span class="math notranslate nohighlight">\(\beta_j\)</span> 等于 0。当 <span class="math notranslate nohighlight">\(p &gt; 2\)</span>，菱形变成了 <strong>偏菱形 (rhomboid)</strong>，而且有许多角，平坦的边和面；对于参数估计有更多的可能为 0。</p>
<p><img alt="" src="../_images/fig3.11.png" /></p>
<blockquote>
<div><p>图 3.11 lasso (左)和岭回归（右）的估计图象。图中显示了误差的等高线和约束函数。实心蓝色区域分别为约束区域<span class="math notranslate nohighlight">\(\vert\beta_1\vert+\vert\beta_2\vert\le t\)</span>以及<span class="math notranslate nohighlight">\(\beta^2_1+\beta_2^2\le t^2\)</span>，红色椭圆为最小二乘误差函数的等高线。</p>
</div></blockquote>
<p>我们可以把岭回归和 lasso 一般化，并且可以看成是贝叶斯估计。考虑下面准则</p>
<div class="math notranslate nohighlight">
\[
\tilde{\beta}=\underset{\beta}{\arg\min}\Big\{\sum\limits_{i=1}^N(y_i-\beta_0-\sum\limits_{j=1}^px_{ij}\beta_j)^2+\lambda\sum\limits_{j=1}^p\vert\beta_j\vert^q\Big\}\tag{3.53}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(q\ge 0\)</span>。图 3.12 显示了两个输入情形下常数值 <span class="math notranslate nohighlight">\(\sum_j\vert\beta_j\vert^q\)</span> 的等高线。</p>
<p><img alt="" src="../_images/fig3.12.png" /></p>
<blockquote>
<div><p>图 3.12 给定值 <span class="math notranslate nohighlight">\(q\)</span> 下常数值 <span class="math notranslate nohighlight">\(\sum_j\vert\beta_j\vert^q\)</span> 的等高线。</p>
</div></blockquote>
<p>将 <span class="math notranslate nohighlight">\(\vert\beta_j\vert^q\)</span> 看成 <span class="math notranslate nohighlight">\(\beta_j\)</span> 的先验概率密度的对数值，同样有参数先验分布的等高线。<span class="math notranslate nohighlight">\(q=0\)</span> 对应变量子集选择，惩罚项是简单地统计非零参数的个数；<span class="math notranslate nohighlight">\(q=1\)</span> 对应 lasso，<span class="math notranslate nohighlight">\(q=2\)</span> 对应岭回归。注意到 <span class="math notranslate nohighlight">\(q\le 1\)</span>，先验在各方向上不是均匀的，而是更多地集中在坐标方向上。对应 <span class="math notranslate nohighlight">\(q=1\)</span> 情形的先验分布是关于每个输入变量是的独立的二重指数分布（或者 Laplace 分布），概率密度为<span class="math notranslate nohighlight">\((1/2\tau)exp(-\vert\beta\vert)/\tau\)</span> 并且 <span class="math notranslate nohighlight">\(\tau=1/\lambda\)</span>。<span class="math notranslate nohighlight">\(q=1\)</span> 的情形（lasso）是使得约束区域为凸的最小 <span class="math notranslate nohighlight">\(q\)</span> 值；非凸约束区域使得优化问题很困难。</p>
<p>从这点看，lasso、岭回归和最优子集选择是有着不同先验分布的贝叶斯估计。然而，注意到它们取自后验分布的众数，即最大化后验分布。在贝叶斯估计中使用后验分布的均值更加常见。岭回归同样是后验分布的均值，但是 lasso 和最优子集选择不是。</p>
<p>再一次观察准则 式（ 3.53 ），我们可能尝试除 0，1，2 外的其它 <span class="math notranslate nohighlight">\(q\)</span> 值。尽管有人可能从数据中估计 <span class="math notranslate nohighlight">\(q\)</span>，我们的经验表明引入额外的方差不值得。<span class="math notranslate nohighlight">\(q\in (1,2)\)</span> 表明在 lasso 和岭回归之间进行权衡。当 <span class="math notranslate nohighlight">\( q &gt; 1\)</span> 时尽管 <span class="math notranslate nohighlight">\(\vert\beta_j\vert^q\)</span> 在 0 处可导，但是并没有lasso（<span class="math notranslate nohighlight">\(q=1\)</span>）的令系数恰巧为零的性质。部分由于这个原因并且考虑计算易处理，Zou and Hastie (2005)<a class="footnote-reference brackets" href="#id13" id="id6">4</a> 引入弹性惩罚</p>
<div class="math notranslate nohighlight">
\[
\lambda \sum\limits_{j=1}^p(\alpha\beta_j^2+(1-\alpha)\vert\beta_j\vert)\tag{3.54}
\]</div>
<p>这是一种岭回归和 lasso之间的平衡。图 3.13 比较了 <span class="math notranslate nohighlight">\(q=1.2\)</span> 下的 <span class="math notranslate nohighlight">\(L_q\)</span> 惩罚以及 <span class="math notranslate nohighlight">\(\alpha=0.2\)</span> 的弹性网惩罚；很难从肉眼来观察出差异。弹性网像 lasso 一样选择变量，同时像岭回归一样收缩相关变量的系数。同时考虑了 <span class="math notranslate nohighlight">\(L_q\)</span> 惩罚的计算优势。我们将在 <span class="xref myst">18.4节</span>介绍弹性网惩罚。</p>
<p><img alt="" src="../_images/fig3.13.png" /></p>
<blockquote>
<div><p>图3.13 <span class="math notranslate nohighlight">\(q=1.2\)</span> 时 <span class="math notranslate nohighlight">\(\sum_j\vert\beta_j\vert^q\)</span> 为常数值的轮廓线（左图）以及 <span class="math notranslate nohighlight">\(\alpha=0.2\)</span> 时弹性网惩罚 <span class="math notranslate nohighlight">\(\sum_j(\alpha\beta_j^2+(1-\alpha)\vert\beta_j\vert)\)</span> 为常数值的轮廓线（右图）。尽管看起来很相似，弹性网有尖角（不可导），而 <span class="math notranslate nohighlight">\(q=1.2\)</span> 的惩罚不会有尖角。</p>
</div></blockquote>
</div>
<div class="section" id="id7">
<h2>（ 4 ）最小角回归<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p><strong>最小角回归 (LAR)</strong> 是相对较新的方法 (Efron et al., 2004<a class="footnote-reference brackets" href="#id14" id="id8">3</a>)，而且可以看成是一种向前逐步回归（3.3.2 节）的“民主 (democratic)”版本。正如我们将看到的，LAR 与 lasso 联系紧密，并且事实上提供了如图 3.10 所示的计算整个 lasso 路径的非常有效的算法。</p>
<blockquote>
<div><p>note “weiya 注：”
在 Efron 的论文中，最小角回归缩写为 LARS，我们这里仍以 ESL 书上的缩写为准——LAR</p>
</div></blockquote>
<p>向前逐步回归逐步建立模型，每次添加一个变量。每一步，它选出最好的变量加入活跃集，然后更新最小二乘来加入所有的活跃变量。</p>
<p>最小角回归采用类似的策略，但是仅仅加入一个变量应有的程度。第一步它确定与响应变量最相关的变量。不是完全的拟合该变量，LAR 使得该变量的系数向最小二乘值连续变化（使得它与进化的残差之间的相关系数绝对值降低）。只要其他变量与残差的相关性与该变量和残差的相关性相等，则该过程暂停。第二个变量加入活跃集，然后它们的系数一起以保持相关性相等并降低的方式变化。这个过程一直继续直到所有的变量都在模型中，然后在全最小二乘拟合处停止。算法 3.2 给出了详细过程。第 5 步的终止条件需要一些解释。如果 <span class="math notranslate nohighlight">\(p &gt;N-1\)</span>，LAR 算法经过 <span class="math notranslate nohighlight">\(N-1\)</span> 步达到 0 残差解（<span class="math notranslate nohighlight">\(-1\)</span> 是因为我们已经对数据进行了中心化）</p>
<p><img alt="" src="../_images/Alg3.2.png" /></p>
<hr class="docutils" />
<p><strong>算法 3.2</strong> 最小角回归</p>
<hr class="docutils" />
<ol class="simple">
<li><p>对预测变量进行标准化处理得到零均值和单位范数。以残差向量 <span class="math notranslate nohighlight">\(\mathbf{r=y-\bar{y}},\beta_1,\ldots,\beta_p=0\)</span> 开始。</p></li>
<li><p>找出与 <span class="math notranslate nohighlight">\(\mathbf{r}\)</span> 最相关的预测变量 <span class="math notranslate nohighlight">\(\mathbf x_j\)</span></p></li>
<li><p>从 0 开始移动 <span class="math notranslate nohighlight">\(\beta_j\)</span> 一直到最小二乘系数 <span class="math notranslate nohighlight">\(\langle\mathbf x_j, \mathbf r\rangle\)</span>，直到存在其它的预测变量 <span class="math notranslate nohighlight">\(\mathbf x_k\)</span> 使得其与当前残差的相关性等于 <span class="math notranslate nohighlight">\(\mathbf x_j\)</span> 与当前残差的相关性。</p></li>
<li><p>在由当前残差在 <span class="math notranslate nohighlight">\((\mathbf x_j,\mathbf x_k)\)</span> 上的联合最小二乘系数方向上移动 <span class="math notranslate nohighlight">\(\beta_j\)</span> 和 <span class="math notranslate nohighlight">\(\beta_k\)</span>，直到存在其它的预测变量 <span class="math notranslate nohighlight">\(x_l\)</span> 与当前残差的相关性和当前残差与 <span class="math notranslate nohighlight">\((\mathbf x_j,\mathbf x_k)\)</span> 的相关性相等。</p></li>
<li><p>按这种方式继续直到所有的 <span class="math notranslate nohighlight">\(p\)</span> 个预测变量加入到模型中。经过 <span class="math notranslate nohighlight">\(\min(N-1, p)\)</span> 步，我们达到了全最小二乘的解。</p></li>
</ol>
<hr class="docutils" />
<blockquote>
<div><p>tip “weiya 注：LAR 示意图”
结合原论文的示意图能帮助理解最小角回归的逻辑。</p>
</div></blockquote>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>![](../img/03/lars.png)
</pre></div>
</div>
<p>假设 <span class="math notranslate nohighlight">\(\mathcal A_k\)</span> 是第 <span class="math notranslate nohighlight">\(k\)</span> 步开始时的变量活跃集，<span class="math notranslate nohighlight">\(\beta_{\mathcal A_k}\)</span> 是这一步中变量的系数向量；其中有 <span class="math notranslate nohighlight">\(k-1\)</span> 个非零值，刚刚进入的变量系数值为 0。如果当前残差为 <span class="math notranslate nohighlight">\(\mathbf r_k=\mathbf y-\mathbf X_{\mathcal A_k}\beta_{\mathcal A_k}\)</span>，则当前步的方向为</p>
<div class="math notranslate nohighlight">
\[
\delta_k=(\mathbf X^T_{ \mathcal A_k}\mathbf X_{\mathcal A_k})^{-1}\mathbf X^T_{\mathcal A_k}\mathbf r_k \tag{3.55}
\]</div>
<p>然后系数迭代为 <span class="math notranslate nohighlight">\(\beta_{\mathcal A_k} (\alpha) = \beta_{\mathcal A_k} + \alpha · \delta_k\)</span>。<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/100">练习 3.23</a> 证明这种方式下选择的方向满足断言：<strong>保持（各个预测变量与残差间的）相关系数相等和递减（tied and decreasing）</strong>。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.23”
已解决，具体证明过程参见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/100">Issue 100: Ex. 3.23</a>。起初翻译时，对 tied 的理解不够，通过求解该练习题，认为 tied 意思其实就是<strong>各个预测变量与残差之间的相关系数保持相等</strong>。</p>
</div></blockquote>
<p>如果该步的开始拟合向量为 <span class="math notranslate nohighlight">\(\hat{\mathbf f}\_k\)</span>，则迭代为 <span class="math notranslate nohighlight">\(\hat{\mathbf f}\_k(\alpha)=\mathbf f_k+\alpha\cdot\mathbf u_k\)</span>，其中 <span class="math notranslate nohighlight">\(\mathbf u_k=\mathbf X_{\mathcal A_k}\delta_k\)</span> 是新的拟合方向。“最小角”由该过程的几何解释得到；<span class="math notranslate nohighlight">\(\mathbf u_k\)</span> 使得活跃集 <span class="math notranslate nohighlight">\({\mathcal A}_k \)</span>中预测变量间的角度最小（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/101">练习 3.24</a>）。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.24”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/101">Issue 101: Ex. 3.24</a>，欢迎交流讨论。</p>
</div></blockquote>
<p>图 3.14 使用模拟数据显示了相关系数的绝对值下降以及每一步 LAR 算法中变量进入的顺序。</p>
<p><img alt="" src="../_images/fig3.14.png" /></p>
<blockquote>
<div><p>图 3.14：通过 6 个预测变量的拟合数据集，每一步 LAR 过程中的相关性绝对值的变化。图象上方的标签表示在每一步哪些变量加进了活跃集。步长是用单位 <span class="math notranslate nohighlight">\(L_1\)</span> 弧长来测量的。</p>
</div></blockquote>
<p><img alt="" src="../_images/fig3.15.png" /></p>
<p>由构造知 LAR 的系数以一种分段线性的方式进行改变。图 3.15（左图）显示了 LAR 系数曲线作为 <span class="math notranslate nohighlight">\(L_1\)</span> 弧长的函数曲线。</p>
<blockquote>
<div><p>note “weiya 注：原书脚注”
<span class="math notranslate nohighlight">\(L_1\)</span> arc length：可导曲线 <span class="math notranslate nohighlight">\(\beta(s), s \in [0,S]\)</span> 的 <span class="math notranslate nohighlight">\(L_1\)</span> 弧长为 <span class="math notranslate nohighlight">\(TV(\beta,S)=\int_0^S\Vert\dot{\beta}(s)\Vert_1ds\)</span>，其中 <span class="math notranslate nohighlight">\(\dot{\beta}(s)=\partial\beta(s)/\partial s\)</span>。对于分段 LAR 函数曲线，这相当于从这一步到下一步系数的 <span class="math notranslate nohighlight">\(L_1\)</span> 范数变化之和。</p>
</div></blockquote>
<blockquote>
<div><p>图 3.15：左图显示了 LAR 系数作为 <span class="math notranslate nohighlight">\(L_1\)</span> 长度的函数在模拟数据上的图象。右图显示了 Lasso 的图象。它们大概在 <span class="math notranslate nohighlight">\(L_1\)</span> 弧长为 18 之前（深蓝色的系数曲线通过 0）都是完全相同的.</p>
</div></blockquote>
<p>注意到我们不需要走很小的步以及重新检查步骤 3 的相关系数；应用预测变量的协方差和算法的分段线性性质，我们可以在每一步开始计算出确切的步长（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/98">练习 3.25</a>）。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 3.25”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/98">Issue 98: Ex. 3.25</a>，欢迎讨论交流！</p>
</div></blockquote>
<p>图 3.15 的右图展示了对同样数据的 lasso 系数曲线。几乎与左图相同，当绿色曲线通过 0 时首次出现不同。对于前列腺癌数据，LAR 系数曲线显示与图 3.10 的 lasso 曲线相同，该曲线从不经过 0。这些观测值促使对 LAR 算法进行简单修改，给出了整个 lasso 路径，它同样也是分段线性的。</p>
<p><img alt="" src="../_images/Alg3.2a.png" /></p>
<hr class="docutils" />
<p><strong>算法 3.2a</strong> 最小角回归：Lasso修正</p>
<hr class="docutils" />
<p>4a. 如果一个非零的系数达到0，则从变量的活跃集中删除该变量并且重新计算当前的联合最小二乘方向。</p>
<hr class="docutils" />
<p>LAR(lasso) 算法是非常有效的，需要用 <span class="math notranslate nohighlight">\(p\)</span> 个预测变量的单最小二乘拟合的相同步骤进行计算。最小角回归总是需要 <span class="math notranslate nohighlight">\(p\)</span> 步达到全最小二乘估计。lasso 路径可能超过 <span class="math notranslate nohighlight">\(p\)</span> 步，尽管这两者经常是非常相似的。经过 lasso 修正的 3.2a 的算法 3.2 是计算任何一个lasso 问题的有效方式，特别是当 <span class="math notranslate nohighlight">\(p &gt; &gt;N\)</span>。Osborne et al. (2000a)<a class="footnote-reference brackets" href="#id15" id="id9">5</a> 也发现了计算 lasso 的分段线性的路径，他们称之为<strong>同伦 (homotopy)</strong> 算法。</p>
<p>我们已经给出一个为什么这些过程很相似的启发式的论据。尽管 LAR 算法是用相关性来叙述的，但如果输入特征是标准化的，它与内积是等价的并且用内积更简单。假设 <span class="math notranslate nohighlight">\(\mathcal A\)</span> 是算法中某些步的变量活跃集，它们与当前残差 <span class="math notranslate nohighlight">\(\mathbf y -\mathbf X\beta\)</span> 的内积的绝对值是结合在一起的。我们可以表达成</p>
<div class="math notranslate nohighlight">
\[
\mathbf x_j^T(\mathbf y-\mathbf X\beta)=\gamma\cdot s_j,\forall j\in {\mathcal A} \tag{3.56}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(s_j\in\\{-1,1\\}\)</span> 表示内积的符号，<span class="math notranslate nohighlight">\(\gamma\)</span> 是普通的数值。并且 <span class="math notranslate nohighlight">\(\vert \mathbf x_k^T(\mathbf y-\mathbf X\beta)\vert\le \gamma\; \forall k\notin \mathcal A\)</span>。现在我们考虑 式（ 3.52 ） 的 lasso 准则，我们可以写成向量形式</p>
<div class="math notranslate nohighlight">
\[
F(\beta)=\frac{1}{2}\Vert\mathbf y-\mathbf X\beta\Vert_2^2+\lambda\Vert\beta\Vert_1\tag{3.57}
\]</div>
<p>令 <span class="math notranslate nohighlight">\(\mathcal B\)</span> 为在给定 <span class="math notranslate nohighlight">\(\lambda\)</span> 值下解中的变量的活跃集。对于这些变量 <span class="math notranslate nohighlight">\(R(\beta)\)</span> 是可导的，并且 <strong>平稳条件 (stationary condition)</strong> 为</p>
<div class="math notranslate nohighlight">
\[
\mathbf x_j^T(\mathbf y-\mathbf X\beta)=\lambda\cdot \mathrm{sign}(\beta_j),\forall j\in {\mathcal B}\tag{3.58}
\]</div>
<p>比较 式（ 3.58 ） 和 式（ 3.56 ），我们看到只有当 <span class="math notranslate nohighlight">\(\beta_j\)</span> 的符号与内积的符号相同时，这两个等式才相同。这也就是为什么 LAR 算法和 lasso 当一个活跃系数经过零开始出现不同；对于不满足条件 式（ 3.58 ） 的变量，会被踢出活跃集 <span class="math notranslate nohighlight">\(\mathcal B\)</span>。<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/100">练习 3.23</a> 证明了这些等式表明随<span class="math notranslate nohighlight">\(\lambda\)</span> 减小的分段线性系数曲线。对于不活跃的变量的平稳条件要求</p>
<div class="math notranslate nohighlight">
\[
\vert\mathbf x_k^T(\mathbf y-\mathbf X\beta)\vert\le\lambda,\forall k\notin{\mathcal B}\tag{3.59}
\]</div>
<p>这与 LAR 算法一致。</p>
<p><img alt="" src="../_images/fig3.16.png" /></p>
<blockquote>
<div><p>图 3.16：LAR、lasso、向前逐步、向前逐渐（FS）和增长向前逐渐（<span class="math notranslate nohighlight">\(FS_0\)</span>）回归之间的比较。设定与图3.6相同，除了这里<span class="math notranslate nohighlight">\(N=100\)</span>而不是300.这里较慢的<span class="math notranslate nohighlight">\(FS\)</span>回归最终表现得比向前逐步好。LAR和lasso表现得和FS、<span class="math notranslate nohighlight">\(FS_0\)</span>相似。因为这些过程采取不同的步数（根据模拟复制和方法），我们画出最小二乘拟合的MSE关于整体<span class="math notranslate nohighlight">\(L_1\)</span>弧长的片段的函数。</p>
</div></blockquote>
<p>图 3.16 将 LAR 和 lasso 与向前逐步（forward stepwise）和向前逐渐（forward stagewise）回归进行比较。设定与图 3.6 是相同的，除了这里的 <span class="math notranslate nohighlight">\(N=100\)</span> 而不是 <span class="math notranslate nohighlight">\(300\)</span>，所以这个问题更加困难。我们可以看到增长性更快的向前逐步很快地过拟合（10 个变量加入模型中之前是很好的），最终比增长性较慢的向前逐渐回归表现得更差。LAR 和 lasso 的行为与向前逐渐回归相似。增长的向前逐渐回归与 LAR 和 lasso 类似，并且将在 <span class="xref myst">3.8.1 节</span>中描述。</p>
<div class="section" id="lar-lasso">
<h3>LAR 和 Lasso 自由度公式<a class="headerlink" href="#lar-lasso" title="Permalink to this headline">¶</a></h3>
<p>假设我们通过最小角回归过程拟合了线性模型，在某步 <span class="math notranslate nohighlight">\(k &lt; p\)</span> 停止，或者等价地用 lasso 的界 <span class="math notranslate nohighlight">\(t\)</span> 得到约束情况下的全最小二乘拟合。我们需要多少参数，或者自由度？</p>
<p>首先考虑采用 <span class="math notranslate nohighlight">\(k\)</span> 个特征的子集的线性回归。如果这个子集是没有通过训练数据而事先确定好的，然后在该拟合模型中的自由度定义为<span class="math notranslate nohighlight">\(k\)</span>。当然，在经典统计学中，线性独立参数的个数也就是自由度。另外地，假设我们用一个最优子集选择确定了最优的 <span class="math notranslate nohighlight">\(k\)</span> 个预测变量。于是结果模型中有 <span class="math notranslate nohighlight">\(k\)</span> 个参数，但在某种意义上我们用了大于 <span class="math notranslate nohighlight">\(k\)</span> 个的自由度。</p>
<p>我们需要一个对于自适应拟合模型的有效自由度的一般定义。我们定义拟合向量 <span class="math notranslate nohighlight">\(\hat{\mathbf y}=(\hat y_1,\hat y_2,\ldots,\hat y_N)\)</span> 的自由度为</p>
<div class="math notranslate nohighlight">
\[
\mathrm{df}(\hat{\mathbf y})=\frac{1}{\sigma^2}\sum\limits_{i=1}^N\mathrm{Cov}(\hat y_i,y_i)\tag{3.60}
\]</div>
<p>这里 <span class="math notranslate nohighlight">\(\mathrm{Cov}(\hat y_i,y_i)\)</span> 指的是预测值 <span class="math notranslate nohighlight">\(\hat y_i\)</span> 和其对应的输出向量 <span class="math notranslate nohighlight">\(y_i\)</span> 之间的协方差。直观上看有意义：当拟合数据越困难，协方差会越大，从而 <span class="math notranslate nohighlight">\(\mathrm{df}(\hat{\mathbf y})\)</span> 越大。表达式 式（ 3.60 ） 是一个有用的自由度的概念，可以应用到任何模型的预测向量 <span class="math notranslate nohighlight">\(\hat{\mathbf y}\)</span>。其中包括那些对训练数据自适应拟合的模型。这个定义将在 <span class="xref myst">7.4-7.6 节</span> 中进一步讨论。</p>
<p>现在对于有 <span class="math notranslate nohighlight">\(k\)</span> 个固定预测变量的线性回归模型，可以简单地证明 <span class="math notranslate nohighlight">\(\mathrm{df}(\hat{\mathbf y})=k\)</span>。同样地，对于岭回归，这一定义导出表达式（3.50）的 <strong>闭型解 (closed-form)</strong>：<span class="math notranslate nohighlight">\(\mathrm{df}(\hat{\mathbf{y}})=\mathrm{tr}(\mathbf S_\lambda)\)</span>。</p>
<p>在这些情况下，式（ 3.60 ） 可以很简单地进行赋值因为 <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}=\mathbf{H}_\lambda\mathbf y\)</span> 关于 <span class="math notranslate nohighlight">\(\mathbf y\)</span> 是线性的。如果我们考虑在大小为 <span class="math notranslate nohighlight">\(k\)</span> 的最优子集选择中的定义 式（ 3.60 ），似乎显然有 <span class="math notranslate nohighlight">\(\mathrm{df}(\hat{\mathbf y})\)</span> 会大于 <span class="math notranslate nohighlight">\(k\)</span>，并且可以通过运用模拟的方法直接地估计 <span class="math notranslate nohighlight">\(\mathrm{Cov}(\hat y_i,y_i)/\sigma^2\)</span> 来验证。然而估计最优子集选择的 <span class="math notranslate nohighlight">\(\mathrm{df}(\hat{\mathbf y})\)</span> 没有闭型解。</p>
<p>对于 LAR 和 lasso，会发生很奇怪的事情。这些技巧的自适应方式比最优集选择更加光滑，因此估计自由度会更加地难以驾驭。特别地，可以证明经过 <span class="math notranslate nohighlight">\(k\)</span> 步 LAR 过程，拟合向量的有效自由度恰巧是 <span class="math notranslate nohighlight">\(k\)</span>。对于 lasso，（改进的）LAR 过程经常需要多余 <span class="math notranslate nohighlight">\(k\)</span> 的步骤，因为可以删去预测变量。因此，定义有点不一样；对于 lasso，在任一小步 <span class="math notranslate nohighlight">\(\mathrm{df}(\hat{\mathbf y})\)</span> 近似等于模型中预测变量的个数。然而这种近似在 lasso 路径中的任何地方都表现得很好，但是对于每个 <span class="math notranslate nohighlight">\(k\)</span>，它在包含 <span class="math notranslate nohighlight">\(k\)</span> 个预测变量的序列中最后一个模型表现得最好。关于 lasso 自由度详细的研究或许可以在 Zou et al. (2007)<a class="footnote-reference brackets" href="#id16" id="id10">6</a> 中找到。</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id11"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p>Hoerl, A. E. and Kennard, R. (1970). Ridge regression: biased estimation for nonorthogonal problems, Technometrics 12: 55–67.</p>
</dd>
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>Chen, S. S., Donoho, D. and Saunders, M. (1998). Atomic decomposition by basis pursuit, SIAM Journal on Scientific Computing 20(1): 33–61.</p>
</dd>
<dt class="label" id="id13"><span class="brackets"><a class="fn-backref" href="#id6">4</a></span></dt>
<dd><p>Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net, Journal of the Royal Statistical Society Series B. 67(2): 301–320.</p>
</dd>
<dt class="label" id="id14"><span class="brackets"><a class="fn-backref" href="#id8">3</a></span></dt>
<dd><p>Efron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2004). Least angle regression (with discussion), Annals of Statistics 32(2): 407–499.</p>
</dd>
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id9">5</a></span></dt>
<dd><p>Osborne, M., Presnell, B. and Turlach, B. (2000a). A new approach to variable selection in least squares problems, IMA Journal of Numerical Analysis 20: 389–404.</p>
</dd>
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id10">6</a></span></dt>
<dd><p>Zou, H., Hastie, T. and Tibshirani, R. (2007). On the degrees of freedom of the lasso, Annals of Statistics 35(5): 2173–2192.</p>
</dd>
</dl>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./03-Linear-Methods-for-Regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="3.3-Subset-Selection.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">3.3 选择预测变量的子集</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3.5-Methods-Using-Derived-Input-Directions.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">3.5 运用派生输入方向的方法</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>