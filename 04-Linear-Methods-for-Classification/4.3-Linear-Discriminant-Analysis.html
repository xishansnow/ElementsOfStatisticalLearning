
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4.3 线性判别分析 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.4 逻辑斯蒂回归" href="4.4-Logistic-Regression.html" />
    <link rel="prev" title="4.2 指示矩阵的线性回归" href="4.2-Linear-Regression-of-an-Indicator-Matrix.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.3-Subset-Selection.html">
     3.3 选择预测变量的子集
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 平滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 平滑参数
     <span class="math notranslate nohighlight">
      \(\lambda\)
     </span>
     的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波平滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核平滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     维空间中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     维空间中的结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差、方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的乐观估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 “条件测试误差”还是“测试误差的期望”？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5
     <code class="docutils literal notranslate">
      <span class="pre">
       EM
      </span>
      <span class="pre">
       算法
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆叠
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14-Unsupervised-Learning/14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   （ 1 ）正则化判别分析
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lda">
   （ 2 ）LDA 的计算
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   （ 3 ）降维线性判别分析
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>4.3 线性判别分析</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   （ 1 ）正则化判别分析
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lda">
   （ 2 ）LDA 的计算
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   （ 3 ）降维线性判别分析
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>4.3 线性判别分析<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>分类的判别理论（<span class="xref myst">2.4节</span>）告诉我们，我们需要知道的是最优分类的类别后验概率 <span class="math notranslate nohighlight">\(\mathrm{Pr}(G\mid X)\)</span>。假设 <span class="math notranslate nohighlight">\(f_k(x)\)</span> 是类别 <span class="math notranslate nohighlight">\(G=k\)</span> 中 <span class="math notranslate nohighlight">\(X\)</span> 的类别条件密度，并令 <span class="math notranslate nohighlight">\(\pi_k\)</span> 为类别 <span class="math notranslate nohighlight">\(k\)</span> 的先验概率，满足 <span class="math notranslate nohighlight">\(\sum_{k=1}^K\pi_k=1\)</span>。简单地应用一下贝叶斯定理得到</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Pr}(G=k\mid X=x)=\dfrac{f_k(x)\pi_k}{\sum_{\ell=1}^Kf_{\ell}(x)\pi_\ell}\tag{4.7}
\]</div>
<p>我们看到就 <strong>判别能力(ability to classify)</strong> 而言，知道 <span class="math notranslate nohighlight">\(f_k(x)\)</span> 几乎等价于知道概率 <span class="math notranslate nohighlight">\(\mathrm{Pr}(G=k\mid X=x)\)</span>。</p>
<p>许多方法是基于类别密度的模型：</p>
<ul class="simple">
<li><p>采用高斯密度的线性和二次判别分析</p></li>
<li><p>更加灵活的混合的高斯密度，允许非线性判别边界（<span class="xref myst">6.8 节</span>）</p></li>
<li><p>对每个类别密度进行一般的非参数密度估计，允许最大的灵活性（<a class="reference external" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification/index.html#_3">6.6.2 节</a>）</p></li>
<li><p><strong>朴素贝叶斯(Naive Bayes)</strong> 模型是上个情形的变种，并且假设每个类密度是边缘密度的乘积，这也就是，假设输入在每一类中都是条件独立的（<a class="reference external" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification/index.html#_4">6.6.3节</a>）</p></li>
</ul>
<p>假设我们每个类别密度用多元高斯分布来建模</p>
<div class="math notranslate nohighlight">
\[
f_k(x)=\frac{1}{(2\pi)^{p/2}\vert \boldsymbol\Sigma_k\vert^{1/2}}e^{-\frac{1}{2}(x-\mu_k)^T\boldsymbol\Sigma^{-1}_k(x-\mu_k)} \tag{4.8}
\]</div>
<p>假设类别有相同的协方差矩阵 <span class="math notranslate nohighlight">\(\Sigma_k=\Sigma\;\forall k\)</span> 时，会导出 <strong>线性判别分析 (LDA)</strong>。在比较两个类别 <span class="math notranslate nohighlight">\(k\)</span> 和 <span class="math notranslate nohighlight">\(\ell\)</span> 时，比较 log-ratio 就足够了，而且我们可以看到</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
&amp;\log\frac{\mathrm{Pr}(G=k\mid X=x)}{\mathrm{Pr}(G=\ell\mid X=x)}\\=&amp;\log\frac{f_k(x)}{f_\ell(x)}+\log\frac{\pi_k}{\pi_\ell}\\
=&amp;\log\frac{\pi_k}{\pi_\ell}-\frac{1}{2}(\mu_k+\mu_\ell)^T\boldsymbol\Sigma^{-1}(\mu_k-\mu_\ell)+x^T\boldsymbol\Sigma^{-1}(\mu_k-\mu_\ell)
\end{align*}\tag{4.9}
\end{split}\]</div>
<blockquote>
<div><p><strong>注解：</strong>
如果 <span class="math notranslate nohighlight">\(D\)</span> 为对称矩阵，则</p>
<div class="math notranslate nohighlight">
\[
a'Da-b'Db=(a+b)'D(a-b)\,.    
&gt;\]</div>
</div></blockquote>
<p>这是个关于 <span class="math notranslate nohighlight">\(x\)</span> 的线性等式。协方差矩阵相等时会消除了正规化因子，以及指数中的二次项。这个线性的 log-odds 函数表明类别 <span class="math notranslate nohighlight">\(k\)</span> 和类别 <span class="math notranslate nohighlight">\(\ell\)</span> 的判别边界为 <span class="math notranslate nohighlight">\(p\)</span> 维超平面——<span class="math notranslate nohighlight">\(\mathrm{Pr}(G=k\mid X=x)=\mathrm{Pr}(G=\ell\mid X=x)\)</span> 的集合，这关于 <span class="math notranslate nohighlight">\(x\)</span> 是线性的。对于任意类别对也是成立的，所以所有的判别边界都是线性的。如果我们把 <span class="math notranslate nohighlight">\(\mathbb{R}^p\)</span> 分成不同区域，记为类别 1，类别 2 等等，这些区域会被超平面所分离。图 4.5（左图）显示了有三个类别且 <span class="math notranslate nohighlight">\(p=2\)</span> 的假想的例子。这里数据是从 3 个协方差矩阵相等的高斯分布中产生的。我们已经在图中画出了对应 95% 概率密度的等高图，以及三个类别的形心。注意到判别边界不是连接两个形心的垂直平分线。如果协方差矩阵 <span class="math notranslate nohighlight">\(\Sigma\)</span> 是 <span class="math notranslate nohighlight">\(\sigma^2\mathbf I\)</span>，且类别的先验概率相等。从 式（ 4.9 ） 我们可以看出线性判别函数为</p>
<div class="math notranslate nohighlight">
\[
\delta_k(x)=x^T\boldsymbol\Sigma^{-1}\mu_k-\frac{1}{2}\mu_k^T\boldsymbol\Sigma^{-1}\mu_k+\log\pi_k\tag{4.10}
\]</div>
<p>这是判别规则的等价描述，<span class="math notranslate nohighlight">\(G(x)=\mathrm{argmax}_k \delta_k(x)\)</span>。</p>
<p><img alt="" src="../_images/fig4.5.png" /></p>
<blockquote>
<div><p>图 4.5. 左图显示了三个高斯分布，有相同的协方差和不同的均值。图中画出了在包含每个类别 95% 可能性的等高线。两两类别之间的贝叶斯判别边界用虚线显示，并且贝叶斯判别边界将所有的三个类别分隔开用实线表示出来（前者的一个子集）。右图我们看到有来自每个高斯分布的 30 个样本点以及画出了拟合后的 LDA 判别边界。</p>
</div></blockquote>
<blockquote>
<div><p>info “weiya 注：重现图 4.5”
详细过程见<span class="xref myst">模拟：Fig. 4.5</span>。</p>
</div></blockquote>
<p>实际应用中我们不知道高斯分布的参数，我们需要用我们的训练数据去估计它们：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat \pi_k=N_k/N\)</span>，其中 <span class="math notranslate nohighlight">\(N_k\)</span> 是第 <span class="math notranslate nohighlight">\(k\)</span> 类观测值的个数；</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat\mu_k=\sum_{g_i=k}x_i/N_k\)</span>；</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\boldsymbol\Sigma}=\sum_{k=1}^K\sum_{g_i=k}(x_i-\hat\mu_k)(x_i-\hat\mu_k)^T/(N-K)\)</span>。</p></li>
</ul>
<p>图 4.5（右图）显示了基于从 3 个高斯分布中取出的大小为 30 的样本集得到的判别边界的估计，图 4.1 是另外一个例子，但是那里类别不是高斯分布。</p>
<blockquote>
<div><p>note “weiya 注：Recall”</p>
<div class="math notranslate nohighlight">
\[
&gt;\underset{\mathbf B}{\min}\sum\limits_{i=1}^N\Vert y_i-[(1,x_i^T)\mathbf B]^T\Vert^2\tag{4.5}    
&gt;\]</div>
</div></blockquote>
<p>两个类别的情况下在线性判别分析和线性最小二乘之间有一个简单的对应，如 式（ 4.5 ） 所示。如果满足下面条件则 LDA 分类规则分给第二类</p>
<!--

$$
x^T\hat{\boldsymbol\Sigma}^{-1}(\hat\mu_2-\hat\mu_1)>\frac{1}{2}\hat\mu^T_2\hat\Sigma^{-1}\hat\mu_2-\frac{1}{2}\hat\mu_1^T\hat{\boldsymbol\Sigma}^{-1}\hat\mu_1+\log(N_1/N)-\log(N_2/N)\tag{4.11}
$$

-->
<div class="math notranslate nohighlight">
\[
x^T\hat{\boldsymbol\Sigma}^{-1}(\hat\mu_2-\hat\mu_1)&gt;\frac{1}{2}(\hat\mu_2+\hat\mu_1)^T\hat{\boldsymbol\Sigma}^{-1}(\hat\mu_2-\hat\mu_1)-\log(N_2/N_1)\tag{4.11}
\]</div>
<p>否则分为第一类。假设我们将两个类别分别编码为 <span class="math notranslate nohighlight">\(+1\)</span> 和 <span class="math notranslate nohighlight">\(-1\)</span>。可以简单地证明得出最小二乘的系数向量与 式（ 4.11 ） 式给出的 LDA 方向成比例（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/108">练习 4.2</a>）。【实际上，这种对应关系对于任意的编码都会有，见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/108">练习 4.2</a>】。然而除了 <span class="math notranslate nohighlight">\(N_1=N_2\)</span> 的情形，截距不同因此得到的判别边界不一样。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 4.2”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/108">Issue 108: Ex. 4.2</a>。</p>
</div></blockquote>
<p>因为通过最小二乘得到的 LDA 方向不需要对特征做高斯分布的假设，它的应用可以不局限于高斯分布的数据。然而，式（ 4.11 ） 式给出的特定的截距和分离点确实需要高斯分布的数据。因此对于给定的数据集凭经验地选择分割点最小化训练误差是有意义的。这也是我们在实际中发现效果很好，但是在理论中没有提到的原因。</p>
<p>多于两个类别的情形时，LDA 与类别指示矩阵的线性回归不是一样的，而且它避免了跟这个方法有关的掩藏问题 (Hastie et al., 1994<a class="footnote-reference brackets" href="#id7" id="id2">1</a>)。 回归与 LDA 之间的对应可以通过在 <span class="xref myst">12.5 节</span>中讨论的最优评分来建立。</p>
<p>回到一般的判别问题 式（ 4.8 ），如果没有假设 <span class="math notranslate nohighlight">\(\boldsymbol\Sigma_k\)</span> 相等，则 式（ 4.9 ） 中的抵消不会发生；特别地，关于 <span class="math notranslate nohighlight">\(x\)</span> 的平方项保留了下来。于是我们得到了平方判别函数 QDA</p>
<div class="math notranslate nohighlight">
\[
\delta_k(x)=-\frac{1}{2}\log\vert\Sigma_k\vert-\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)+\log\pi_k\tag{4.12}
\]</div>
<p>每个 <strong>类别对 (pair of classes)</strong> <span class="math notranslate nohighlight">\(k\)</span> 和 <span class="math notranslate nohighlight">\(\ell\)</span> 的判别边界由二次等式来描述 <span class="math notranslate nohighlight">\(\\{x:\delta_k(x)=\delta_\ell(x)\\}\)</span>。</p>
<p>图 4.6 显示了一个例子（图 4.1），这三个类别都是混合高斯分布（<span class="xref myst">6.8 节</span>），并且判别边界由关于 <span class="math notranslate nohighlight">\(x\)</span> 的二次函数近似给出。这里我们描述两种拟合这些判别边界的方式。右图使用这里描述的 QDA，而左图是在增广的五维二次多项式空间中使用 LDA。两者之间的差别很小，QDA 是最好的方式，LDA 方法是更方便的替换。</p>
<p><img alt="" src="../_images/fig4.6.png" /></p>
<blockquote>
<div><p>图 4.6. 拟合二次边界的两种方法。左图显示了图 4.1 的二次判别边界（在五维空间 <span class="math notranslate nohighlight">\(X_1,X_2,X_1X_2,X_1^2,X_2^2\)</span> 中运用 LDA 得到）。右图显示了通过 QDA 寻找到的二次判别边界。两者差别很小，通常也是这种情况。</p>
</div></blockquote>
<p>QDA 的估计类似 LDA 的估计，除了协方差矩阵必须要按每一类来估计。当 <span class="math notranslate nohighlight">\(p\)</span> 很大这意味着系数有显著性的增长。因为判别边界是系数密度的函数，参数的个数必须要考虑。对于 LDA，似乎有 <span class="math notranslate nohighlight">\((K-1)\times(p+1)\)</span> 个参数，因为我们仅仅需要判别函数之间的不同 <span class="math notranslate nohighlight">\(\delta_k(x)-\delta_K(x)\)</span>，其中 <span class="math notranslate nohighlight">\(K\)</span> 是一些预先选好的类别（这里我们已经选了最后一类），每个差异需要 <span class="math notranslate nohighlight">\(p+1\)</span> 个参数。对于 QDA 类似地，我们会有 <span class="math notranslate nohighlight">\((K-1)\times \\{p(p+3)/2+1\\}\)</span> 个参数。LDA 和 QDA 在非常大以及离散的数据集的分类上面表现得很好。举个例子，在 STATLOG 项目中 (Michie et al. 1994<a class="footnote-reference brackets" href="#id8" id="id3">3</a>) LDA 在 7 个数据集（总共 22 个数据集）中的表现排前三名，QDA 在四个数据集中排前三名，对于 10 个数据集两种方法的其中一种排前三名。两种方法都被广泛运用，整本书集中讨论 LDA。在各种外来方法风靡一时的今天，我们总是会有两种简单的方法可以使用。为什么 LDA 和 QDA 有那么好的效果？原因不可能是数据近似服从高斯分布，对于 LDA 协方差矩阵也不可能近似相等。很可能的一个原因是数据仅仅可以支持简单的判别边界比如线性和二次，并且通过高斯模型给出的估计是稳定的，这是一个偏差与方差之间的权衡——我们可以忍受线性判别边界的偏差因为它可以通过用比其它方法更低的方差来弥补。这个论点对于 QDA 更是不可想象，因为它自身有许多的参数，尽管或许比非参估计的参数要少。</p>
<div class="section" id="id4">
<h2>（ 1 ）正则化判别分析<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>Friedman (1989)<a class="footnote-reference brackets" href="#id9" id="id5">2</a> 提出 LDA 和 QDA 之间的一个权衡，使得 QDA 的协方差阵向 LDA 中的共同协方差阵收缩。这些方法非常类似岭回归。正则化协方差矩阵有如下形式</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol\Sigma}_k(\alpha)=\alpha\hat{\boldsymbol\Sigma}_k+(1-\alpha)\hat{\boldsymbol\Sigma}\tag{4.13}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(\hat\Sigma\)</span> 是和 LDA 一样用的联合协方差矩阵。这里 <span class="math notranslate nohighlight">\(\alpha\in[0,1]\)</span> 允许在 LDA 和 QDA 之间连续变化的模型，而且需要指定。实际中，<span class="math notranslate nohighlight">\(\alpha\)</span> 可以基于在验证数据的表现上进行选择，或者通过交叉验证。</p>
<p><img alt="" src="../_images/fig4.7.png" /></p>
<blockquote>
<div><p>图 4.7. 对元音数据应用一系列 <span class="math notranslate nohighlight">\(\alpha\in[0,1]\)</span> 的正则化判别分析的测试和训练误差。测试数据的最优点发生在 <span class="math notranslate nohighlight">\(\alpha=0.9\)</span> 附近，离二次判别分析很相近。</p>
</div></blockquote>
<p>图 4.7 显示了将 RDA 运用到元音数据的结果。训练和测试误差都随着 <span class="math notranslate nohighlight">\(\alpha\)</span> 的增大获得了改善，尽管在 <span class="math notranslate nohighlight">\(\alpha=0.9\)</span> 后测试误差急剧上升。训练和测试误差最大的区别部分因为在小数量的个体上有很多重复观测，在训练和测试集上是不同的。</p>
<p>类似的修改使得 <span class="math notranslate nohighlight">\(\hat\Sigma\)</span> 向着标量协方差收缩，对于 <span class="math notranslate nohighlight">\(\gamma\in[0,1]\)</span>，有</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol\Sigma}(\gamma)=\gamma\hat{\boldsymbol\Sigma}+(1-\gamma)\hat{\sigma}^2\mathbf I.\tag{4.14}
\]</div>
<p>用 <span class="math notranslate nohighlight">\(\hat{\boldsymbol\Sigma}(\gamma)\)</span> 替换掉 式（ 4.13 ） 的 <span class="math notranslate nohighlight">\(\hat{\boldsymbol\Sigma}\)</span> 导出了一个更加一般的协方差阵族 <span class="math notranslate nohighlight">\(\hat{\boldsymbol\Sigma}(\alpha,\gamma)\)</span>，这是由一对参数来表示的。</p>
<p>在 12 章，我们将讨论 LDA 的另一种正则化版本，当数据来自数字化的相似的信号和图像时，这种方法会更加地合适。在这些情形下特征是高维的且相关的，LDA 系数可以通过正则化在原始信号域变得光滑和稀疏。这导出了更好的一般化并且对于这些系数有着简单的解释。在第 18 章中我们也处理非常高维的问题，举个例子如微阵列研究中基因表达测量的特征。那里方法集中在 式（ 4.14 ） 中 <span class="math notranslate nohighlight">\(\gamma=0\)</span> 的情形，以及其它 LDA 的正则化版本。</p>
</div>
<div class="section" id="lda">
<h2>（ 2 ）LDA 的计算<a class="headerlink" href="#lda" title="Permalink to this headline">¶</a></h2>
<p>作为下一主题的铺垫，我们简单地岔开去讨论 LDA，特别是 QDA 的计算。这些计算可以通过对角化 <span class="math notranslate nohighlight">\(\hat\Sigma\)</span> 或 <span class="math notranslate nohighlight">\(\hat\Sigma_k\)</span> 来简化。对于后者，假设我们对每一个计算特征值分解 <span class="math notranslate nohighlight">\(\hat\Sigma_k=\mathbf U_k\mathbf D_k\mathbf U_k^T\)</span>，其中 <span class="math notranslate nohighlight">\(\mathbf U_k\)</span> 是 <span class="math notranslate nohighlight">\(p\times p\)</span> 的正交矩阵，<span class="math notranslate nohighlight">\(\mathbf D_k\)</span> 是正的特征值 <span class="math notranslate nohighlight">\(d_{k\ell}\)</span> 组成的对角矩阵。则 <span class="math notranslate nohighlight">\(\delta_k(x)\)</span> 式（ 4.12 ） 的组成成分是</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((x-\hat\mu_k)^T\hat\Sigma_k^{-1}(x-\hat\mu_k)=[U_k^T(x-\hat\mu_k)]^T\mathbf D_k^{-1}[\mathbf U_k^T(x-\hat\mu_k)]\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\log\vert \hat\Sigma_k\vert=\sum_{\ell}\log d_{k\ell}\)</span></p></li>
</ul>
<p>按照上面列出的计算步骤，LDA 分类器可以通过下面的步骤来实现</p>
<ul class="simple">
<li><p>对数据关于协方差矩阵 <span class="math notranslate nohighlight">\(\hat{\boldsymbol\Sigma}\)</span> 球面化：<span class="math notranslate nohighlight">\(X^*\leftarrow \mathbf D^{-\frac{1}{2}}\mathbf U^T X\)</span>，其中 <span class="math notranslate nohighlight">\(\hat{\boldsymbol\Sigma}=\mathbf U\mathbf D\mathbf U^T\)</span>。<span class="math notranslate nohighlight">\(X^*\)</span> 的共同协方差矩阵变为单位阵。</p></li>
<li><p>考虑类别先验概率 <span class="math notranslate nohighlight">\(\pi_k\)</span> 的影响，在变换后的空间里面分到最近的类别形心。</p></li>
</ul>
</div>
<div class="section" id="id6">
<h2>（ 3 ）降维线性判别分析<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>至此我们已经讨论了限制为高斯分类器的 LDA。它受欢迎的部分原因是因为额外的限制使得我们可以看到数据在低维空间中富含信息的投影。</p>
<p>在 <span class="math notranslate nohighlight">\(p\)</span> 维输入空间的 <span class="math notranslate nohighlight">\(K\)</span> 个形心位于维数 <span class="math notranslate nohighlight">\(\le K-1\)</span> 的超平面子空间中，如果 <span class="math notranslate nohighlight">\(p\)</span> 比 <span class="math notranslate nohighlight">\(K\)</span> 大很多，维数上会有显著的降低。更多地，在确定最近的形心时，我们可以忽略到子空间的垂直距离，因为它们对每个类的作用同样大。因此我们可能仅仅需要将数据 <span class="math notranslate nohighlight">\(X^*\)</span> 投射到形心张成的子空间 <span class="math notranslate nohighlight">\(H_{K-1}\)</span> 中，并且在子空间内比较距离。因此在 LDA 中存在一个基础维数的降低，换句话说就是，我们仅仅需要考虑在维数至多为 <span class="math notranslate nohighlight">\(K-1\)</span> 的子空间的数据。如果 <span class="math notranslate nohighlight">\(K=3\)</span>，举个例子，这允许我们可以在二维图中观察数据，对类别进行颜色编码。这样做我们不会丢失任何 LDA 分类需要的信息。</p>
<blockquote>
<div><p><strong>注解：</strong>
更直观的解释如下图
<img alt="" src="../_images/note4.1.jpg" />
在考虑点 <span class="math notranslate nohighlight">\(A\)</span> 的最近形心时，不需要考虑 <span class="math notranslate nohighlight">\(A\)</span> 点垂直于 <span class="math notranslate nohighlight">\(H_2\)</span> 的距离，也就是直接在 <span class="math notranslate nohighlight">\(H_2\)</span> 子空间中比较点 <span class="math notranslate nohighlight">\(A\)</span> 在 <span class="math notranslate nohighlight">\(H_2\)</span> 中的投影点 <span class="math notranslate nohighlight">\(A'\)</span> 与三个待选形心的距离。</p>
</div></blockquote>
<p>如果 <span class="math notranslate nohighlight">\(K &gt; 3\)</span>？我们可能在某种意义下要求一个最优的 <span class="math notranslate nohighlight">\(L &lt; K-1\)</span> 维子空间 <span class="math notranslate nohighlight">\(H_L\subseteq H_{K-1}\)</span>。Fisher 定义的最优意思是投影形心关于方差要尽可能地分散。这意味着寻找形心的主成分空间（主成分在 <span class="xref myst">3.5.1 节</span>中有简短的描述，在 <span class="xref myst">14.5.1 节</span>中将详细讨论）。<span class="xref myst">图 4.4</span> 显示了对于元音数据的一个最优的二维子空间。这里是一个 <span class="math notranslate nohighlight">\(10\)</span> 维的输入空间， 其中有 <span class="math notranslate nohighlight">\(11\)</span> 个类别，每一类指不同的元音发音。形心在这种情形下要求全空间，因为 <span class="math notranslate nohighlight">\(K-1=p\)</span>，但是我们已经展示了一个最优的二维子空间。维数是有序的，所以我们可以依次计算额外的维数。图 4.8 显示了 <span class="math notranslate nohighlight">\(4\)</span> 个额外的坐标对，也被称作 <strong>典则 (canonical)</strong> 或者 <strong>判别 (discriminant)</strong> 变量。</p>
<p><img alt="" src="../_images/fig4.8.png" /></p>
<blockquote>
<div><p>图 4.8. 在成对典则变量上的四个投影。注意到当典则变量的秩增大，形心变得更不发散。右下角的图像看起来像是叠加上去的，类别也更加难以确定。</p>
</div></blockquote>
<p>总结一下，寻找 LDA 的最优子空间序列涉及以下步骤：</p>
<ul class="simple">
<li><p>计算 <span class="math notranslate nohighlight">\(K\times p\)</span> 的类别形心矩阵 <span class="math notranslate nohighlight">\(\mathbf M\)</span> 以及共同协方差矩阵 <span class="math notranslate nohighlight">\(\mathbf W\)</span>（<strong>组内 (within-class)</strong> 协方差）</p></li>
<li><p>使用 <span class="math notranslate nohighlight">\(\mathbf W\)</span> 特征值分解计算 <span class="math notranslate nohighlight">\(\mathbf M^*=\mathbf M\mathbf W^{-\frac{1}{2}}\)</span></p></li>
<li><p>计算 <span class="math notranslate nohighlight">\(\mathbf M^*\)</span> 的协方差矩阵 <span class="math notranslate nohighlight">\(\mathbf B^*\)</span>，（<span class="math notranslate nohighlight">\(\mathbf B\)</span> 是 <strong>组间 (between-class)</strong> 协方差），以及特征值分解 <span class="math notranslate nohighlight">\(\mathbf B^*=\mathbf V^*\mathbf D_B{\mathbf V^*}^T\)</span>。<span class="math notranslate nohighlight">\(\mathbf V^*\)</span> 的列 <span class="math notranslate nohighlight">\(v_\ell^*\)</span> 从第一个到最后一个依次定义了最优子空间的坐标。</p></li>
</ul>
<p>结合上述的操作，第 <span class="math notranslate nohighlight">\(\ell\)</span> 个 <strong>判别变量 (discriminant variable)</strong> 由 <span class="math notranslate nohighlight">\(Z_\ell=v_\ell^TX\)</span> 给出，其中 <span class="math notranslate nohighlight">\(v_\ell=W^{-\frac{1}{2}}v_\ell^*\)</span>。</p>
<blockquote>
<div><p>note “weiya 注：”</p>
<p>结合 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/142">Ex. 4.1</a> 的证明过程来理解上述算法。主要思想是先求 <span class="math notranslate nohighlight">\(\mathbf W^{-\frac 12}\mathbf B\mathbf W^{-\frac 12}\)</span> 的特征向量 <span class="math notranslate nohighlight">\(v_\ell^*\)</span>，则 <span class="math notranslate nohighlight">\(\mathbf W^{-1}\mathbf B\)</span> 的特征向量为 <span class="math notranslate nohighlight">\(\mathbf W^{-\frac 12}v_\ell^*\)</span>。同时注意到我们有 <span class="math notranslate nohighlight">\(\mathrm{Var}(v_\ell^TX)=1\)</span> 及 <span class="math notranslate nohighlight">\(\mathrm{Cov}(v_\ell^TX,v_k^TX)=0, \ell\neq k\)</span>，则判别函数 <span class="math notranslate nohighlight">\(\delta_k(z)\)</span> 中的平方和项为</p>
<div class="math notranslate nohighlight">
\[
(Z - \mu_k[Z])'(Z - \mu_k[Z]) = \sum_{\ell=1}^s(Z_\ell - \mu_{k}[Z_\ell])^2\,    
&gt;\]</div>
<p>其中 <span class="math notranslate nohighlight">\(s\)</span> 为判别变量的个数，<span class="math notranslate nohighlight">\(\mu_k[Z_\ell] = v_\ell^T\mu_k\)</span>.</p>
</div></blockquote>
<p>Fisher 通过不同的方式得到这个分解，完全没有引用高斯分布。他提出下面的问题：</p>
<blockquote>
<div><p>寻找线性组合 <span class="math notranslate nohighlight">\(Z=a^TX\)</span> 使得组间方差相对于组内方差最大化。</p>
</div></blockquote>
<p>再一次，组间方差是 <span class="math notranslate nohighlight">\(Z\)</span> 的均值的方差，而组内方差是关于均值的联合方差。图 4.9 显示了为什么这个准则是有意义的。尽管连接形心的方向能将均值尽可能分离开（比如，使得组间方差最大化），但是由于协方差的本性在投影类别上有很大重叠。同时考虑协方差，可以找到最小化重叠的方向。</p>
<p><img alt="" src="../_images/fig4.9.png" /></p>
<blockquote>
<div><p>图4.9. 尽管连接形心的直线定义了最大形心分散的方向，但是由于协方差（左图）投影数据会发生重叠。判别边界的方向使得高斯数据的重叠最小（右图）。</p>
</div></blockquote>
<p><span class="math notranslate nohighlight">\(Z\)</span> 的组间方差为 <span class="math notranslate nohighlight">\(a^T\mathbf Ba\)</span>，而组内方差为 <span class="math notranslate nohighlight">\(a^T\mathbf Wa\)</span>，<span class="math notranslate nohighlight">\(\mathbf W\)</span> 是很早定义的，<span class="math notranslate nohighlight">\(\mathbf B\)</span> 是类别形心矩阵 <span class="math notranslate nohighlight">\(\mathbf M\)</span> 的协方差矩阵。注意到 <span class="math notranslate nohighlight">\(\mathbf {B+W=T}\)</span>，其中 <span class="math notranslate nohighlight">\(\mathbf T\)</span> 是 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 的总协方差矩阵，忽略掉了类别信息。</p>
<blockquote>
<div><p>note “weiya 注：”</p>
<p>这里 <span class="math notranslate nohighlight">\(\mathbf B\)</span>，<span class="math notranslate nohighlight">\(\mathbf W\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf T\)</span> 均忽略掉了自由度。若有 <span class="math notranslate nohighlight">\(N\)</span> 个观测，<span class="math notranslate nohighlight">\(K\)</span> 个类别，且 <span class="math notranslate nohighlight">\(\bar x_i,\,\bar x,\,\bar x_{ij}\)</span> 均为 <span class="math notranslate nohighlight">\(p\)</span> 维向量，则有</p>
<p>$$</p>
</div></blockquote>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbf B &amp;= \sum\limits_{i=1}^Kn_i(\bar x_i-\bar x)(\bar x_i-\bar x)'\\
    \mathbf W &amp;= \sum\limits_{i=1}^K\sum\limits_{j=1}^{n_i}(x_{ij}-\bar x_i)(x_{ij}-\bar x_i)'\\
    \mathbf T &amp;= \sum\limits_{i=1}^K\sum\limits_{j=1}^{n_i}(x_{ij}-\bar x)(x_{ij}-\bar x)'
    \end{align*}\]</div>
<blockquote>
<div><p>$$</p>
<p>它们自由度分别为 <span class="math notranslate nohighlight">\(K-1\)</span>，<span class="math notranslate nohighlight">\(N-K\)</span> 和 <span class="math notranslate nohighlight">\(N-1\)</span>，易证 <span class="math notranslate nohighlight">\(\mathbf{T=B+W}\)</span>。</p>
</div></blockquote>
<p>Fisher 问题因此等价于最大化 Rayleigh quotient,</p>
<div class="math notranslate nohighlight">
\[
\underset{a}{\max}\;\dfrac{a^T\mathbf Ba}{a^T\mathbf Wa}\tag{4.15}
\]</div>
<p>或者等价地，</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\underset{a}{\max}\;a^T\mathbf Ba \; \\ \mathrm{\text{subject to}}\; a^T\mathbf Wa=1\tag{4.16}
\end{split}\]</div>
<p>这是一个一般化的特征值问题，<span class="math notranslate nohighlight">\(a\)</span> 是由 <span class="math notranslate nohighlight">\(\mathbf W^{-1}\mathbf B\)</span> 的最大特征值给出。不难证明（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/142">练习 4.1</a>）最优 <span class="math notranslate nohighlight">\(a_1\)</span> 等于上面定义的 <span class="math notranslate nohighlight">\(v_1\)</span>。类似地，可以找到下一个方向 <span class="math notranslate nohighlight">\(a_2\)</span>，在 <span class="math notranslate nohighlight">\(\mathbf W\)</span> 中与 <span class="math notranslate nohighlight">\(a_1\)</span> 正交，使得 <span class="math notranslate nohighlight">\(a_2^T{\mathbf B}a_2/a_2^T{\mathbf W}a_2\)</span> 最大化；解为 <span class="math notranslate nohighlight">\(a_2=v_2\)</span>，其余类似。<span class="math notranslate nohighlight">\(a_\ell\)</span> 被称作 <strong>判别坐标 (discriminant coordinates)</strong>，不要与判别函数相混淆。他们也被称作 <strong>典则变量 (canonical variables)</strong>，因为这些结果的一个变形是通过在预测变量矩阵 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 上对指示响应矩阵 <span class="math notranslate nohighlight">\(\mathbf Y\)</span> 进行典则相关分析得到的。这一点将在 <span class="xref myst">12.5 节</span>继续讨论。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 4.1”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/142">Issue 142: Ex. 4.1</a>。</p>
</div></blockquote>
<p>总结一下至今为止的发展：</p>
<ul class="simple">
<li><p>有相同的协方差矩阵的高斯分类导出线性判别边界。分类可以通过对数据关于 <span class="math notranslate nohighlight">\(\mathbf W\)</span> 球面化得到，并且划分到球空间的最近形心内（矫正因子 <span class="math notranslate nohighlight">\(\log\pi_k\)</span>）</p></li>
<li><p>因为只计算了到形心的相对距离，所以可以把数据局限于在球空间的形心张成的子空间。</p></li>
<li><p>子空间可以进一步分解为关于形心分离的最优子空间。这个分解与 Fisher 的分解相同。</p></li>
</ul>
<p>降维后的子空间可以看成是数据降维（为了观察）的一个工具。它们是否可以用于分类以及它的基本原理是什么？很明显它们可以用在分类中，正如我们在最初的变形中一样；我们可以简单地把到形心的距离计算限制到选定的子空间中。可以证明这是有着额外限制条件——高斯形心位于 <span class="math notranslate nohighlight">\(\mathbb{R}^p\)</span> 的 <span class="math notranslate nohighlight">\(L\)</span> 维子空间中——的高斯分类器。通过极大似然法拟合这样一个模型，然后运用贝叶斯定理构造后验概率，恰巧是上面描述的分类准则。（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/143">练习 4.8</a>）</p>
<p>高斯分类器在距离计算时要求矫正因子 <span class="math notranslate nohighlight">\(\log\pi_k\)</span>。使用矫正因子的理由可以在图 4.9 中看出。错分类率是基于两个密度计算的重叠部分的面积。如果 <span class="math notranslate nohighlight">\(\pi_k\)</span> 是相等的（图中隐含了），则最优的分离点是在投射均值之间。如果 <span class="math notranslate nohighlight">\(\pi_k\)</span> 不相等，朝着最小类别移动分类点会改善错误率。正如之前提到的两个类别，可以通过使用 LDA（或者其他任何方法）导出线性规则，然后选择分类点去最小化训练集上的误判率。</p>
<p>作为一个展现降维限制优点的例子，我们回到元音数据。这里有 11 个类别 10 个变量，因此该分类器有 10 个可能的维数。我们可以在每个层次空间计算训练和测试误差；图 4.10 显示了这个结果。图 4.11 显示了基于二维 LDA 的解的分类器的判别边界。</p>
<p><img alt="" src="../_images/fig4.10.png" /></p>
<blockquote>
<div><p>图 4.10. 对于元音数据，训练和测试误差作为判别边界的维数的函数的图象。这种情况下最优的误差率是维数等于 2 的情况。图 4.11 显示了这个空间的判别边界。</p>
</div></blockquote>
<p>Fisher <strong>降秩判别分析 (RDA)</strong> 和指示响应矩阵回归之间存在着紧密的联系。事实表明 LDA 意味着先回归然后对 <span class="math notranslate nohighlight">\(\hat{\mathbf  Y}^T\mathbf Y\)</span> 做特征值分解。在两个类的情形下，对于 <span class="math notranslate nohighlight">\(\hat{\mathbf Y}\)</span> 的任一列，都存在一个单判别变量，乘上一个标量后能与该列相等。这些联系将在<span class="xref myst">第 12 章</span>中讨论。一个相关的事实是先将原始的预测变量 <span class="math notranslate nohighlight">\(\mathbf X\)</span> 转换为 <span class="math notranslate nohighlight">\(\hat{\mathbf Y}\)</span>，然后用 <span class="math notranslate nohighlight">\(\hat{\mathbf Y}\)</span> 做 LDA 与在原空间中做 LDA 是相同的（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/144">练习 4.3</a>）。</p>
<blockquote>
<div><p>info “weiya 注：Ex. 4.3”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/144">Issue 144: Ex. 4.3</a>，欢迎讨论交流～</p>
</div></blockquote>
<p><img alt="" src="../_images/fig4.11.png" /></p>
<blockquote>
<div><p>图 4.11. 对于元音训练数据，由前两个典则变量张成的二维子空间中的判别边界。注意到在任何高维子空间下，判别边界是高维仿射平面，而且不可以表示成直线。</p>
</div></blockquote>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id7"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>Hastie, T., Tibshirani, R. and Buja, A. (1994). Flexible discriminant analysis by optimal scoring, Journal of the American Statistical Association 89: 1255–1270.</p>
</dd>
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Michie, D., Spiegelhalter, D. and Taylor, C. (eds) (1994). Machine Learning, Neural and Statistical Classification, Ellis Horwood Series in Artificial Intelligence, Ellis Horwood.</p>
</dd>
<dt class="label" id="id9"><span class="brackets"><a class="fn-backref" href="#id5">2</a></span></dt>
<dd><p>Friedman, J. (1989). Regularized discriminant analysis, Journal of the American Statistical Association 84: 165–175.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./04-Linear-Methods-for-Classification"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="4.2-Linear-Regression-of-an-Indicator-Matrix.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">4.2 指示矩阵的线性回归</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="4.4-Logistic-Regression.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">4.4 逻辑斯蒂回归</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>