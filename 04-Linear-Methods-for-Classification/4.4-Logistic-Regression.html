
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4.4 逻辑斯蒂回归 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.5 分离超平面" href="4.5-Separating-Hyperplanes.html" />
    <link rel="prev" title="4.3 线性判别分析" href="4.3-Linear-Discriminant-Analysis.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.3-Subset-Selection.html">
     3.3 选择预测变量的子集
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 平滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 平滑参数
     <span class="math notranslate nohighlight">
      \(\lambda\)
     </span>
     的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波平滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核平滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     维空间中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(p\)
     </span>
     维空间中的结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差、方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的乐观估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交互验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 “条件测试误差”还是“测试误差的期望”？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5
     <code class="docutils literal notranslate">
      <span class="pre">
       EM
      </span>
      <span class="pre">
       算法
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆叠
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14-Unsupervised-Learning/14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/04-Linear-Methods-for-Classification/4.4-Logistic-Regression.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   （ 1 ）拟合逻辑斯蒂回归模型
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   （ 2 ）例子：南非心脏病
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   （ 3 ）二次拟合和推断
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#l-1">
   （ 4 ）
   <span class="math notranslate nohighlight">
    \(L_1\)
   </span>
   正则化逻辑斯蒂回归
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lda">
   （ 5 ）逻辑斯蒂回归或者 LDA？
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>4.4 逻辑斯蒂回归</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   （ 1 ）拟合逻辑斯蒂回归模型
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id4">
   （ 2 ）例子：南非心脏病
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   （ 3 ）二次拟合和推断
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#l-1">
   （ 4 ）
   <span class="math notranslate nohighlight">
    \(L_1\)
   </span>
   正则化逻辑斯蒂回归
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lda">
   （ 5 ）逻辑斯蒂回归或者 LDA？
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>4.4 逻辑斯蒂回归<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>逻辑斯蒂回归来自用 <span class="math notranslate nohighlight">\(x\)</span> 的线性函数来建立 <span class="math notranslate nohighlight">\(K\)</span> 个类别后验概率模型的需要，同时保证后验概率的和为 1 且每一个都落在 <span class="math notranslate nohighlight">\([0,1]\)</span>。模型有如下形式</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\log\dfrac{\mathrm{Pr}(G=1\mid X=x)}{\mathrm{Pr}(G=K\mid X=x)}&amp;=\beta_{10}+\beta_1^Tx\\
\log\dfrac{\mathrm{Pr}(G=2\mid X=x)}{\mathrm{Pr}(G=K\mid X=x)}&amp;=\beta_{20}+\beta_2^Tx\\
&amp;\ldots\\
\log\dfrac{\mathrm{Pr}(G=K-1\mid X=x)}{\mathrm{Pr}(G=K\mid X=x)}&amp;=\beta_{(K-1)0}+\beta_{K-1}^Tx\\
\end{align*}
\tag{4.17}
\end{split}\]</div>
<p>模型由 <span class="math notranslate nohighlight">\(K-1\)</span> 个 log-odds  或 logit 变换来确定（反映了概率之和为 1 的约束）。虽然模型采用最后一类来作为 odds-ratios 的分母，但分母的选择其实是任意的，因为在这个选择下估计值是等价的。简单地计算可以得到</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathrm{Pr}(G=k\mid X=x)&amp;=\frac{\exp(\beta_{k0}+\beta_k^Tx)}{1+\sum_{\ell=1}^{K-1}\exp(\beta_{\ell0}+\beta_\ell^Tx)}\qquad k=1,\ldots, K-1\\
\mathrm{Pr}(G=K\mid X=x)&amp;=\frac{1}{1+\sum_{\ell=1}^{K-1}\exp(\beta_{\ell0}+\beta_\ell^Tx)}
\end{align*}
\tag{4.18}
\end{split}\]</div>
<p>显然它们相加等于 1。为了强调对参数集 <span class="math notranslate nohighlight">\(\theta=\\{\beta_{10},\beta_1^T,\ldots,\beta_{(K-1)0},\beta_{K-1}^T\\}\)</span> 的依赖，我们将概率记为 <span class="math notranslate nohighlight">\(p_k(x,\theta)\)</span>。</p>
<blockquote>
<div><p>note “weiya 注:”
与从 式（ 4.17 ） 中的两两比较出发得到 式（ 4.18 ） 式不同的是，PRML 中 4.3.4 节直接定义 <span class="math notranslate nohighlight">\(\mathrm{Pr}(G=k\mid X=x)\)</span>，</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Pr}(G=k\mid X=x) = \frac{\exp(\beta_k^Tx)}{\sum_\ell \exp(\beta_\ell^Tx)}\,,    
&gt;\]</div>
<p>这样会得到 <span class="math notranslate nohighlight">\(K\)</span> 个系数向量，比 式（ 4.18 ） 多一个。</p>
<p>对于多类别 <code class="docutils literal notranslate"><span class="pre">multi_class</span> <span class="pre">=</span> <span class="pre">'multinomial'</span></code>，<a class="reference external" href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"><code class="docutils literal notranslate"><span class="pre">sklearn.linear_model.LogisticRegression</span></code></a> 采用这种形式，因而其系数矩阵维度为 <span class="math notranslate nohighlight">\(K\times p\)</span>. 另外也可以采用 <code class="docutils literal notranslate"><span class="pre">One-vs-Rest</span></code> 的思路 <code class="docutils literal notranslate"><span class="pre">multi_class</span> <span class="pre">=</span> <span class="pre">'ovr'</span></code>，即 <span class="math notranslate nohighlight">\(K\)</span> 个二分类问题，依次将所考虑的类视为类别 1，而其余类为类别 0。</p>
</div></blockquote>
<p>当 <span class="math notranslate nohighlight">\(K=2\)</span> 时，模型非常简单，因为只有一个单线性函数。在生物统计应用中应用很广，因为经常会有二进制（两个类别）的响应变量。举个例子，病人获救或死亡，患心脏病和不患心脏病，或者某个条件存在与否。</p>
<div class="section" id="id2">
<h2>（ 1 ）拟合逻辑斯蒂回归模型<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>逻辑斯蒂回归经常通过极大似然法求解，采用在给定 <span class="math notranslate nohighlight">\(X\)</span> 时 <span class="math notranslate nohighlight">\(G\)</span> 的条件概率。因为 <span class="math notranslate nohighlight">\(\mathrm{Pr}(G\mid X)\)</span> 完全明确了条件分布，选择 <strong>多项式分布 (multinomial)</strong> 是合适的。</p>
<blockquote>
<div><p>note “weiya 注：多项式分布的合理性”
因为条件概率之和等于 1，而各个条件概率可以看成是互斥事件的概率，满足多项式分布的条件。</p>
</div></blockquote>
<p><span class="math notranslate nohighlight">\(N\)</span> 个观测的对数似然为</p>
<div class="math notranslate nohighlight">
\[
\ell(\theta)=\sum\limits_{i=1}^N\log\,p_{g_i}(x_i;\theta)\,,\tag{4.19}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(p_k(x_i;\theta)=\mathrm{Pr}(G=k\mid X=x_i;\theta)\)</span></p>
<p>我们将详细讨论两个类别的情形，因为此时算法可以非常简化。我们可以用 0/1 来对两个类别 <span class="math notranslate nohighlight">\(g_i\)</span> 的响应变量 <span class="math notranslate nohighlight">\(y_i\)</span> 进行编码，当 <span class="math notranslate nohighlight">\(g_i=1\)</span> 时 <span class="math notranslate nohighlight">\(y_i=1\)</span>，当 <span class="math notranslate nohighlight">\(g_i=2\)</span> 时 <span class="math notranslate nohighlight">\(y_i=0\)</span>。令 <span class="math notranslate nohighlight">\(p_1(x;\theta)=p(x,\theta)\)</span>,<span class="math notranslate nohighlight">\(p_2(x;\theta)=1-p(x;\theta)\)</span>，对数似然为</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{lll}
\ell(\beta)&amp;=&amp;\sum\limits_{i=1}^N\{y_i\log p(x_i;\beta)+(1-y_i)\log(1-p(x_i;\beta))\}\\
&amp;=&amp;\sum\limits_{i=1}^N\{y_i\beta^Tx_i-\log(1+e^{\beta^Tx_i})\}
\end{array}
\tag{4.20}
\end{split}\]</div>
<p>这里 <span class="math notranslate nohighlight">\(\beta=\\{\beta_{10},\beta_1\\}\)</span>，而且我们假设输入向量 <span class="math notranslate nohighlight">\(x_i\)</span> 包含表示截距的项 <span class="math notranslate nohighlight">\(1\)</span>。</p>
<p>为了最大化对数似然，令微分为 0，得到</p>
<div class="math notranslate nohighlight">
\[
\dfrac{\partial \ell(\beta)}{\partial \beta}=\sum\limits_{i=1}^Nx_i(y_i-p(x_i;\beta))=0,\tag{4.21}
\]</div>
<p>这是关于 <span class="math notranslate nohighlight">\(\beta\)</span> 的 <span class="math notranslate nohighlight">\(p+1\)</span> 个非线性等式。注意到因为 <span class="math notranslate nohighlight">\(x_i\)</span> 的第一个组分为 1，第一个组分满足的等式为 <span class="math notranslate nohighlight">\(\sum_{i=1}^Ny_i=\sum_{i=1}^Np(x_i;\beta)\)</span>；这表明类别一概率的期望值与观测值一致（也因此类别二概率的期望值与观测值一致）。</p>
<p>为了求解 式（ 4.21 ） 的得分等式，我们采用 Newton-Raphson 算法，需要 Hessian 矩阵</p>
<div class="math notranslate nohighlight">
\[
\dfrac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T}=-\sum\limits_{i=1}^Nx_ix_i^Tp(x_i;\beta)(1-p(x_i;\beta))\tag{4.22}
\]</div>
<p>以 <span class="math notranslate nohighlight">\(\beta^{old}\)</span> 开始，新的 Newton 更新为</p>
<div class="math notranslate nohighlight">
\[
\beta^{new}=\beta^{old}-(\dfrac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T})^{-1}\dfrac{\partial \ell(\beta)}{\partial(\beta)}\tag{4.23}
\]</div>
<p>其中微分值由 <span class="math notranslate nohighlight">\(\beta^{old}\)</span> 处的值得到。</p>
<p>把得分和 Hessian 写成矩阵形式是很方便的。记 <span class="math notranslate nohighlight">\(\mathbf y\)</span> 为 <span class="math notranslate nohighlight">\(y_i\)</span> 的值，<span class="math notranslate nohighlight">\(\mathbf X\)</span> 是 <span class="math notranslate nohighlight">\(x_i\)</span> 的 <span class="math notranslate nohighlight">\(N\times (p+1)\)</span> 矩阵，<span class="math notranslate nohighlight">\(\mathbf p\)</span> 是拟合概率的向量且第 <span class="math notranslate nohighlight">\(i\)</span> 个元素为 <span class="math notranslate nohighlight">\(p(x_i;\beta^{old})\)</span>，<span class="math notranslate nohighlight">\(\mathbf W\)</span> 是第 <span class="math notranslate nohighlight">\(i\)</span> 个对角元为 <span class="math notranslate nohighlight">\(p(x_i;\beta^{old})(1-p(x_i;\beta^{old}))\)</span> 的 <span class="math notranslate nohighlight">\(N\times N\)</span> 的对角矩阵。则我们有</p>
<div class="math notranslate nohighlight">
\[
\dfrac{\partial \ell(\beta)}{\partial \beta}=\mathbf{X^T(y-p)}\tag{4.24}
\]</div>
<div class="math notranslate nohighlight">
\[
\dfrac{\partial^2\ell(\beta)}{\partial\beta\partial\beta^T}=-\mathbf{X^TWX}\tag{4.25}
\]</div>
<p>牛顿迭代为</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{ll}
\beta^{new}&amp;=\beta^{old}+\mathbf{(X^TWX)^{-1}X^T(y-p)}\\
&amp;=\mathbf {(X^TWX)^{-1}X^TW(X\beta^{old}+W^{-1}(y-p))}\\
&amp;=\mathbf{(X^TWX)^{-1}X^TWz}\tag{4.26}
\end{array}
\end{split}\]</div>
<p>在第二和第三行我们已经重新把牛顿迭代表达成最小二乘迭代，响应变量为</p>
<div class="math notranslate nohighlight">
\[
\mathbf z=\mathbf X\beta^{old}+\mathbf{W^{-1}(y-p)}\tag{4.27}
\]</div>
<p>有时也被称作 <strong>调整后的响应变量 (adjusted response)</strong>。重复地进行求解这些方程，每一次迭代时，<span class="math notranslate nohighlight">\(\mathbf p\)</span> 改变，因此 <span class="math notranslate nohighlight">\(\mathbf W\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf z\)</span> 也改变。这个算法被称作 <strong>加权迭代最小二乘 (iteratively reweighted least squares)</strong> 或者 IRLS，因为每次迭代求解加权最小二乘问题：</p>
<div class="math notranslate nohighlight">
\[
\beta^{new}\leftarrow \arg\,\underset{\beta}{\min}(\mathbf z-\mathbf X\beta)^T\mathbf W(\mathbf z-\mathbf X\beta)\tag{4.28}
\]</div>
<blockquote>
<div><p>note “weiya注：IRLS”
<a class="reference external" href="https://github.com/xqwen/IRLS">Github: xqwen/IRLS</a>用C++实现IRLS。</p>
</div></blockquote>
<p>似乎<span class="math notranslate nohighlight">\(\beta=0\)</span>是迭代过程一个很好的初始值，尽管不会保证收敛性。一般地，算法确实是收敛的，因为对数似然是 <strong>凹的 (concave)</strong>，但是可能出现过收敛的情况。如果在罕见的情形下对数似然值下降，对步长进行折半会保证收敛性。</p>
<blockquote>
<div><p>note “weiya 注：”
首先对于凸函数 (convex function)，其局部最优点即为全局最优点，所以总是 (?) 可以收敛到最优点的，除非数值不稳定，比如矩阵的条件数过大。而凹函数取负号就变成了凸函数。
<strong>步长折半 (step size halving):</strong></p>
</div></blockquote>
<p>对于多类别的情况 <span class="math notranslate nohighlight">\(K\ge 3\)</span>，牛顿算法也可以表达成加权迭代最小二乘，只是对于每一个观测用一个 <span class="math notranslate nohighlight">\(K-1\)</span> 的响应向量和非对角系数矩阵。后者阻碍了算法的简化，在这种情况下直接使用展开向量 <span class="math notranslate nohighlight">\(\theta\)</span> 数值上会更加方便（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/211">练习 4.4</a>）。另一种是坐标下降方法（<a class="reference external" href="/03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms/index.html#pathwise-coordinate-optimization">3.8.6 节</a>）也可以有效地最大化对数似然。<code class="docutils literal notranslate"><span class="pre">R</span></code>语言的 <code class="docutils literal notranslate"><span class="pre">glmnet</span></code> 包 (Friedman et al., 2010<a class="footnote-reference brackets" href="#id12" id="id3">1</a>) 可以有效地拟合 <span class="math notranslate nohighlight">\(N\)</span> 和 <span class="math notranslate nohighlight">\(p\)</span> 都非常大时的逻辑斯蒂回归。尽管是为了拟合正则化模型设计的，但是也适用于非正则化的拟合。</p>
<p>逻辑斯蒂回归经常被用作一种数据分析和推断的工具，目标就是理解在解释输出时输入变量的角色。一般地，拟合时要寻找变量的**最简洁 (parsimonious)**的模型，很可能还有一些交互项。下面的例子说明了涉及到的一些问题。</p>
</div>
<div class="section" id="id4">
<h2>（ 2 ）例子：南非心脏病<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>这里我们展示一个用二值数据分析去说明逻辑斯蒂回归的传统统计学应用。图4.12中的数据取自CORIS调查的部分数据，这个调查在南非Western Cape的三个乡村进行(Rousseauw et al., 1983<a class="footnote-reference brackets" href="#id13" id="id5">2</a>)。调查的目标是建立在高发生率的地区中缺血性心脏病影响因子的强度模型。数据中有15到64岁的白人男性，响应变量是MI的存在或者缺失（该地区整个 MI 流行程度为5.1%）。在我们数据集中有160个案例，以及302个控制个体。数据在Hastie and Tibshirani(1987)<a class="footnote-reference brackets" href="#id14" id="id6">3</a>中有详细描述。</p>
<p><img alt="" src="../_images/fig4.12.png" /></p>
<blockquote>
<div><p>图4.12. 南非心脏病数据的散点图。每张图片是一对影响因子，案例集和控制集用不同颜色区分（红色的为案例）。家族心脏病史(famhist)是二值变量（是或否）</p>
</div></blockquote>
<p>我们通过极大似然法拟合了逻辑斯蒂回归模型，给出了如表 4.2 所示的结果。结果概要包含了模型中每一个系数的 <span class="math notranslate nohighlight">\(Z\)</span> 分数（系数除以他们的标准差）；不重要的 <span class="math notranslate nohighlight">\(Z\)</span> 分数表明该系数可以从模型中剔除。这对应着检验该系数为 0 而其它系数不为 0 的零假设（也被称作 Wald 检验）。<span class="math notranslate nohighlight">\(Z\)</span> 分数的绝对值大于 2 表明在 <span class="math notranslate nohighlight">\(5\%\)</span> 的水平下是显著的。</p>
<p><img alt="" src="../_images/tab4.2.png" /></p>
<blockquote>
<div><p>表4.2. 南非心脏病数据的逻辑斯蒂回归结果。</p>
</div></blockquote>
<p>在系数表中有一些比较奇怪的结果，这必须认真对待。收缩压 <code class="docutils literal notranslate"><span class="pre">sbp</span></code> 竟然不显著！肥胖 (<code class="docutils literal notranslate"><span class="pre">obesity</span></code>) 也不显著，它的符号是负的。这个混乱是因为预测变量之间的相关关系。单独地来看，<code class="docutils literal notranslate"><span class="pre">sbp</span></code> 和 <code class="docutils literal notranslate"><span class="pre">obesity</span></code> 都是显著的，而且都是正号。然而，当有其它相关变量时它们便不再需要了（甚至可以得到一个负号）。</p>
<p>这时分析者可能会做一些模型选择；寻找能够充分解释他们在 <code class="docutils literal notranslate"><span class="pre">chd</span></code> 上联合影响的子集变量。一种方式是删掉显著性最低的系数然后重新拟合模型。这个可以重复做下去直到没有更多的项可以从模型中剔除。这样得到了表 4.3 所示例的模型。</p>
<p><img alt="" src="../_images/tab4.3.png" /></p>
<blockquote>
<div><p>表4.3. 对南非心脏病数据逐步逻辑斯蒂回归的结果</p>
</div></blockquote>
<p>一个更好但是需要花费更多时间的策略是对每一个剔除一个变量后的模型进行重新拟合，然后进行 <strong>偏差分析 (analysis of deviance)</strong> 确定哪个变量需要剔除。拟合模型的**残偏差 (residual deviance)**是负的两倍的对数似然，两个模型的偏差是它们个体残偏差的差别（类似于平方和），这个策略给出了上面同样的最终结果。</p>
<p>举个例子，怎么解释 <code class="docutils literal notranslate"><span class="pre">tobacco</span></code> 的系数 0.081（标准偏差为 0.026）？<code class="docutils literal notranslate"><span class="pre">tobacco</span></code> 是整个时间段中使用的烟草总千克数，<strong>控制集 (controls)</strong> 中位数为 1.0kg，而这个<strong>案例集 (cases)</strong> 为 4.1kg。因此每增加 1kg 的烟草使用量意味着冠状心脏病的几率为 <span class="math notranslate nohighlight">\(\exp(0.081)=1.084\)</span>（或者 8.4%）。结合标准差我们得到 95%的置信区间 <span class="math notranslate nohighlight">\(\exp(0.081\pm 2\times 0.026)=(1.03,1.14)\)</span></p>
<p>我们将在第 5 章再次用到这些数据，我们将会看到一些变量会有非线性影响，当进行合适的建模后不会被剔除模型。</p>
</div>
<div class="section" id="id7">
<h2>（ 3 ）二次拟合和推断<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>极大似然法的参数估计 <span class="math notranslate nohighlight">\(\hat\beta\)</span> 满足 <strong>自一致性 (self-consistency)</strong> 关系：它们是加权最小二乘拟合的系数，其中响应变量为</p>
<div class="math notranslate nohighlight">
\[
z_i=x_i^T\hat\beta+\dfrac{y_i-\hat p_i}{\hat p_i(1-\hat p_i)}\tag{4.29}
\]</div>
<p>系数为 <span class="math notranslate nohighlight">\(w_i=\hat p_i(1-\hat p_i)\)</span>，都依赖 <span class="math notranslate nohighlight">\(\hat\beta\)</span> 自身。除了提供了一个方便的算法，这与最小二乘的联系还有其它用处：</p>
<ul class="simple">
<li><p>加权的残差平方和是熟悉的 Pearson 卡方统计量</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\sum\limits_{i=1}^N\dfrac{(y_i-\hat p_i)^2}{\hat p_i(1-\hat p_i)}\tag{4.30}
\]</div>
<p>这是对偏差的二次近似。</p>
<ul class="simple">
<li><p>极限概率理论表明如果模型是正确的，则 <span class="math notranslate nohighlight">\(\hat\beta\)</span> 为常数（即收缩到真实的 <span class="math notranslate nohighlight">\(\beta\)</span>）</p></li>
<li><p>中心极限定理证明 <span class="math notranslate nohighlight">\(\hat \beta\)</span> 的分布收敛到 <span class="math notranslate nohighlight">\(N(\beta,\mathbf{(X^TWX)^{-1}})\)</span>。这个可以模仿正态理论推断直接从加权最小二乘得到。</p></li>
<li><p>对于逻辑斯蒂回归建立模型计算量比较大，因为每个模型拟合需要迭代。受欢迎的方法是 Rao score 检验来检验是否包含某一项，以及 Wald 检验来检验是否剔除某一项。这些都不需要迭代拟合，而且都是基于当前模型的极大似然拟合。结果是两者都代表在加权最小二乘拟合中运用相同的系数加上或剔除某一项。这些计算可以高效地进行，不需要重新计算整个加权最小二乘拟合。</p></li>
</ul>
<p>软件实现可以利用这些联系。举个例子，R 中的广义线性模型（包括作为二项式模型族中的逻辑斯蒂回归）充分运用了他们。**GLM（广义线性模型）**对象可以当作线性模型对象来处理，而且所有对于线性模型可用的工具都可以自动应用起来。</p>
</div>
<div class="section" id="l-1">
<h2>（ 4 ）<span class="math notranslate nohighlight">\(L_1\)</span>正则化逻辑斯蒂回归<a class="headerlink" href="#l-1" title="Permalink to this headline">¶</a></h2>
<p>Lasso 中使用的 <span class="math notranslate nohighlight">\(L_1\)</span> 惩罚（<span class="xref myst">3.4.2节</span>）可以应用到任意线性回归模型的变量选择和收缩上面。对于逻辑斯蒂回归，我们最大化 式（ 4.20 ） 带惩罚项的版本：</p>
<div class="math notranslate nohighlight">
\[
\underset{\beta_0,\beta}{\max}\left\{\sum\limits_{i=1}^N[y_i(\beta_0+\beta^Tx_i)-\log(1+e^{\beta_0+\beta^Tx_i})]-\lambda\sum\limits_{j=1}^p\vert \beta_j\vert\right\}\tag{4.31}
\]</div>
<p>对于 lasso，我们一般不对截距项进行惩罚，对预测变量标准化后加惩罚才有意义。准则 式（ 4.31 ） 是凹的，而且运用非线性规划方法可以找到一个解（举个例子，Koh et al., 2007<a class="footnote-reference brackets" href="#id15" id="id8">4</a>）。另外，运用我们在 <span class="xref myst">4.4.1 节</span>运用的 Netwon 算法的二次逼近，我们可以通过重复应用加权的 lasso 算法求解 式（ 4.31 ）。有趣的是非零系数变量的得分等式（式（ 4.24 ））有如下形式</p>
<div class="math notranslate nohighlight">
\[
\mathbf {x_j^T(y-p)} = \lambda\cdot \mathrm{sign}(\beta_j)\tag{4.32}
\]</div>
<p>这一般化了 <a class="reference external" href="/03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods/index.html#mjx-eqn-3.58">3.4.4 节</a> 中的 式（ 3.58 ）；活跃变量再与残差的一般化相关性结合在一起。</p>
<blockquote>
<div><p>回顾</p>
<div class="math notranslate nohighlight">
\[
\mathbf x_j^T(\mathbf y-\mathbf X\beta)=\lambda\cdot \mathrm{sign}(\beta_j),\forall j\in {\mathcal B}\tag{3.58}    
&gt;\]</div>
</div></blockquote>
<p>路径算法（比如 lasso 的 LAR 算法）变得更加困难，因为系数曲线是逐段光滑而不是线性的。然而，可以通过二次逼近来实现。</p>
<p>图 4.13 显示了 <a class="reference external" href="4.4-Logistic-Regression/index.html#_2">4.4.2 节</a>南非心脏病数据的 <span class="math notranslate nohighlight">\(L_1\)</span> 正则化路径。这是通过 R 语言<code class="docutils literal notranslate"><span class="pre">glmpath</span></code>包 (Park and Hastie, 2007<a class="footnote-reference brackets" href="#id15" id="id9">4</a>) 实现的，运用了凸优化的 predictor-corrector 方法确定了 <span class="math notranslate nohighlight">\(\lambda\)</span> 在哪个非零活跃集值发生改变（图中的垂直线）。这里直线看似线性的，但在其它的例子中曲率将会更加明显。</p>
<p><img alt="" src="../_images/fig4.13.png" /></p>
<blockquote>
<div><p>图 4.13. 南非心脏病数据的 <span class="math notranslate nohighlight">\(L_1\)</span> 正则化逻辑斯蒂回归系数，画出了作为 <span class="math notranslate nohighlight">\(L_1\)</span> 范数函数的曲线。这些变量都标准化后得到单位方差。在图像每一个点处都精确算出来了。</p>
</div></blockquote>
<p>坐标下降方法（<span class="xref myst">3.8.6 节</span>）对于计算 <span class="math notranslate nohighlight">\(\lambda\)</span> 网格上的值的系数曲线是很有效的。<code class="docutils literal notranslate"><span class="pre">R</span></code> 语言 <code class="docutils literal notranslate"><span class="pre">glmnet</span></code> 包 (Friedman et al., 2010<a class="footnote-reference brackets" href="#id12" id="id10">1</a>) 对于非常大的逻辑斯蒂回归问题拟合系数路径是非常有效的（<span class="math notranslate nohighlight">\(N\)</span> 或 <span class="math notranslate nohighlight">\(p\)</span> 很大时）。他们的算法可以利用预测变量矩阵 <span class="math notranslate nohighlight">\(X\)</span> 的稀疏性，以至于对于大规模问题也适用。<span class="xref myst">18.4 节</span>有更详细的解释，而且将会讨论 <span class="math notranslate nohighlight">\(L_1\)</span> 正则多项分布模型。</p>
</div>
<div class="section" id="lda">
<h2>（ 5 ）逻辑斯蒂回归或者 LDA？<a class="headerlink" href="#lda" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p>info
基于本小节内容，回答了<a class="reference external" href="https://www.zhihu.com/question/61101057/answer/324078106">Logistic回归模型和判别分析之间的关系？ - 知乎</a>。</p>
</div></blockquote>
<p>在 <span class="xref myst">4.3 节</span>我们发现类别 <span class="math notranslate nohighlight">\(k\)</span> 和 <span class="math notranslate nohighlight">\(K\)</span> 类关于 <span class="math notranslate nohighlight">\(x\)</span> 线性函数的后验概率 odds 的对数 式（ 4.9 ）：</p>
<blockquote>
<div><p>回顾</p>
<p>$$</p>
</div></blockquote>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    &amp;\log\frac{\mathrm{Pr}(G=k\mid X=x)}{\mathrm{Pr}(G=\ell\mid X=x)}\\=&amp;\log\frac{f_k(x)}{f_\ell(x)}+\log\frac{\pi_k}{\pi_\ell}\\
    =&amp;\log\frac{\pi_k}{\pi_\ell}-\frac{1}{2}(\mu_k+\mu_\ell)^T\boldsymbol\Sigma^{-1}(\mu_k-\mu_\ell)+x^T\boldsymbol\Sigma^{-1}(\mu_k-\mu_\ell)
    \end{align*}\]</div>
<blockquote>
<div><p>$$</p>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{ll}
\log\dfrac{\mathrm{Pr}(G=k\mid X=x)}{\mathrm{Pr}(G=K\mid X=x)}&amp;=\log\dfrac{\pi_k}{\pi_K}-\frac{1}{2}(\mu_k+\mu_K)^T\Sigma^{-1}(\mu_k-\mu_K)+x^T\Sigma^{-1}(\mu_k-\mu_K)\\
&amp;=\alpha_{k0}+\alpha_k^Tx\qquad\tag{4.33}
\end{array}
\end{split}\]</div>
<p>线性是因为对类别的概率密度函数做了正态性假设及协方差矩阵相等的假设。通过构造线性逻辑斯蒂模型 式（ 4.17 ） 有线性 logits:</p>
<div class="math notranslate nohighlight">
\[
\log\frac{\mathrm{Pr}(G=k\mid X=x)}{\mathrm{Pr}(G=K\mid X=x)}=\beta_{k0}+\beta_k^Tx\tag{4.34}
\]</div>
<p>看起来模型似乎是一样的。尽管它们确实有相同的形式，区别在于它们系数估计的方式。逻辑斯蒂回归模型更加一般，因为它做了更少的假设。我们可以把 <span class="math notranslate nohighlight">\(X\)</span> 和 <span class="math notranslate nohighlight">\(G\)</span> 的联合概率密度写成</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Pr}(X,G=k)=\mathrm{Pr}(X)\mathrm{Pr}(G=k\mid X)\tag{4.35}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mathrm{Pr}(X)\)</span> 为 <span class="math notranslate nohighlight">\(X\)</span> 的边缘概率密度。对于 LDA 和逻辑斯蒂回归，右边的第二项有线性 logit 形式</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Pr}(G=k\mid X=x)=\dfrac{e^{\beta_{k0}+\beta_k^Tx}}{1+\sum_{\ell=1}^{K-1}e^{\beta_{\ell0}+\beta_\ell^Tx}}\tag{4.36}
\]</div>
<p>我们再一次任意地选择最后一类作为标准。</p>
<p>逻辑斯蒂回归模型中，<span class="math notranslate nohighlight">\(X\)</span> 的边缘概率密度可以取任意的边界密度函数 <span class="math notranslate nohighlight">\(\mathrm{Pr}(X)\)</span>，然后通过最大化条件概率（<span class="math notranslate nohighlight">\(\mathrm{Pr}(G=k\mid X)\)</span>的多项式概率）来拟合 <span class="math notranslate nohighlight">\(\mathrm{Pr}(G\mid X)\)</span> 的参数。尽管可以完全忽略掉 <span class="math notranslate nohighlight">\(\mathrm{Pr}(X)\)</span>，但我们可以把边缘密度看成在全非参和无约束情形下的估计，即采用在每个观测值上权重为 <span class="math notranslate nohighlight">\(1/N\)</span> 的经验分布。</p>
<p>进行 LDA 时，我们通过基于下面的联合密度来最大化对数似然进行拟合参数</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Pr}(X,G=k)=\phi(X;\mu_k,\Sigma)\pi_k\tag{4.37}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\phi\)</span> 是高斯密度函数。用标准正态的理论可以很容易估计 <span class="xref myst">4.3 节</span>的 <span class="math notranslate nohighlight">\(\hat\mu_k,\hat\Sigma\)</span> 和 <span class="math notranslate nohighlight">\(\hat\pi_k\)</span>。因为 式（ 4.33 ） 形式的逻辑斯蒂回归的线性参数是高斯分布参数的函数，我们通过插入对应的估计得到它们的极大似然估计。然而，与条件概率密度不同，边缘密度 <span class="math notranslate nohighlight">\(\mathrm{Pr}(X)\)</span> 起着重要作用。它是一个混合密度</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Pr}(X)=\sum\limits_{k=1}^K\pi_k\phi(X;\mu_k,\Sigma)\tag{4.38}
\]</div>
<p>同样涉及到参数。</p>
<p>这个额外的组分（约束条件）起着什么作用呢？通过依赖额外的模型假设，我们可以得到更多的参数信息，因此可以更加有效地估计它们（低方差）。事实上，如果 <span class="math notranslate nohighlight">\(f_k(x)\)</span> 的真实分布为高斯分布，则在最坏的情形下忽略密度的边缘项会在误差率上有近 <span class="math notranslate nohighlight">\(30\%\)</span> 的效率损失 (Efron, 1975<a class="footnote-reference brackets" href="#id16" id="id11">6</a>)。换句话说：当再有 <span class="math notranslate nohighlight">\(30\%\)</span> 的数据，条件概率才会达到一样的效果。</p>
<p>举个例子，远离判别边界的观测点（权重被逻辑斯蒂回归 <strong>低估 (down-weighted)</strong>）在估计斜方差阵中起着作用。这完全不是一个好消息，因为这意味着当有许多的边界点时，LDA 不够稳健。</p>
<p>从混合模型的组成来看，很显然没有类别标签的观测甚至都有参数的信息。产生类别标签经常是不划算的，但是产生未分类的观测值是容易的。通过依赖对模型强的假设，正如这里的一样，我们可以同时运用两种类型的信息。</p>
<p><strong>边缘似然 (marginal likelihood)</strong> 可以认为是一个正则器，从边缘角度来看在某种程度上类别密度是已知的。举个例子，如果在两个类别的逻辑斯蒂回归模型中，数据可以很好地被超平面分隔开，则参数的极大似然估计是没有定义的（比如说无穷，见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/110">练习 4.5</a>）。而对于同样的数据 LDA 的系数有很好的定义，因为边缘密度函数不会允许这些退化。</p>
<p>实际应用中这些假设从不正确，而且 <span class="math notranslate nohighlight">\(X\)</span> 的有些组分是定性变量。一般感觉逻辑斯蒂回归更加安全，比 LDA 模型更稳健，因为它依赖更少的假设。从我们经验来看，即便不恰当地运用 LDA，比如说对定性预测变量应用 LDA，两个模型都会得到非常相似的结果。</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id12"><span class="brackets">1</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id10">2</a>)</span></dt>
<dd><p>Friedman, J., Hastie, T. and Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent, Journal of Statistical Software 33(1): 1–22.</p>
</dd>
<dt class="label" id="id13"><span class="brackets"><a class="fn-backref" href="#id5">2</a></span></dt>
<dd><p>Rousseauw, J., du Plessis, J., Benade, A., Jordaan, P., Kotze, J., Jooste, P. and Ferreira, J. (1983). Coronary risk factor screening in three rural communities, South African Medical Journal 64: 430–436.</p>
</dd>
<dt class="label" id="id14"><span class="brackets"><a class="fn-backref" href="#id6">3</a></span></dt>
<dd><p>Hastie, T. and Tibshirani, R. (1987). Nonparametric logistic and proportional odds regression, Applied Statistics 36: 260–276.</p>
</dd>
<dt class="label" id="id15"><span class="brackets">4</span><span class="fn-backref">(<a href="#id8">1</a>,<a href="#id9">2</a>)</span></dt>
<dd><p>Park, M. Y. and Hastie, T. (2007). l1-regularization path algorithm for generalized linear models, Journal of the Royal Statistical Society Series B 69: 659–677.</p>
</dd>
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id11">6</a></span></dt>
<dd><p>Efron, B. (1975). The efficiency of logistic regression compared to normal discriminant analysis, Journal of the American Statistical Association 70: 892–898.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./04-Linear-Methods-for-Classification"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="4.3-Linear-Discriminant-Analysis.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">4.3 线性判别分析</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="4.5-Separating-Hyperplanes.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">4.5 分离超平面</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>