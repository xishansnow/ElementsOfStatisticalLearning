# 第六章 核平滑方法

<style>p{text-indent:2em;2}</style>

| 原文   | [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf) |
| --- | ---------------------------------------- |
| 翻译 | szcf-weiya                               |  
| 修订 | Xishansnow                               | 
| 发布 | 2016-09-30                               | 
| 更新 | 2022-01-04 16:37:39                      | 
| 状态 | Done                                     | 



这章中我们介绍一类回归技巧，这类技巧能够在每个查询点 $x_0$ 处，分别拟合不同但简单的模型，进而能够在 $p$ 维输入空间 $\mathbb{R}^p$ 中灵活地估计回归函数 $f(X)$ 。其实现方式是仅使用距离目标点很近的观测点来拟合该点处的简单模型，并且得到的估计函数 $\hat f(X)$ 在输入空间 $\mathbb{R}^p$ 上是平滑的。这种局部化可以通过一个加权函数或者`核函数` $K_\lambda(x_0,x_i)$ 实现，其权重通常根据 $x_i$ 到 $x_0$ 的距离计算得到。

核 $K_\lambda$ 一般地通过参数 $\lambda$ 来索引（ 即不同的参数 $\lambda$ 对应不同的核函数 ），参数 $\lambda$ 规定了邻域的宽度。原则上，这种基于记忆的方法需要很少甚至不需要训练；所有工作在评估阶段可以直接完成。根据训练集唯一需要确定的参数是 $\lambda$。然而，模型是整个训练数据集。

我们也会讨论一些比较通用的基于核的技术类，这些技术类与其他章节中的结构化方法有一定联系，并且在密度估计和分类中非常有用。

本章中的技巧不应该与最近使用很多的 **“核方法”** 混淆。本章中所指的核，大多作为一种局部化工具。我们会在本书第 [5.8 节](../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces/index.html)，[第 14.5.4 节](../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces/index.html)，[第 18.5 节](../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable/index.html)和[第 12 章](../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction/index.html)讨论核方法。在这些章节中，核的内积计算发生在高维（隐式的）特征空间中，而且核被用于带正则的非线性建模中。我们将在本章的 [第 6.7 节](../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels/index.html)的最后，把这些方法联系起来。
