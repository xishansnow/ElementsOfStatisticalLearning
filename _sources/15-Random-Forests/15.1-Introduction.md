# 15.1 导言

<style>p{text-indent:2em;2}</style>

[8.7 节](/08-Model-Inference-and-Averaging/8.7-Bagging/index.html)中的 `bagging` 或者 bootstrap aggregation 都是为了降低估计的预测函数的方差。Bagging 似乎对于高方差、低偏差的过程表现得特别好，比如树。对于回归， 我们简单地对训练数据的 bootstrap 版本进行多次同样的回归树，然后取结果的平均。对于分类，作为 `committee` 的每棵树对预测的类别进行投票。

!!! note "weiya 注"
    在 [8.8 节](/08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking/index.html)，用了这样一句话来介绍 Committee Method，“Committee methods take a simple unweighted average of the predictions from each model, essentially giving equal probability to each model.” 简单来说就是，对于一系列模型，committee 方法将这一系列模型得到的预测值进行简单平均。比如，对于分类而言，模型的预测值是类别标签，Committee 方法则是统计各个模型预测的类别标签（然后进行平均，如果只关注票数，是否平均并不重要），这就类似选举时委员会投票。

[第 10 章](/10-Boosting-and-Additive-Trees/10.1-Boosting-Methods/index.html)中的 boosting 一开始也是作为 `committee` 的方法提出来的，尽管不像 `bagging`，其中作为 `committee` 的 weak learners 随着时间不断进化，并且成员会投出带有权重的票。Boosting 似乎在大部分问题上表现最好，而且成为更好的选择。

**随机森林 (Random forests)** (Breiman, 2001[^1])是对 `bagging` 本质上的修改，而且建了一个 **去相关性(de-correlated)** 树的集合，然后进行平均。在许多问题上，随机森林的表现与 boosting 的表现很相似，而且可以更加简单地进行训练和调参。结果导致，随机森林很流行，而且可以通过各种包来实现。

[^1]: Breiman, L. (2001). Random forests, Machine Learning 45: 5–32.
