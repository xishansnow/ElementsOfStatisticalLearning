# 7.7 贝叶斯方法和 BIC
<style>p{text-indent:2em;2}</style>

和 AIC 一样，**贝叶斯信息准则（ BIC ）** 在由极大似然得到的拟合的设定中是可行的。BIC 一般形式为


$$
\mathrm{BIC} = -2\cdot \text{loglik} +(\log N)\cdot d\tag{7.35} 

$
BIC 准则（乘以 1/2）也被称作 Schwarz 准则 (Schwarz，1978[^1])。

高斯模型下，假设方差 $\sigma_\varepsilon^2$ 已知，$-2\cdot \text{loglik}$ 在忽略常数意义下等于 $\sum_i(y_i-\hat f(x_i))^2/(\sigma_\varepsilon^2)$，对于平方误差损失等于 $N\cdot\overline{\text{err}}/\sigma_\varepsilon^2$。因此我们可以写成：


$$
\mathrm{BIC} = \frac{N}{\sigma_\varepsilon^2}[\overline{\text{err}}+(\log N)\cdot\frac{d}{N}\sigma_\varepsilon^2] \tag{7.36}
$$

因此当用 2 替换 $\log N$ 后，BIC 与 AIC（$C_p$）成比例的。假设 $N > e^2\approx 7.4$，BIC 趋向于对复杂模型惩罚更重，偏向于选择更简单的模型。如同 AIC，$\sigma_\varepsilon^2$ 一般通过低偏差模型的均方误差来估计。

```{note}
注意此处假设 $\sigma_\varepsilon^2$ 已知，如果将其看成未知，并通过 MLE 求解，则 BIC 准则（忽略常数）为

$$
\mathrm{BIC} = N\log\mathrm{RSS} + d\log N\,,
$$

详见 [Wiki: BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion#Gaussian_special_case)
```

对于分类问题，选择交互熵作为误差衡量，使用多项对数似然会导出与 AIC 更相似的关系。注意到误分类误差率不会出现在 BIC  中，因为它并不对应于在任何概率模型下数据的对数似然。

尽管和 AIC 很相似，但 BIC 的来源 (motivation) 截然不同。它来源于使用贝叶斯方法来选择模型，我们现在进行讨论。

假设我们有一系列预选模型 ${\mathcal M_m},m=1,\ldots,M$，以及对应的模型参数 $\theta_m$，我们希望从中选择最优的模型。假设我们对每个模型 $\mathcal M_m$ 的参数有先验分布 $\mathrm{Pr}(\theta_m\mid\mathcal M_m)$，则模型的后验概率为：


$$
\begin{align*}
\mathrm{Pr}({\mathcal M}_m\mid \mathbf Z) &\varpropto \mathrm{Pr}({\mathcal M_m})\cdot \mathrm{Pr}(\mathbf Z\mid {\mathcal M_m})\qquad\tag{7.37} \\
&\varpropto \mathrm{Pr}({\mathcal M_m})\cdot \int \mathrm{Pr}(\mathbf Z\mid \theta_m,{\mathcal M_m})\mathrm{Pr}(\theta_m\mid\mathcal M_m)d\theta_m 
\end{align*}
$$

其中 $\mathbf Z$ 表示训练数据 $\\{x_i,y_i\\}\_1^N$。为了比较两个模型 $\mathcal M_m$ 和 $\mathcal M_\ell$，我们构造后验 odds


$$
\frac{\mathrm{Pr}({\mathcal M_m}\mid\mathbf Z)}{\mathrm{Pr}({\mathcal M_\ell\mid \mathbf Z})}=\frac{\mathrm{Pr}(\mathcal M_m)}{\mathrm{Pr}(\mathcal M_\ell)}\cdot\frac{\mathrm{Pr}(\mathbf Z\mid \mathcal M_m)}{\mathrm{Pr}(\mathbf Z\mid \mathcal M_\ell)}\tag{7.38}
$$

如果 odds 大于 1，则我们选择模型 $m$，否则我们选择模型 $\ell$。最右端的值：


$$
\mathrm{BF}(\mathbf Z)=\frac{\mathrm{Pr}(\mathbf Z\mid\mathcal M_m)}{\mathrm{Pr}(\mathbf Z\mid \mathcal M_\ell)}\tag{7.39}
$$

称为 **贝叶斯因子 (Bayes factor)**, 这是数据对于后验 odds 的贡献。

一般地我们假设模型的先验分布是均匀的，所以 $\mathrm{Pr}(\mathcal M_m)$ 为常值。我们需要其它的方式来估计 $\mathrm{Pr}(\mathbf Z\mid \mathcal M_m)$。在某些简化 (Ripley, 1996[^2]) 下，对式（7.37） 采用被称作对积分的 Laplace 近似得到


$$
\log \mathrm{Pr}(\mathbf Z\mid {\mathcal M_m})=\mathrm{log} \mathrm{Pr}(\mathbf Z\mid \hat \theta_m,{\mathcal M_m})-\frac{d_m}{2}\cdot \mathrm{log} N + O(1) \tag{7.40}
$$

这里 $\hat \theta_m$ 为极大似然估计，且 $d_m$ 为模型 $\mathcal M_m$ 自由参数的个数。如果我们定义损失函数为：


$$

-2\log \mathrm{Pr}(\mathbf Z\mid \hat\theta_m,\mathcal M_m)

$$

这等价于式（7.35） 的 BIC 准则。

因此，选择 BIC 最小的模型等价于选择有最大（近似）后验概率的模型。但是这个框架可以给我们更多的信息。如果我们对 $M$ 个元素的模型集合进行计算 BIC 准则，得到 $\mathrm{BIC}_m,m=1,2,\ldots,M$，则我们可以估计每个模型 $\mathcal M_m$ 的后验概率为：


$$
\frac{e^{-\frac{1}{2}\cdot \mathrm{BIC}_m}}{\sum_{\ell=1}^Me^{-\frac{1}{2}\cdot \mathrm{BIC}_\ell}}\tag{7.41}
$$

因此我们不仅可以估计最优的模型，还可以所考虑的模型的相对表现。

用于模型选择，AIC 和 BIC 之间没有明显的选择。BIC 作为选择准则是渐近一致的，这意味着给出模型族，其中包含真实模型，BIC 选择正确模型的概率当 $N\rightarrow \infty$ 时是 1。对于 AIC 这是不成立的，当 $N\rightarrow \infty$ 时，趋向于选择太复杂的模型。另一方面，在有限样本时，BIC 经常选择太过简单的模型，因为对模型复杂度的惩罚更大。

```{note}
BIC 是强相合的，而 AIC 不是。
**强相合估计：** 如果 $\hat\theta_N\; a.s.$ 收敛到 $\theta$，则称 $\hat\theta_N$ 是 $\theta$ 的强相合估计。
```

[^1]: Schwarz, G. (1978). Estimating the dimension of a model, Annals of Statistics 6(2): 461–464.
[^2]: Ripley, B. D. (1996). Pattern Recognition and Neural Networks, Cambridge University Press.
