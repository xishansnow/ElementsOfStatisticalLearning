# 2.7 结构化的回归模型

<style>p{text-indent:2em;2}</style>

尽管最近邻和其它局部的方法都是直接关注在某一点估计函数，但在高维都会遇到困难。甚至在低维中有很多结构化的方式能充分利用数据的情形下，这些方法也不合适。这一节介绍了结构化方式的类别。在我们讨论之前，我们更深入地讨论这些类别的需要。

## 问题的困难度

对任意函数 $f$,考虑 RSS 准则


$$
\begin{equation}
\mathrm{RSS}(f)=\sum\limits_{i=1}^{N}(y_i-f(x_i))^2
\tag{2.37}
\end{equation}
$$

有许多最小化式 式（ 2.37 ） 的方法：任何过所有训练点 $(x_i,y_i)$ 的函数 $\hat{f}$ 是一个解。任意选定的解都可能在与训练点不同的测试点上是一个糟糕的预测。如果在每个 $x_i$ 值处有多个观测对 $x_i,y_{i\ell},\ell =1,\ldots,N_i$，风险会被限制。在这种情形下，解会过每个 $x_i$ 对应的所有 $y_{i\ell}$ 的平均值，见[练习 2.6](https://github.com/szcf-weiya/ESL-CN/issues/161)。这种情形类似我们已经在统计判别理论章节中的已经讨论的；当然，式（ 2.37 ） 是式 (2.11) 的有限样本的版本。如果样本规模 $N$ 充分大使得重复是保证和密集排列的，这些解可能都倾向于条件期望的极限。

> info "weiya 注：Ex. 2.6"
    已解决，详见 [Ex. 2.6](https://github.com/szcf-weiya/ESL-CN/issues/161)。问题结论是，若存在重复数据，则普通最小二乘可以看成是 **（样本量）减少的加权最小二乘 (reduced weighted least squares)**。

为了得到有限 $N$ 的有用结果，我们必须限制满足条件的式 式（ 2.37 ） 的解到一个小的集合。怎样决定限制条件的本质是基于数据之外的一些考虑。这些限制条件有时通过 $f_0$ 的参数表示，或者在学习方法本身里面构建，有隐式、也有显式。这些带限制的解的类别是本书讨论的重点。一件事需要说清楚。任何加到 $f$ 上的限制条件会导致式 式（ 2.37 ） 的唯一解并不能消除解的多样性带来的不确定性。因为有无数种能导出唯一解的可能的限制条件，所以不确定性简单地被转换为限制条件的选择。

一般地，大多数学习方法施加的约束条件都可以描述为这样或那样对复杂度的限制。这通常也意味着在输入空间的小邻域的一些规则的行为。这就是，对于在某种度量下充分互相接近的所有输入点 $x$，$\hat{f}$ 展现了一些特殊的结构比如说接近常值，线性或者低次的多项式。然后通过在邻域中平均或者进行多项式拟合得到估计量。

限制的强度由邻域的大小所确定。邻域的规模越大，限制条件则越强，而且解对特定限制条件的选择也很敏感。举个例子，在无穷小邻域内局部常值拟合根本不是限制；在一个比较大的邻域内进行局部线性拟合几乎是全局线性模型，而且限制性是非常强的。

限制的本质取决于采用的度量。一些像核方法、局部回归和基于树的方法，直接明确了度量以及邻域的规模。至今为止讨论的最近邻方法是基于函数局部为常值的假设；当离目标输入 $x_0$ 越近，函数值就不会改变太多，因此可以对输出值进行平均得到的 $\hat{f}(x_0)$ 也会很接近。其它像样条、神经网络以及基于函数的方法隐性定义了邻域的局部行为。在 5.4.1 我们将要讨论 **等价核 (equivalent kernel)** 的概念（见图 5.8），描述了任意输出的线性方法的局部依赖。这些在很多情形下的等价核恰恰像上面讨论过的显式定义的系数核一样——在目标点达到巅峰然后从该点光滑下降。

现在必须澄清一个事实。任何在 **各向同性的 (isotropic)** 邻域中试图产生局部变化的函数会在高维中遇到问题——还是维数灾难。相反地，所有克服维数问题的方法在衡量邻域时有一个对应的度量（经常是隐式的或者自适应的），该度量的基本要求是不允许邻域在各个方向都同时小。
