
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9.1 广义可加模型 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.2 基于树的方法(CART)" href="9.2-Tree-Based-Methods.html" />
    <link rel="prev" title="第九章 加法模型、树及相关方法" href="9.0-Overview.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.3-Subset-Selection.html">
     3.3 子集的选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多重输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 光滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 光滑参数的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波光滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核光滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差、方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的乐观估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 “条件测试误差”还是“测试误差的期望”？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5 EM 算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆栈
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14-Unsupervised-Learning/14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   拟合可加模型
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   例子：可加逻辑斯蒂回归
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     例子：预测垃圾邮件
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id10">
   总结
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>9.1 广义可加模型<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>回归模型在许多数据分析中起着重要的作用，提供了预测和分类的规则、以及理解不同输入变量重要性的数据分析工具。</p>
<p>尽管非常简单，但是传统的线性模型经常在这些情形下失效：实际生活中，变量的影响往往不是线性的。在前面的章节中我们讨论使用预定义的基函数来实现非线性的技巧。这部分描述更多 <strong>自动灵活 (automatic flexible)</strong> 的统计方法来识别和表征非线性回归的影响。这些方法被称为“广义可加模型”。</p>
<p>在回归的设定中，广义可加模型有如下形式</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}(Y\mid X_1,X_2,\ldots,X_p) = \alpha+f_1(X_1)+f_2(X_2)+\cdots+f_p(X_p).\tag{9.1}
\]</div>
<p>和通常一样，<span class="math notranslate nohighlight">\(X_1,X_2,\ldots,X_p\)</span> 代表预测变量，<span class="math notranslate nohighlight">\(Y\)</span> 表示输出变量；<span class="math notranslate nohighlight">\(f_j\)</span> 是未定义的（非参）光滑函数。如果我们用基函数的展开（<span class="xref myst">第 5 章</span>）对每个函数进行建模，则最终的模型可以通过简单的最小二乘实现。我们这里的方式有些不同：我们用散点图光滑器来拟合每个函数（比如，三次光滑样条或核光滑），并且提供了同时估计所有 <span class="math notranslate nohighlight">\(p\)</span> 个函数的算法（<a class="reference external" href="#_1">9.1.1节</a>）</p>
<p>对于二分类，回忆在<span class="xref myst"> 4.4 节</span> 中讨论的二值数据的逻辑斯蒂回归模型。我们通过一个线性回归模型和 logit 链接函数将预测变量与二进制响应变量的均值 <span class="math notranslate nohighlight">\(\mu(X)=\mathrm{Pr}(Y=1\mid X)\)</span> 关联起来：</p>
<div class="math notranslate nohighlight">
\[
\log\left(\frac{\mu(X)}{1-\mu(X)}\right)=\alpha+\beta_1X_1+\cdots+\beta_pX_p\tag{9.2}
\]</div>
<p>可加逻辑斯蒂回归模型将每个线性项换成更一般的函数形式</p>
<div class="math notranslate nohighlight">
\[
\log\left(\frac{\mu(X)}{1-\mu(X)}\right)=\alpha+f_1(X_1)+f_2(X_2)+\cdots+f_p(X_p)\tag{9.3}
\]</div>
<p>其中每个 <span class="math notranslate nohighlight">\(f_j\)</span> 也是未定义的光滑函数。尽管函数 <span class="math notranslate nohighlight">\(f_j\)</span> 的非参形式使得模型更加灵活，但是仍然保留着可加性，而且允许我们用前面一样的方式来解读模型。可加逻辑斯蒂回归模型是广义加性模型的一个例子。一般地，响应变量 <span class="math notranslate nohighlight">\(Y\)</span> 的条件均值 <span class="math notranslate nohighlight">\(\mu(X)\)</span> 通过一个链接函数 <span class="math notranslate nohighlight">\(g\)</span> 将预测变量的可加函数联系起来：</p>
<div class="math notranslate nohighlight">
\[
g[\mu(X)]=\alpha+f_1(X_1)+\cdots+f_p(X_p)\tag{9.4}
\]</div>
<p>典型的链接函数的例子如下：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(g(\mu)=\mu\)</span> 是等值链接，用于高斯响应数据的线性和可加模型</p></li>
<li><p>如上面所示的对二项分布概率建模的 <span class="math notranslate nohighlight">\(g(\mu)=\mathrm{logit}(\mu)\)</span>，或者 <span class="math notranslate nohighlight">\(g(\mu)=\probit(\mu)\)</span> （probit 链接函数）。probit 函数是高斯分布函数的反函数：<span class="math notranslate nohighlight">\(\probit(\mu)=\Phi^{-1}(\mu)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(g(\mu)=\log(\mu)\)</span> 应用于处理泊松计数数据的对数线性或者对数加性模型。</p></li>
</ul>
<p>上面的三种情形都是来自指数族采样模型，另外也包括 Gamma 分布和负二项分布。这个分布族产生了著名的广义线性模型类，它们都是以同样的方式扩展为广义可加模型。</p>
<p>使用基本构造块 (basic building block) 为散点图平滑器 (scatterplot smoother) 的算法来灵活地估计函数 <span class="math notranslate nohighlight">\(f_j\)</span>。估计的函数 <span class="math notranslate nohighlight">\(\hat{f_j}\)</span> 然后可以揭示 <span class="math notranslate nohighlight">\(X_j\)</span> 可能的非线性影响。并不是所有的函数 <span class="math notranslate nohighlight">\(f_j\)</span> 需要是非线性。我们可以简单地将线性项和其它带非线性项的参数形式混合，当一些输入变量是定性的（或者说是因子）时这个混合是必要的。非线性项也没有约束为主要影响 (main effect)；可以存在关于两个或更多个变量的非线性成分，或者因子 <span class="math notranslate nohighlight">\(X_k\)</span> 的每个层次关于 <span class="math notranslate nohighlight">\(X_j\)</span> 的单一曲线。因此下列的每种情形都有可能：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(g(\mu)=X^T\beta+\alpha_k+f(Z)\)</span>，这是一个半参模型，其中 <span class="math notranslate nohighlight">\(X\)</span> 是需要线性建模的预测变量的向量，<span class="math notranslate nohighlight">\(\alpha_k\)</span> 是定性输入变量 <span class="math notranslate nohighlight">\(V\)</span> 的第 <span class="math notranslate nohighlight">\(k\)</span> 个层次的影响，而且非参数地建模预测变量 <span class="math notranslate nohighlight">\(Z\)</span> 的影响。</p></li>
<li><p><span class="math notranslate nohighlight">\(g(\mu)=f(X)+g_k(Z)\)</span>，同样 <span class="math notranslate nohighlight">\(k\)</span> 表示定性输入 <span class="math notranslate nohighlight">\(V\)</span> 的层次，因此这构造出一个关于 <span class="math notranslate nohighlight">\(V\)</span> 和 <span class="math notranslate nohighlight">\(Z\)</span> 影响的交叉项 <span class="math notranslate nohighlight">\(g(V,Z)=g_k(Z)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(g(\mu)=f(X)+g(Z,W)\)</span>，其中 <span class="math notranslate nohighlight">\(g\)</span> 是关于两个特征的非参数函数。</p></li>
</ul>
<p>可加模型可以用各种设置来替换线性模型，举个例子，时间序列的可加性分解：</p>
<div class="math notranslate nohighlight">
\[
Y_t=S_t+T_t+\varepsilon_t\tag{9.5}
\]</div>
<p>其中，<span class="math notranslate nohighlight">\(S_t\)</span> 是季节项，<span class="math notranslate nohighlight">\(T_t\)</span> 是趋势项，<span class="math notranslate nohighlight">\(\varepsilon_t\)</span> 是误差项。</p>
<div class="section" id="id2">
<h2>拟合可加模型<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>在这一节我们讨论拟合加性模型和它们的一般形式的标准算法。building block 是用一种灵活的方式拟合非线性影响的散点图光滑器。具体地，我们将第五章中讨论的三次光滑样条作为我们的散点图光滑器。</p>
<p>可加模型有如下形式</p>
<div class="math notranslate nohighlight">
\[
Y=\alpha + \sum_{j=1}^pf_j(X_j)+\varepsilon\tag{9.6}
\]</div>
<p>其中误差项 <span class="math notranslate nohighlight">\(\varepsilon\)</span> 均值为 <span class="math notranslate nohighlight">\(0\)</span>。给定观测值 <span class="math notranslate nohighlight">\(x_i,y_i\)</span>，类似 <span class="xref myst">5.4 节</span>中讨论的惩罚平方和 \eqref{5.9} 准则，可以确定出下面的准则，</p>
<p>!!! note “weiya 注: Recall”
$<span class="math notranslate nohighlight">\(
    \mathrm{RSS}(f,\lambda)=\sum\limits_{i=1}^N\{y_i-f(x_i)\}^2+\lambda\int \{f''(t)\}^2dt\tag{5.9}
    \)</span>$</p>
<div class="math notranslate nohighlight">
\[
\mathrm{PRSS}(\alpha,f_1,f_2,\ldots,f_p)=\sum\limits_{i=1}^N(y_i-\alpha-\sum\limits_{j=1}^pf_j(x_{ij}))^2+\sum\limits_{j=1}^p\lambda_j\int f_j''(t_j)^2dt_j\tag{9.7}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\lambda_j\ge 0\)</span> 是调整参数。可以证明 \eqref{9.7} 的最小值是可加三次样条模型；每个函数 <span class="math notranslate nohighlight">\(f_j\)</span> 是一个关于组分 <span class="math notranslate nohighlight">\(X_j\)</span> 的三次样条，结点在每个唯一的 <span class="math notranslate nohighlight">\(x_{ij},i=1,\ldots,N\)</span> 值处。然而，如果没有对模型有进一步的约束，解不是唯一的。常数 <span class="math notranslate nohighlight">\(\alpha\)</span> 不是唯一的，因为我们可以对每个函数 <span class="math notranslate nohighlight">\(f_j\)</span> 进行加减任意常数，然后因此调整 <span class="math notranslate nohighlight">\(\alpha\)</span>。标准的规定是假设 <span class="math notranslate nohighlight">\(\sum_1^Nf_j(x_{ij})=0,\forall j\)</span> ——这些函数在数据上均值为 0。可以很简单地证明得到在这种情形下 <span class="math notranslate nohighlight">\(\hat\alpha=\mathrm {ave}(y_i)\)</span>。除了这条约束，再加上输入矩阵（第 <span class="math notranslate nohighlight">\(ij\)</span> 位置值为 <span class="math notranslate nohighlight">\(x_{ij}\)</span>）是列满秩的条件，则 \eqref{9.7} 是严格凸准则并且最小值是唯一的。如果矩阵是奇异的，则不能唯一确定组分 <span class="math notranslate nohighlight">\(f_j\)</span> 的线性部分（尽管非线性部分可以！）(Buja et al., 1989<a class="footnote-reference brackets" href="#id14" id="id3">1</a>)</p>
<p>进一步，有一个简单的迭代过程来寻找解。令 <span class="math notranslate nohighlight">\(\hat\alpha=\mathrm {ave}(y_i)\)</span>，而且保持不变。我们对关于 <span class="math notranslate nohighlight">\(x_{ij}\)</span> 函数的目标<span class="math notranslate nohighlight">\(\\{y_i-\hat\alpha-\sum_{k\neq j}\hat f_k(x_{ik})\\}\_1^N\)</span> 应用三次光滑样条 <span class="math notranslate nohighlight">\(\mathcal S_j\)</span> 得到新估计 <span class="math notranslate nohighlight">\(\hat{f_j}\)</span>。对每个预测变量轮流这样处理，当计算 <span class="math notranslate nohighlight">\(y_i-\hat\alpha-\sum_{k\neq j}\hat f_k(x_{ik})\)</span> 时使用其它函数的当前估计 <span class="math notranslate nohighlight">\(\hat f_k\)</span>。重复这个过程直到估计值 <span class="math notranslate nohighlight">\(\hat{f_j}\)</span> 稳定。这个过程被称作 “backfitting”（算法 9.1 中给出了详细细节），而且最后的拟合与线性模型的多项式回归是相似的。</p>
<p><img alt="" src="../_images/alg9.1.png" /></p>
<p>原则上，算法 9.1 中第 2(2) 步不是必需的，因为光滑样条对 0 均值响应变量拟合均值为 0（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/167">练习 9.1</a>）。实际中，机器舍入误差会导致下降，所以建议进行调整。</p>
<p>当确定了合适的光滑算子 <span class="math notranslate nohighlight">\(\mathcal S_j\)</span>，同样的算法可以用完全相同的方式适应其他的拟合方法：</p>
<ul class="simple">
<li><p>其它的单变量回归光滑器，比如局部多项式回归和核方法；</p></li>
<li><p>线性回归算子，可以得到多项式拟合，分段常值拟合，参数化样条拟合，序列和傅里叶拟合；</p></li>
<li><p>更加复杂的算子，比如对于二阶或者高阶交互项或者对于季节影响的周期光滑器。</p></li>
</ul>
<p>如果我们仅仅在训练集点上考虑光滑器算子 <span class="math notranslate nohighlight">\(\mathcal S_j\)</span>，则可以用 <span class="math notranslate nohighlight">\(N\times N\)</span> 的算子矩阵 <span class="math notranslate nohighlight">\(\mathbf S_j\)</span> 来表示（见<span class="xref myst"> 5.4.1 节</span>）。则第 <span class="math notranslate nohighlight">\(j\)</span> 项的自由度（近似）是 <span class="math notranslate nohighlight">\(\mathrm {df}_j=\mathrm{trace}[\mathbf S_j]-1\)</span>，这与第 5 和第 6 章中讨论的光滑器的自由度是类似的。</p>
<p>对于一个大类别的线性光滑器 <span class="math notranslate nohighlight">\(\mathbf S_j\)</span>，backfitting 等价于求解确定线性系统的 Gauss-Seidel 算法。细节在<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/68">练习 9.2</a>中给出。</p>
<p>对于逻辑斯蒂回归和其他广义加性模型，合适的准则是带惩罚的对数似然。为了最大化它，backfitting 过程与概率最大化一起作用。在广义线性模型中最大化对数似然的 Newton-Raphson 方法可以重新改造成一个 IRLS（迭代重加权最小二乘）算法。这涉及在协变量上对工作中的响应变量重复进行加权线性回归；每个回归得到一个新的参数估计，反过来又给了新的工作相应变量和系数，而且这一过程迭代进行（见 <span class="xref myst">4.4.1 节</span>）。在广义加性模型中，加权线性回归可以简单地用加权 backfitting 算法来替换。我们将在下面讨论逻辑斯蒂回归时该算法的细节，更一般的讨论可以在 Hastie and Tibshirani(1990)<a class="footnote-reference brackets" href="#id15" id="id4">2</a>的第 6 章中找到。</p>
</div>
<div class="section" id="id5">
<h2>例子：可加逻辑斯蒂回归<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>在医学研究中应用最广的模型或许是对二值数据的逻辑斯蒂回归模型。在这个模型中，输出 <span class="math notranslate nohighlight">\(Y\)</span> 可以编码成 0 或 1，其中 1 代表事件发生（比如死亡，疾病复发），0 代表没有事件发生。我们希望建立 <span class="math notranslate nohighlight">\(\mathrm{Pr}(Y=1\mid X)\)</span> 的模型，其中 <span class="math notranslate nohighlight">\(\mathrm{Pr}(Y=1\mid X)\)</span> 是给定预测因子 <span class="math notranslate nohighlight">\(X^T=(X_1,\ldots,X_p)\)</span> 后事件发生的概率。目标通常是确定预测因子的作用，而不是对新个体进行分类。逻辑斯蒂回归也应用到当对估计类别比例感兴趣的情况下，比如 risk screening。除了医学上的应用，信用风险 screening 也是一个受欢迎的应用方面。</p>
<p>广义加性模型有如下形式
$<span class="math notranslate nohighlight">\(
\log\frac{\mathrm{Pr}(Y=1\mid X)}{\mathrm{Pr}(Y=0\mid X)}=\alpha+f_1(X_1)+\cdots+f_p(X_p)\tag{9.8}
\)</span>$</p>
<p>函数 <span class="math notranslate nohighlight">\(f_1,f_2,\ldots,f_p\)</span> 是通过在 Newton-Raphson 过程中运用 backfitting 算法估计得到的，算法 9.2 中显示了具体过程。</p>
<p><img alt="" src="../_images/alg9.2.png" /></p>
<p>可加模型在算法 9.2 中的第 (2) 步拟合需要一个带权重的散点图光滑器。许多光滑过程可以接受观测值权重（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/107">练习 5.12</a>）；更多细节参见 Hastie and Tibshirani (1990)<a class="footnote-reference brackets" href="#id15" id="id6">2</a> 的第三章。</p>
<p>可加逻辑斯蒂回归模型可以进一步推广为解决超过两个类别的情形，通过采用在 <span class="xref myst">4.4 节</span>中列出的多维逻辑斯蒂公式。尽管这个形式是 \eqref{9.8} 式的直接推广，但拟合这些模型的算法更加复杂。更多细节参见 Yee and Wild(1996)<a class="footnote-reference brackets" href="#id16" id="id7">3</a>，并且 VGAM 软件可以从下面网站中得到</p>
<p><a class="reference external" href="http://www.stat.auckland.ac.nz/~yee">http://www.stat.auckland.ac.nz/∼yee</a></p>
<div class="section" id="id8">
<h3>例子：预测垃圾邮件<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>我们对第一章的 spam 数据应用广义可加模型。数据包含 4601 条电子邮件信息，在一项研究中去扫描邮件来找出 “spam”（也就是，垃圾邮件）。数据可以在<span class="xref myst">ftp.ics.uci.edu</span> 上获得，而且收到了 California 的 Palo Alto 的 Hewlett-Packard 实验室中 George Forman 的捐赠。</p>
<p>响应变量是二值数据，值为 email 或者 spam，在下面描述了 57 个预测变量。</p>
<ul class="simple">
<li><p>48 个定量预测变量——邮件中匹配给定单词的单词比例。举个例子，比如说有 <code class="docutils literal notranslate"><span class="pre">business</span></code>，<code class="docutils literal notranslate"><span class="pre">address</span></code>，<code class="docutils literal notranslate"><span class="pre">internet</span></code>，<code class="docutils literal notranslate"><span class="pre">free</span></code> 和 <code class="docutils literal notranslate"><span class="pre">george</span></code>。这个想法是这些对不同用户会不一样。</p></li>
<li><p>6 个定量预测变量——邮件中匹配给定字符的字符比例。字符为 <code class="docutils literal notranslate"><span class="pre">ch;</span></code>，<code class="docutils literal notranslate"><span class="pre">ch(</span></code>，<code class="docutils literal notranslate"><span class="pre">ch[</span></code>，<code class="docutils literal notranslate"><span class="pre">ch!</span></code>，<code class="docutils literal notranslate"><span class="pre">ch$</span></code>，和 <code class="docutils literal notranslate"><span class="pre">ch#</span></code>。</p></li>
<li><p>未间断的大写字符序列的平均长度：<code class="docutils literal notranslate"><span class="pre">CAPAVE</span></code></p></li>
<li><p>未间断的大写字符序列的最长长度：<code class="docutils literal notranslate"><span class="pre">CAPMAX</span></code></p></li>
<li><p>未间断的大写字符序列的总长度：<code class="docutils literal notranslate"><span class="pre">CAPTOT</span></code></p></li>
</ul>
<p>我们将 spam 编码成 1，email 编码成 0。随机选取大小为 1536 的测试集，剩下的 3065 个测试集作为测试集。一个广义加性模型通过对每个预测变量用四个名义自由度的三次光滑样条。这意味着对于每个预测变量 <span class="math notranslate nohighlight">\(X_j\)</span>，选择光滑样条的参数 <span class="math notranslate nohighlight">\(\lambda_j\)</span> 使得 <span class="math notranslate nohighlight">\(\mathrm {trace}[\mathbf S_j(\lambda_j)]-1=4\)</span>，其中 <span class="math notranslate nohighlight">\(\mathbf S_j(\lambda)\)</span> 是使用观测数据 <span class="math notranslate nohighlight">\(x_{ij},i=1,\ldots,N\)</span> 构造出的光滑样条算子矩阵。这是在这样一个复杂模型中确定光滑数量的方便方式。</p>
<p>大多数的 spam 预测变量服从一个非常长的长尾分布。在拟合 GAM 模型之前，对每个变量进行对数变换（实际上是 <span class="math notranslate nohighlight">\(\log(x+0.1)\)</span>），但是在图 9.1 画出的是关于原始变量的函数。</p>
<p><img alt="" src="../_images/fig9.1.png" /></p>
<blockquote>
<div><p>图 9.1. 垃圾邮件分析：对于显著性预测变量的估计函数。在每张图的横轴的须状图 (rug plot) 表示对应的预测变量。对于许多预测变量非线性是在 0 处的不连续性导致的。</p>
</div></blockquote>
<p>表 9.1 显示了测试误差率；总体误差率为 <span class="math notranslate nohighlight">\(5.3\%\)</span>。为了对比，线性逻辑斯蒂回归有 <span class="math notranslate nohighlight">\(7.6\%\)</span> 的测试误差。表 9.2 显示了在可加性模型中显著性强的预测变量。</p>
<p><img alt="" src="../_images/tab9.1.png" /></p>
<blockquote>
<div><p>表 9.1. 对垃圾训练数据的可加逻辑斯蒂回归模型拟合的混淆矩阵。整体测试误差率为 <span class="math notranslate nohighlight">\(5.5\%\)</span>.</p>
</div></blockquote>
<p><img alt="" src="../_images/tab9.2.png" /></p>
<blockquote>
<div><p>表 9.2. 对 spam 训练数据进行可加模型拟合的显著性预测变量。系数和它们的标准误差以及 Z 得分来表示 <span class="math notranslate nohighlight">\(\hat{f_j}\)</span> 的线性部分。非线性 p 值是对<span class="math notranslate nohighlight">\(\hat{f_j}\)</span> 非线性的检验。</p>
</div></blockquote>
<p>为了解释的方便，表 9.2 中有每个变量分解成线性组分和剩余非线性组分的贡献。靠前的预测变量与 spam 正相关，而最后的预测变量负相关。线性组分是在预测变量上对拟合曲线的加权最小二乘线性拟合，而非线性部分是剩余部分。估计函数的线性部分是由系数、标准误差和 <span class="math notranslate nohighlight">\(Z\)</span> 得分来总结（表示）；后者（指 Z 得分）是系数除以它的标准误差，而且当它超过合适的标准正态分布的分位数认为是显著的。标签为 nonlinear P-value 的列是对估计函数非线性部分的假设检验。然而，注意到，每个预测变量的影响是根据其他变量的整体影响来调整的，而不仅仅是它们的线性部分。表中显示的预测变量通过在 <span class="math notranslate nohighlight">\(p=0.01\)</span> 水平下（双边）进行至少一个检验（线性或者非线性）来判断是否显著的。</p>
<p>图 9.1 显示了在表 9.2 中出现的显著预测变量的估计函数。许多非线性影响表明在 0 处的强不连续性。举个例子，当 <code class="docutils literal notranslate"><span class="pre">george</span></code> 的频率从 <span class="math notranslate nohighlight">\(0\)</span> 增长， <code class="docutils literal notranslate"><span class="pre">spam</span></code> 的概率显著下降，但是此后没有太大改变。这表明或许会用一个表示计数为 0 的指示变量来替换每个频率预测变量，这样回到线性逻辑斯蒂回归。这给出 <span class="math notranslate nohighlight">\(7.4\%\)</span> 的测试误差；包括频率的线性影响将测试误差降低到 <span class="math notranslate nohighlight">\(6.6\%\)</span>。看起来在可加模型中的非线性有着额外的预测力量。</p>
<p>将真正的 <code class="docutils literal notranslate"><span class="pre">email</span></code> 分类成 <code class="docutils literal notranslate"><span class="pre">spam</span></code> 是很严重的，因为好邮件会被过滤掉而不能到达用户手中。我们可以通过改变损失来平衡类别误差率（见 2.4 节）。如果我们对将真实的类别 0 预测成类别 1 赋予损失 <span class="math notranslate nohighlight">\(L_{01}\)</span>，<span class="math notranslate nohighlight">\(L_{10}\)</span> 为将真实的类别 1 预测成类别 0 的损失，于是如果概率大于 <span class="math notranslate nohighlight">\(L_{01}/(L_{01}+L_{10})\)</span> 则估计的贝叶斯准则预测为类别 1。举个例子，如果我们取 <span class="math notranslate nohighlight">\(L_{01}=10,L_{10}=1\)</span> 则（真实的）类别 0 和类别 1 误差率变成 <span class="math notranslate nohighlight">\(0.8\%\)</span> 和 <span class="math notranslate nohighlight">\(8.7\%\)</span>。</p>
<p>更 ambitiously，我们通过对类别 0 的观测赋予权重 <span class="math notranslate nohighlight">\(L_{01}\)</span>，对类别 1 的观测赋予权重 <span class="math notranslate nohighlight">\(L_{10}\)</span> 使得模型更好地对类别 0 中的数据进行拟合。像上面一样，我们接着运用贝叶斯准则来预测。在类别 0 和类别 1 中误差率分别为 <span class="math notranslate nohighlight">\(1.2\%\)</span> 和 <span class="math notranslate nohighlight">\(8.0\%\)</span>。我们将在基于树的模型中讨论损失不相等的问题。</p>
<p>拟合加性模型之后，应该检查一下加入交互关系之后是否显著性提高拟合。这可以通过插入一些或全部的显著性输入来“手动”完成，或者通过 MARS 过程来自动完成。（见<span class="xref myst"> 9.4 节</span>）</p>
<p>这个例子以一种自动的方式来使用加性模型。作为一个数据分析工具，可加模型经常以一种更交互的方式来使用，通过添加或者删除项来确定它们的影响。通过校准关于 <span class="math notranslate nohighlight">\(df_j\)</span> 的光滑数量，可以在线性模型 (<span class="math notranslate nohighlight">\(df_j=1\)</span>) 和部分线性模型中移动，其中对一些项更加灵活地建模。更多细节见 Hastie and Tibshirani(1990)<a class="footnote-reference brackets" href="#id15" id="id9">2</a>。</p>
</div>
</div>
<div class="section" id="id10">
<h2>总结<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<p>可加模型提供一个有用的线性模型拓展，使得不仅保留着大部分的可解释性，也更加灵活。在线性模型中建模和推断熟悉的工具对于加性模型也是适用的，见表 9.2 的例子。拟合这些模型的 backfitting 过程是简单而有模式的，允许对于每个输入变量去选择合适的拟合方法。在统计领域它们越来越被广泛地应用。</p>
<p>然而可加模型对于大的数据挖掘的应用是有限制的。backfitting 算法拟合所有的预测变量，而大部分的预测变量是不可行的也不是必需的。BRUTO 过程（Hastie and Tibshirani, 1990<a class="footnote-reference brackets" href="#id15" id="id11">2</a>，第 9 章）结合 backfitting 和输入选择，但是不是为了大规模的数据挖掘的问题。最近也有用 lasso 型惩罚来估计稀疏加性模型的工作，举个例子，Lin and Zhang (2006)<a class="footnote-reference brackets" href="#id17" id="id12">4</a> 提出的 COSSO 过程，以及 Ravikumar et al., (2008)<a class="footnote-reference brackets" href="#id18" id="id13">5</a> 提出的 SpAM。对于大型问题，向前逐步方法（比如第 10 章的 boosting）更加地有效，而且允许交叉项包含进模型中。</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id14"><span class="brackets"><a class="fn-backref" href="#id3">1</a></span></dt>
<dd><p>Buja, A., Hastie, T. and Tibshirani, R. (1989). Linear smoothers and additive models (with discussion), Annals of Statistics 17: 453–555</p>
</dd>
<dt class="label" id="id15"><span class="brackets">2</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id6">2</a>,<a href="#id9">3</a>,<a href="#id11">4</a>)</span></dt>
<dd><p>Hastie, T. and Tibshirani, R. (1990). Generalized Additive Models, Chapman and Hall, London.</p>
</dd>
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id7">3</a></span></dt>
<dd><p>Yee, T. and Wild, C. (1996). Vector generalized additive models, Journal of the Royal Statistical Society, Series B. 58: 481–493.</p>
</dd>
<dt class="label" id="id17"><span class="brackets"><a class="fn-backref" href="#id12">4</a></span></dt>
<dd><p>Lin, Y. and Zhang, H. (2006). Component selection and smoothing in smoothing spline analysis of variance models, Annals of Statistics 34: 2272–2297.</p>
</dd>
<dt class="label" id="id18"><span class="brackets"><a class="fn-backref" href="#id13">5</a></span></dt>
<dd><p>Ravikumar, P., Liu, H., Lafferty, J. and Wasserman, L. (2008). Spam: Sparse additive models, in J. Platt, D. Koller, Y. Singer and S. Roweis (eds), Advances in Neural Information Processing Systems 20, MIT Press, Cambridge, MA, pp. 1201–1208.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./09-Additive-Models-Trees-and-Related-Methods"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="9.0-Overview.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">第九章 加法模型、树及相关方法</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="9.2-Tree-Based-Methods.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">9.2 基于树的方法(CART)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>