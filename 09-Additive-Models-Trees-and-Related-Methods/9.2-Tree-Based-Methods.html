
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>9.2 基于树的方法(CART) &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.3 PRIM" href="9.3-PRIM.html" />
    <link rel="prev" title="9.1 广义可加模型" href="9.1-Generalized-Additive-Models.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.3-Subset-Selection.html">
     3.3 子集的选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多重输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 光滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 光滑参数的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波光滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核光滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差，方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的 optimism
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.11-Bootstrap-Methods.html">
     7.11 自助法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 条件测试误差或期望测试误差？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.2-The-Bootstrap-and-Maximum-Likelihood-Methods.html">
     8.2 自助法和最大似然法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.4-Relationship-Between-the-Bootstrap-and-Bayesian-Inference.html">
     8.4 自助法和贝叶斯推断之间的关系
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5 EM 算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆栈
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../12-Support-Vector-Machines-and-Flexible-Discriminants/Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14-Unsupervised-Learning/14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   背景
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   回归树
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   分类树
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   其他的问题
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     类别型预测变量
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     损失矩阵
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     缺失预测变量的值
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     为什么二值分割？
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     其他建树的过程
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id17">
     线性组合分割
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id18">
     树的不稳定性
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id19">
     缺乏光滑性
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id20">
     捕捉加性结构的困难
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id21">
   垃圾邮件的例子（继续）
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="cart">
<h1>9.2 基于树的方法(CART)<a class="headerlink" href="#cart" title="Permalink to this headline">¶</a></h1>
<div class="section" id="id1">
<h2>背景<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p><strong>基于树 (Tree-based)</strong> 的方法将特征空间划分成一系列的长方形，然后对每个长方形拟合简单的模型（比如常数）。想法很简单但是很有用。我们首先描述一个基于树的回归和分类的方法，称为 CART，然后将之与 C4.5 进行比较，这是它主要的一个对手。</p>
<p>让我们考虑含连续响应变量 <span class="math notranslate nohighlight">\(Y\)</span> 和输入变量 <span class="math notranslate nohighlight">\(X_1\)</span> 和 <span class="math notranslate nohighlight">\(X_2\)</span> 的回归问题，每个都在单位区间内取值。</p>
<p><img alt="" src="../_images/fig9.2.png" /></p>
<blockquote>
<div><p>图 9.2. 划分和 CART。右上图展示了二维特征空间被一些模拟的值进行递归二值分割得到的一个划分，就像 CART 中使用的那样。左上图展示了一个不能通过递归二分得到的一般划分。左下图展示了对应右上图划分的树，右下图则画出了预测平面的透视图。</p>
</div></blockquote>
<p>图 9.2 的左上图展示了用平行于坐标轴的直线对特征空间的一个划分。在每个划分元素里面我们可以用不同的常值来对 <span class="math notranslate nohighlight">\(Y\)</span> 进行建模。然而，这里有个问题：尽管每个划分直线有个类似 <span class="math notranslate nohighlight">\(X_1=c\)</span> 的简单表示，但有一些区域是很难描述的。</p>
<p>为了简化问题，我们限制在如图 9.2 的右上图所示的递归二值划分。我们首先将空间分成两个区域，然后在每个区域里面用 <span class="math notranslate nohighlight">\(Y\)</span> 的均值来对响应变量进行建模。我们选择变量和分离点以实现最优拟合。然后这些区域又被分成两个区域，这个过程一直进行下去，直到满足停止规则。举个例子，在图 9.2 的右上图，我们首先在 <span class="math notranslate nohighlight">\(X_1=t_1\)</span> 处进行划分。然后在 <span class="math notranslate nohighlight">\(X_2=t_2\)</span> 处对区域 <span class="math notranslate nohighlight">\(X_1\le t_1\)</span> 进行划分，在<span class="math notranslate nohighlight">\(X_1=t_3\)</span> 对区域 <span class="math notranslate nohighlight">\(X_1&gt; t_1\)</span> 进行划分。最后，在 <span class="math notranslate nohighlight">\(X_2=t_4\)</span> 处对区域 <span class="math notranslate nohighlight">\(X_1&gt;t_3\)</span> 进行划分。这一过程的结果是得到如图所示的五个区域 <span class="math notranslate nohighlight">\(R_1,R_2,\ldots,R_5\)</span> 的划分。对应的回归模型在区域 <span class="math notranslate nohighlight">\(R_m\)</span> 中用常数 <span class="math notranslate nohighlight">\(c_m\)</span> 来预测 <span class="math notranslate nohighlight">\(Y\)</span>，也就是</p>
<div class="math notranslate nohighlight">
\[
\hat f(X)=\sum\limits_{m=1}^5c_mI\{(X_1,X_2)\in R_m\}\tag{9.9}
\]</div>
<p>同样的模型也可以用图 9.2 的左下图的二叉树来表示。在树的顶端标出全数据集。在每个交叉点处满足条件的观测分配到左枝上去，其他的分到右枝上去。终止结点或者树的叶子对应区域 <span class="math notranslate nohighlight">\(R_1,R_2,\ldots, R_5\)</span>。图 9.2 的右下图是该模型回归表面的透视图。为了说明，我们选择结点均值 <span class="math notranslate nohighlight">\(c_1=-5,c_2=-7,c_3=0,c_4=2,c_5=4\)</span> 来画这张图。</p>
<p>递归二叉树一个重要的优点在于它的解释性。特征空间被一棵树完全描述。当超过两个输入，类似图 9.2 的右上图的划分很难画出来，但是二叉树表示能够仍然起作用。这种表示方法在医学科学家中很流行，或许它在模仿医生的思考方式。基于病人的特征，树将总体分成了高低不同的阶层。</p>
</div>
<div class="section" id="id2">
<h2>回归树<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>我们现在讨论怎样生成一个回归树的问题。我们的数据由 <span class="math notranslate nohighlight">\(p\)</span> 个输入变量和 1 个输出变量的 <span class="math notranslate nohighlight">\(N\)</span> 个观测值构成：这就是，<span class="math notranslate nohighlight">\((x_i,y_i),i=1,2,\ldots, N,\)</span>其中，<span class="math notranslate nohighlight">\(x_i=(x_{i1},x_{i2},\ldots,x_{ip})\)</span>. 这个算法需要自动确定分离变量和分类点，而且确定树应该有的拓扑结构（形状）。首先假设我们划分成 <span class="math notranslate nohighlight">\(M\)</span> 个区 <span class="math notranslate nohighlight">\(R_1,R_2,\ldots, R_M\)</span>，而且我们在每个区域内用常数 <span class="math notranslate nohighlight">\(c_m\)</span> 来对响应变量建模：</p>
<div class="math notranslate nohighlight">
\[
f(x)=\sum\limits_{m=1}^Mc_mI(x\in R_m)\tag{9.10}
\]</div>
<p>如果我们采取最小化平方和 <span class="math notranslate nohighlight">\(\sum(y_i-f(x_i))^2\)</span> 的准则，则可以很简单地证明最优的 <span class="math notranslate nohighlight">\(\hat c_m\)</span> 恰巧是区域 <span class="math notranslate nohighlight">\(R_m\)</span> 中 <span class="math notranslate nohighlight">\(y_i\)</span> 的平均值：</p>
<div class="math notranslate nohighlight">
\[
\hat c_m=\mathrm{ave}(y_i\mid x_i\in R_m)\tag{9.11}
\]</div>
<p>但找出使得平方和最小的最优二分在计算上一般是不可行的。因此我们采用一种贪婪算法。从所有变量开始，考虑分离变量 <span class="math notranslate nohighlight">\(j\)</span> 和分离点 <span class="math notranslate nohighlight">\(s\)</span>，定义半平面对</p>
<div class="math notranslate nohighlight">
\[
R_1(j,s)=\{X\mid X_j\le s\}\;\text{and}\; R_2(j,s)=\{X\mid X_j&gt;s\}\tag{9.12}
\]</div>
<p>于是我们寻找分离变量 <span class="math notranslate nohighlight">\(j\)</span> 和分离点 <span class="math notranslate nohighlight">\(s\)</span> 求解</p>
<div class="math notranslate nohighlight">
\[
\underset{j,s}{\min}\left[\underset{c_1}{\min}\;\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\underset{c_2}{\min}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]\tag{9.13}\label{9.13}
\]</div>
<p>对于每个选定的 <span class="math notranslate nohighlight">\(j\)</span> 和 <span class="math notranslate nohighlight">\(s\)</span>，里面的最小化通过下式来求解</p>
<div class="math notranslate nohighlight">
\[
\hat c_1=\mathrm{ave}(y_i\mid x_i\in R_1(j,s))\;\text{and}\; \hat c_2=\mathrm{ave}(y_i\mid x_i\in R_2(j,s))\tag{9.14}
\]</div>
<p>对于每个分离变量，可以非常快地确定分离点 <span class="math notranslate nohighlight">\(s\)</span>，因此对所有输入进行扫描，确定最优的对 <span class="math notranslate nohighlight">\((j,s)\)</span> 是可行的。</p>
<p>找到最优的分割之后，我们将数据分成两个区域，然后对每个区域重复分割过程。然后在对所有区域都重复同样的分割过程。</p>
<p>我们应该生成多大的树？很显然，树太大会过拟合数据，而树太小则可能捕捉不了重要的结构。树的大小也是一个调整参数来控制模型复杂度，最优的树规模应该从数据中自适应选取。一种方式是仅当分割平方和降低超出某阈值才分割结点。然而，这种策略太目光短浅了，因为看似无用的分割或许会导致下面得到很好的分割。</p>
<p>更好的策略是生成一个大树 <span class="math notranslate nohighlight">\(T_0\)</span>，只有当达到最小结点规模（比如 5）才停止分割过程。接着大树采用 <strong>成本复杂度剪枝 (cost-complexity pruning)</strong> 来调整，这也是下面我们要讨论的内容。</p>
<p>我们定义子树 <span class="math notranslate nohighlight">\(T\subset T_0\)</span> 为任何可以通过对 <span class="math notranslate nohighlight">\(T_0\)</span> 剪枝得到的树，也就是，压缩任意数目的中间（非终止）结点。我们用 <span class="math notranslate nohighlight">\(m\)</span> 表示终止结点，其中结点 <span class="math notranslate nohighlight">\(m\)</span> 表示区域 <span class="math notranslate nohighlight">\(R_m\)</span>。令 <span class="math notranslate nohighlight">\(\vert T\vert \)</span> 为 <span class="math notranslate nohighlight">\(T\)</span> 中终止结点的个数。令
$$</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
N_m&amp;=\#\{x_i\in R_m\}\\
\hat c_m&amp;=\frac{1}{N_m}\sum\limits_{x_i\in R_m}y_i\\
Q_m(T)&amp;=\frac{1}{N_m}\sum\limits_{x_i\in R_m}(y_i-\hat c_m)^2
\end{align*}\]</div>
<p>\tag{9.15}\label{9.15}
$<span class="math notranslate nohighlight">\(
定义成本复杂度准则
\)</span><span class="math notranslate nohighlight">\(
C_\alpha(T)=\sum\limits_{m=1}^{\vert T\vert}N_mQ_m(T)+\alpha\vert T\vert\tag{9.16}
\)</span><span class="math notranslate nohighlight">\(
想法是对于每个 \)</span>\alpha<span class="math notranslate nohighlight">\(，寻找子树 \)</span>T_\alpha\subset T_0<span class="math notranslate nohighlight">\( 使得 \)</span>C_\alpha(T)<span class="math notranslate nohighlight">\( 最小化。调整参数 \)</span>\alpha\ge 0<span class="math notranslate nohighlight">\( 可以控制树的规模和它对数据拟合的程度之间的平衡。较大的 \)</span>\alpha<span class="math notranslate nohighlight">\( 值导致小规模的树 \)</span>T_\alpha<span class="math notranslate nohighlight">\(，对于较小的 \)</span>\alpha<span class="math notranslate nohighlight">\( 情况反过来了。这种记号表明，当 \)</span>\alpha=0<span class="math notranslate nohighlight">\( 时，解为全树 \)</span>T_0<span class="math notranslate nohighlight">\(. 我们下面讨论怎么自适应地选择 \)</span>\alpha$.</p>
<p>对于每个 <span class="math notranslate nohighlight">\(\alpha\)</span>，可以找到唯一最小的子树 <span class="math notranslate nohighlight">\(T_\alpha\)</span> 使得 <span class="math notranslate nohighlight">\(C_\alpha(T)\)</span> 最小化。为了寻找 <span class="math notranslate nohighlight">\(T_\alpha\)</span> 我们采用 <strong>最差连接剪枝 (weakest link pruning)</strong>：逐步合并在 <span class="math notranslate nohighlight">\(\sum_mN_mQ_m(T)\)</span> 中单节点增长最小的中间结点，直到我们得到单结点（根）的树。这给出了子树的（有限）序列，而且可以证明这条序列一定包含 <span class="math notranslate nohighlight">\(T_\alpha\)</span>. 更多细节见 Breiman et al. (1984)<a class="footnote-reference brackets" href="#id24" id="id3">1</a> 或者 Ripley (1996)<a class="footnote-reference brackets" href="#id25" id="id4">2</a>。<span class="math notranslate nohighlight">\(\alpha\)</span> 的估计可以通过 5 折或者 10 折交叉验证得到：我们选择 <span class="math notranslate nohighlight">\(\hat\alpha\)</span> 使得交叉验证平方和最小。我们最终的树为 <span class="math notranslate nohighlight">\(T_{\hat \alpha}\)</span>.</p>
</div>
<div class="section" id="id5">
<h2>分类树<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>如果目标是结果取值为 <span class="math notranslate nohighlight">\(1,2,\ldots,K\)</span> 的分类问题，则在树的算法中唯一需要改变的是关于分离结点以及剪枝的准则。对于回归我们用 \eqref{9.15} 式定义的平方误差结点纯度，但是这对于分类便不适合了。在结点 <span class="math notranslate nohighlight">\(m\)</span>，用 <span class="math notranslate nohighlight">\(N_m\)</span> 个观测值表示区域 <span class="math notranslate nohighlight">\(R_m\)</span>，令
$<span class="math notranslate nohighlight">\(
\hat p_{mk} = \frac{1}{N_m}\sum\limits_{x_i\in R_m}I(y_i=k)
\)</span><span class="math notranslate nohighlight">\(
类别 \)</span>k<span class="math notranslate nohighlight">\( 的观测在结点 \)</span>m<span class="math notranslate nohighlight">\( 处的比例。我们将结点 \)</span>m<span class="math notranslate nohighlight">\( 处的观测划分为类别 \)</span>k(m)=\mathrm{arg; max}_k\hat p_{mk}<span class="math notranslate nohighlight">\(，结点 \)</span>m<span class="math notranslate nohighlight">\( 处最主要的类别。不同衡量结点纯度 \)</span>Q_m(T)<span class="math notranslate nohighlight">\( 的方法包括如下：
\)</span>$</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\text{误分类误差：}&amp; \frac{1}{N_m}\sum_{i\in R_m}I(y_i\neq k(m))=1-\hat p_{mk(m)}\\
\text{基尼指数：}&amp;\sum_{k\neq k'}\hat p_{mk}\hat p_{mk'}=\sum_{k=1}^K\hat p_{mk}(1-\hat p_{mk})\\
\text{交叉熵或者偏差：}&amp;-\sum_{k=1}^K\hat p_{mk}\mathbf{log} \hat p_{mk}
\end{align*}\]</div>
<p>\tag{9.17}
$<span class="math notranslate nohighlight">\(
对于两个类别，如果 \)</span>p<span class="math notranslate nohighlight">\( 是第二类的比例，则这三个衡量指标分别为 \)</span>1-\max(p,1-p), 2p(1-p)<span class="math notranslate nohighlight">\( 以及 \)</span>-p\mathrm{log}p-(1-p)\mathrm{log}(1-p)<span class="math notranslate nohighlight">\(. 图 9.3 画出了它们的函数曲线。这三者都类似，但是 **交叉熵 (cross-entropy)** 和 **基尼指数 (Gini index)** 是可微的，因此更加适合数值优化。比较 \eqref{9.13} 和 \eqref{9.15}，我们看到我们需要由分离结点\)</span>m<span class="math notranslate nohighlight">\( \)</span>N_{m_L}<span class="math notranslate nohighlight">\( 和 \)</span>N_{m_R}$ 来对结点纯度赋予权重。</p>
<p><img alt="" src="../_images/fig9.3.png" /></p>
<blockquote>
<div><p>图 9.3 两类别分类的结点纯度关于类别 2 比例 <span class="math notranslate nohighlight">\(p\)</span> 的函数。交叉熵已被缩放使能过点 <span class="math notranslate nohighlight">\((0.5,0.5)\)</span>.</p>
</div></blockquote>
<p>另外，交叉熵和基尼指数比误分类率对结点概率的改变更加敏感。举个例子，在每个类别都有 400 个观测的二分类问题中（记为（400,400）），假设一个分割得到结点（300,100）和结点（100,300），而另外一个分割得到结点（200,400）和结点（200，0）。两个分割过程的误分类误差率都是 0.25，但是第二个分割产生一个纯结点，似乎更好一些。对于第二个分割，基尼指数和交叉熵都更低一点。基于这个原因，当生成一棵树时应该使用基尼指数或者交叉熵中的一个。为了引导成本复杂度剪枝，三种衡量指标的任意一个都可以使用，但是一般地是采用误分类误差率。</p>
<p>基尼指数可以用两种有趣的方式来解读。与其将观测值划分到结点的主要类别中，我们可以以一定概率 <span class="math notranslate nohighlight">\(\hat p_{mk}\)</span> 将它们划分到类别 <span class="math notranslate nohighlight">\(k\)</span>。于是该结点的这条规则的训练误差率的期望是 <span class="math notranslate nohighlight">\(\sum_{k\neq k'}\hat p_{mk}\hat p_{mk'}\)</span>——基尼指数。类似地，如果我们对类别 <span class="math notranslate nohighlight">\(k\)</span> 的观测编码为1，而对其它的观测编码为0，则该结点的 0-1 响应变量的方差为 <span class="math notranslate nohighlight">\(\hat p_{mk}(1-\hat p_{mk})\)</span>. 将 <span class="math notranslate nohighlight">\(k\)</span> 个类别的全部相加也可以得到基尼指数。</p>
<p>!!! note “weiya注”
下面看一下如何在 R 中实现 CART 算法，采用 rpart 包来完成，下面是一个很简单的例子。</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>library(rpart)

library(rpart.plot)

fit &lt;- rpart(Kyphosis ~ Age + Number + Start, data = kyphosis)

rpart.plot(fit)

最终，我们得到
![](../img/09/ex_CART.png)
</pre></div>
</div>
</div>
<div class="section" id="id6">
<h2>其他的问题<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id7">
<h3>类别型预测变量<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>当对一个有着 <span class="math notranslate nohighlight">\(q\)</span> 个可能的无次序值的预测变量，则将这 <span class="math notranslate nohighlight">\(q\)</span> 个值分成两块有 <span class="math notranslate nohighlight">\(2^{q-1}-1\)</span> 种可能的划分，对于大的 <span class="math notranslate nohighlight">\(q\)</span> 值，计算变得很困难。然而，输出为 0-1时，计算可以简化。我们根据落入类别 1 的比例对预测变量的类别进行排序。然后把预测变量看成是有序预测变量进行划分。可以证明这给出了在交叉熵或者基尼指数意义下 <span class="math notranslate nohighlight">\(2^{q-1}-1\)</span> 种可能分割中的最优分割。这个结果对与定量输出和平方误差损失也一样成立——通过输出的均值对类别进行升序排列。尽管很直观，但是这些断言的证明不是很简单。二值输出变量的证明由 Breiman et al. (1984)<a class="footnote-reference brackets" href="#id24" id="id8">1</a> 和 Ripley (1996)<a class="footnote-reference brackets" href="#id25" id="id9">2</a> 给出；定量输出变量的证明可以在 Fisher (1958)<a class="footnote-reference brackets" href="#id26" id="id10">3</a> 中找到。对于多重类别的输出，没有这样的简化，尽管已经提出不同的近似（Loh and Vanichsetakul, 1988<a class="footnote-reference brackets" href="#id27" id="id11">4</a>）</p>
<p>这个划分算法趋向于偏爱有多个层次 <span class="math notranslate nohighlight">\(q\)</span> 的类别型预测变量；划分的种类随着 <span class="math notranslate nohighlight">\(q\)</span> 呈指数增长，而且我们有更多的选择，我们更有可能找到一个对于当前数据好的方法。如果 <span class="math notranslate nohighlight">\(q\)</span> 很大，这导致严重的过拟合，而且类似的变量应该被避免。</p>
</div>
<div class="section" id="id12">
<h3>损失矩阵<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p>在分类问题中，某些类别的误分类的后果会比其他类别的误分类后果要严重。举个例子，当某人实际上会发生心脏病而预测他/她不会发生心脏病比反过来的情形要糟糕。为了说明这一点，我们定义 <span class="math notranslate nohighlight">\(K\times K\)</span> 的损失矩阵 <span class="math notranslate nohighlight">\(\mathbf L\)</span>，其中 <span class="math notranslate nohighlight">\(L_{kk'}\)</span> 为将类别 <span class="math notranslate nohighlight">\(k\)</span> 的观测划分为类别 <span class="math notranslate nohighlight">\(k'\)</span> 产生的损失。一般地，对于正确的分类不会带来损失，也就是 <span class="math notranslate nohighlight">\(L_{kk}=0\;\forall k\)</span>. 为了将损失结合到模型过程中，我们可以将基尼指数修改成 <span class="math notranslate nohighlight">\(\sum_{k\neq k'}L_{kk'}\hat p_{mk}\hat p_{mk'}\)</span>；这个可以成为由于随机规则引起的期望损失。这对于多重类别的情形也成立，但是对于两个类别的情形没有影响，因为 <span class="math notranslate nohighlight">\(\hat p_{mk}\hat p_{mk'}\)</span> 的系数为 <span class="math notranslate nohighlight">\(L_{kk'}+L_{k'k}\)</span>. 对于两个类别的情形更好的方式是对类别 <span class="math notranslate nohighlight">\(k\)</span> 的观测赋予权重 <span class="math notranslate nohighlight">\(L_{kk'}\)</span>. 这个可以在多重类别情形下使用仅仅当关于 <span class="math notranslate nohighlight">\(k\)</span> 的函数 <span class="math notranslate nohighlight">\(L_{kk'}\)</span> 不依赖 <span class="math notranslate nohighlight">\(k'\)</span>. 观测的权重也可以同偏差一起使用。观测权重的影响是去更改类别的先验概率。在终止结点上，<strong>经验贝叶斯 (empirical Bayes)</strong> 规则表明我们划分到类别 <span class="math notranslate nohighlight">\(k(m)=\mathrm{arg\; min_k}\sum_\ell L_{\ell k}\hat p_{m\ell}\)</span>.</p>
</div>
<div class="section" id="id13">
<h3>缺失预测变量的值<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>假设我们的数据在部分或全部变量中有一些缺失的预测变量的值。我们可能丢弃含有缺失数据的观测，但是这会导致训练集严重的消耗。另外我们或许试图填补缺失数据，如用非缺失观测的均值来填补。对于基于树的模型，有两种更好的方式。第一种是应用到类别型预测变量上：我们简单地新建一个“缺失”的类别。从这里我们可能发现含缺失值的观测在某些指标上的表现与其他不含缺失值的表现有不同。第二种更一般的方式是构造代理变量。当对一个分割考虑一个预测变量，我们仅仅考虑该变量未缺失的观测。选择好最优的（主要的）预测变量和分割点，我们构造代理预测变量和分离点的列表。<strong>第一代理 (The first surrogate)</strong> 是对由 <strong>主要分割 (primary split)</strong> 得到的训练数据进行分割时的最优预测变量及其对应的分离点，而第二代理是次优的预测变量和对应的分离点，以此类推。当在训练阶段或者预测阶段使得观测沿着树从上往下，如果主要分割变量缺失，则依次采用代理分割。代理分割探索预测变量之间的相关性来试图减轻丢失数据的影响。缺失预测变量和其他预测变量的相关度越高，则由于缺失数据造成信息丢失的影响越小。缺失数据的一般问题将在 <span class="xref myst">9.6 节</span>中讨论。</p>
</div>
<div class="section" id="id14">
<h3>为什么二值分割？<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>与其在每一步对每个结点只分割成两个群体（如上面讨论的），我们或许可以考虑多重分割成多于两个群体。尽管这个在某些情况下是有用的，但不是一个好的一般策略。问题在于多重分割将数据分得太快，以至于在下一层次没有充分多的数据。因此我们仅仅当需要的时候采用这种分割。因为多重分割可以通过一系列的二值分割实现，所以后者更好一点。</p>
</div>
<div class="section" id="id15">
<h3>其他建树的过程<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>上面的讨论集中在 <strong>分类和回归树 (classification and regression tree, CART)</strong> 的实现。其他流行的方法是 ID3 和后来的版本，C4.5 和 C5.0（Quinlan, 1993<a class="footnote-reference brackets" href="#id28" id="id16">5</a>). 程序的早期版本只局限于类别型变量，而且采用自上而下没有剪枝的规则。经过最近的发展，C5.0 已经变得与 CART 很相似。C5.0 独有的最显著的特征是用于导出规则集的方案。树建成之后，定义终止结点的分割规则有时可以被简化：这就是，可以在不改变落入结点里面的观测时删掉一个或多个条件。最终我们得到定义每个终止结点的简化规则集；这些不再遵循树的结构，但是它们的简化可能使得它们更加吸引人们使用。</p>
</div>
<div class="section" id="id17">
<h3>线性组合分割<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<p>除了将分割限制成 <span class="math notranslate nohighlight">\(X_j\le s\)</span> 的形式，还可以允许线性组合形式的分割 <span class="math notranslate nohighlight">\(\sum a_jX_j\le s\)</span>. 优化权重 <span class="math notranslate nohighlight">\(a_j\)</span> 和分离点 <span class="math notranslate nohighlight">\(s\)</span> 使得有关的准则最小（比如基尼指数）。尽管这个可以提高树的预测能力，但这也会损害解释性。计算上，分离点搜索的分离性妨碍了对权重的光滑优化。更好地结合线性组合分割的方式是在 <strong>专家系统混合模型 (hierarchical mixtures of experts, HME)</strong> 中，这是 <span class="xref myst">9.5 节</span>的主题。</p>
</div>
<div class="section" id="id18">
<h3>树的不稳定性<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<p>树的一个主要问题是它们的高方差性。经常在数据中的一个小改动导致完全不同的分割点序列，使得解释不稳定。这种不稳定的主要原因是这个过程的层次性：上一个分割点的误差会传递到下面所有的分割点上。可以试图采取更加稳定的分离准则在某种程度上减轻这一影响，但是固有的不稳定性没有移除。这是从数据中估计一个简单的、基于树结构的代价。Bagging（<span class="xref myst">8.7 节</span>）对很多树进行平均来降低方差。</p>
</div>
<div class="section" id="id19">
<h3>缺乏光滑性<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h3>
<p>树的另一个限制是预测表面缺乏光滑性，如在图 9.2 中的右下图中那样。在 0/1 损失的分类问题中，这不会有太大的损伤，因为类别概率估计的偏差的影响有限。然而，在回归问题中这会降低效果，正常情况下我们期望潜在的函数是光滑的。<span class="xref myst">9.4 节</span>介绍的 MARS 过程可以看出是为了减轻 CART 缺乏光滑性而做的改动。</p>
</div>
<div class="section" id="id20">
<h3>捕捉加性结构的困难<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<p>另外一个树的问题是对加性结构建模的困难。举个例子，在回归问题中，假设 <span class="math notranslate nohighlight">\(Y=c_1I(X_1 &lt; t_1)+c_2I(X_2 &lt; t_2)+\varepsilon\)</span>，其中 <span class="math notranslate nohighlight">\(\varepsilon\)</span> 是 0 均值噪声。则二叉树会在 <span class="math notranslate nohighlight">\(t_1\)</span> 附近对 <span class="math notranslate nohighlight">\(X_1\)</span> 做第一次分割。为了捕捉加性结构，下一层两个结点都需要在 <span class="math notranslate nohighlight">\(t_2\)</span> 处对 <span class="math notranslate nohighlight">\(X_2\)</span> 分割。在充足数据情况下这个或许可以发生，但是这个模型没有给予特别鼓励来找到这一结构。如果有 10 个而不是 2 个加性影响，需要花费很多偶然的分割来重造这个结构，并且在估计的树中，通过数据分析来识别它会变得非常困难。原因再一次可以归结为二叉树的结构，既有优点也有不足。为了捕捉加性结构，MARS 方法（<span class="xref myst">9.4 节</span>）再一次放弃树的结构。</p>
</div>
</div>
<div class="section" id="id21">
<h2>垃圾邮件的例子（继续）<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h2>
<p>前面我们已经对 spam 例子应用了分类树的方法。我们运用偏差来建树，用误分类率来剪枝。图 9.4 显示了 10 折交叉验证误差率作为剪枝树的大小的函数，以及均值的 <span class="math notranslate nohighlight">\(\pm2\)</span> 倍标准误差。测试误差曲线用橘黄色来表示。注意到交叉验证误差率是通过 <span class="math notranslate nohighlight">\(\alpha\)</span> 的序列而非树的大小来索引的；对于不同“折”建的树，同个 <span class="math notranslate nohighlight">\(\alpha\)</span> 可能对应不同的大小。在图的底部所示的大小指的是剪枝的原始树的大小 <span class="math notranslate nohighlight">\(\vert T_\alpha\vert\)</span>.</p>
<p><img alt="" src="../_images/tab9.3.png" /></p>
<blockquote>
<div><p>表 9.3. 垃圾邮件数据：测试数据上 17 个结点（通过交叉验证选出来的）树的混淆比例。总的误差率为 9.3%。</p>
</div></blockquote>
<p><img alt="" src="../_images/fig9.4.png" /></p>
<blockquote>
<div><p>图 9.4. spam 例子的结果。蓝色曲线是 10 折交叉验证误分类率的估计作为树大小的函数的曲线。最小值发生在含有 17 个终止结点的树处（采用“one-standard-error”规则）。橘黄色曲线展示了测试误差，其非常接近地沿着 CV 误差。交叉验证由如上所示的 <span class="math notranslate nohighlight">\(\alpha\)</span> 值索引。下面显示的树的大小指的是由 <span class="math notranslate nohighlight">\(\alpha\)</span> 索引的原始树的大小 <span class="math notranslate nohighlight">\(\vert T_\alpha\vert\)</span>。</p>
</div></blockquote>
<p>误差在大概 17 个终止结点处变平，给出了图 9.5 的剪枝树。在树的 13 个不同特征中，有 11 个与加性模型中 16 个显著特征重合（表 9.2）.表 9.3 显示的整体误差率比表 9.1 显示的加性模型大概高出 50%.</p>
<p><img alt="" src="../_images/fig9.5.png" /></p>
<blockquote>
<div><p>图 9.5. spam 例子的剪枝树。分离变量在树枝上用蓝色显示，分类结果在每个结点处显示。在终止结点处的数目表示在测试数据上的错误分类率。</p>
</div></blockquote>
<p>考虑树的最右边分支，如果超过 5.5% 的字符是 <code class="docutils literal notranslate"><span class="pre">$</span></code> 符号则将分到右边分支并且显示为 <code class="docutils literal notranslate"><span class="pre">spam</span></code> 的警告。然而，另外如果词语 <code class="docutils literal notranslate"><span class="pre">hp</span></code> 经常出现，则很可能是公司事务并且我们分成 <code class="docutils literal notranslate"><span class="pre">email</span></code>。在测试集里的所有 22 个满足这样准则的情形都被正确分类。如果第二个条件不满足，而且重复出现的大写字母平均长度 <code class="docutils literal notranslate"><span class="pre">CAPAVE</span></code> 大于 2.9，则我们分成 <code class="docutils literal notranslate"><span class="pre">spam</span></code>。在 227 个测试情形中，只有 7 个被错误分类。</p>
<p>在医学分类问题中，<strong>敏感度 (sensitivity)</strong> 和 <strong>特异度 (specificity)</strong> 经常用来衡量一个准则。它们按如下定义：</p>
<ul class="simple">
<li><p>敏感度：给定真实状态为患病预测为患病的概率</p></li>
<li><p>特异度：给定真实状态为未患病预测为未患病的概率</p></li>
</ul>
<p>如果我们将 <code class="docutils literal notranslate"><span class="pre">spam</span></code> 和 <code class="docutils literal notranslate"><span class="pre">email</span></code> 看成有无患病，则从表 9.3 我们有
$$</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\text{Sensitivity} &amp; = 100\times\frac{33.4}{33.4+5.3}=86.3\%\\
\text{Specificity} &amp;  = 100\times \frac{57.3}{57.3+4.0}=93.4\%
\end{align*}\]</div>
<p>$<span class="math notranslate nohighlight">\(
在这个分析中我们使用了相等的损失。和之前一样令 \)</span>L_{kk’}<span class="math notranslate nohighlight">\( 为将类别 \)</span>k<span class="math notranslate nohighlight">\( 预测成类别 \)</span>k’<span class="math notranslate nohighlight">\( 对应的损失。通过改变损失 \)</span>L_{01}<span class="math notranslate nohighlight">\( 和 \)</span>L_{10}<span class="math notranslate nohighlight">\( 的相对大小，我们增大该规则的敏感性降低该规则的特异性，或者反过来。在这个例子中，我们想要避免将好的 `email` 预测成 `spam`，因此我们希望特异度非常高。我们可以通过设置 \)</span>L_{01}&gt;1,L_{10}=1<span class="math notranslate nohighlight">\( 来实现。如果 `spam` 的比例 \)</span>\ge L_{01}/(L_{10}+L_{01})<span class="math notranslate nohighlight">\(，则在每个终止结点处的贝叶斯规则将此结点归类为类别 1，否则归为类别 0. **受试者工作特征曲线 (receiver operating characteristic curve, ROC)** 是用于评估敏感度和特异度之间折中的常用概述。当我们改变分类规则的参数便会得到敏感度关于特异度的图像。在 0.1 和 10 之间改变损失 \)</span>L_{01}$，然后对图 9.4 中选定的 17 个结点的贝叶斯规则，得到如图 9.6 所示的 ROC 曲线。</p>
<p><img alt="" src="../_images/fig9.6.png" /></p>
<blockquote>
<div><p>图 9.6. 对 <code class="docutils literal notranslate"><span class="pre">spam</span></code> 数据应用分类规则拟合的 ROC 曲线。越靠近东北角落的曲线表示越好的分类器。这种情形下，GAM 分类器比树模型更好。加权树比未加权树在更高特异性情况下有更好的敏感性。图例中的数字表示曲线下面的面积。</p>
</div></blockquote>
<p>每条曲线靠近 0.9 的标准误差近似为 <span class="math notranslate nohighlight">\(\sqrt{0.9(1-0.9)/1536}=0.008\)</span>，因此差别的标准误差大概为 0.01。我们为了实现接近 100% 的特异性，敏感性大致降低到了50%. 曲线下面的面积是通常使用的定量概述；在曲线的两端线性扩张使得它在 [0, 100] 上有定义，面积大概为 0.95. 为了比较，我们也画出了在 <span class="xref myst">9.1 节</span>对这些数据进行 GAM 模型拟合的 ROC 曲线；它给出了对于任意损失更好的分类规则，面积为 0.98.</p>
<p>与其仅仅修改结点处的贝叶斯规则，在建树的时候将不相等的损失进行全面考虑更好，正如我们在 9.2 节做的那样。当只有两个类别 0 和 1 时，通过对类别 <span class="math notranslate nohighlight">\(k\)</span> 的观测赋予权重 <span class="math notranslate nohighlight">\(L_{k,1-k}\)</span> 将损失考虑到建树的过程中。这里我们选择 <span class="math notranslate nohighlight">\(L_{01}=5,L_{10}=1\)</span> 而且像前面一样拟合同样大小的树（<span class="math notranslate nohighlight">\(\vert T_\alpha\vert=17\)</span>）. 这棵树在高特异度性下比原树有更高的敏感度，但是在其它极端情况下表现得更差。前几层的分割与原树是一样的，但是接着就变不同了。因为这个应用通过 <span class="math notranslate nohighlight">\(L_{01}=5\)</span> 建的树显然好于原来的树。</p>
<p>上面使用的 ROC 曲线下的面积有时被称作 <strong><span class="math notranslate nohighlight">\(c\)</span> 统计量 (c-statistics)</strong>。有趣地是，对于两个群体中预测得分差异的中位数（Hanley and McNeil，1982<a class="footnote-reference brackets" href="#id29" id="id22">6</a>），可以证明 ROC 曲线下的面积等于 Mann-Whitney U 统计量（或者 Wilcoxon 秩和检验）. 为了衡量额外的预测变量加入到标准模型中时衡量该预测变量的贡献度，<span class="math notranslate nohighlight">\(c\)</span> 统计量或许不是一个有信息的衡量。新预测变量可以对模型偏差改变很显著，但是 <span class="math notranslate nohighlight">\(c\)</span> 统计量却只有很小的增大。举个例子，从表 9.2 中移除高显著项 <code class="docutils literal notranslate"><span class="pre">george</span></code> 会导致 <span class="math notranslate nohighlight">\(c\)</span> 统计量的增长小于 0.01. 相反地，基于个体样本基来检查额外的预测变量怎样改变分类是有用的。这点将在 Cook (2007)<a class="footnote-reference brackets" href="#id30" id="id23">7</a> 中进行充分讨论。</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id24"><span class="brackets">1</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id8">2</a>)</span></dt>
<dd><p>Breiman, L., Friedman, J., Olshen, R. and Stone, C. (1984). Classification and Regression Trees, Wadsworth, New York.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">2</span><span class="fn-backref">(<a href="#id4">1</a>,<a href="#id9">2</a>)</span></dt>
<dd><p>Ripley, B. D. (1996). Pattern Recognition and Neural Networks, Cambridge University Press.</p>
</dd>
<dt class="label" id="id26"><span class="brackets"><a class="fn-backref" href="#id10">3</a></span></dt>
<dd><p>Fisher, W. (1958). On grouping for maximum homogeniety, Journal of the American Statistical Association 53(284): 789–798.</p>
</dd>
<dt class="label" id="id27"><span class="brackets"><a class="fn-backref" href="#id11">4</a></span></dt>
<dd><p>Loh, W. and Vanichsetakul, N. (1988). Tree structured classification via generalized discriminant analysis, Journal of the American Statistical Association 83: 715–728.</p>
</dd>
<dt class="label" id="id28"><span class="brackets"><a class="fn-backref" href="#id16">5</a></span></dt>
<dd><p>Quinlan, R. (1993). C4.5: Programs for Machine Learning, Morgan Kaufmann, San Mateo.</p>
</dd>
<dt class="label" id="id29"><span class="brackets"><a class="fn-backref" href="#id22">6</a></span></dt>
<dd><p>Hanley, J. and McNeil, B. (1982). The meaning and use of the area under a receiver operating characteristic (roc) curve, Radiology 143: 29–36.</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id23">7</a></span></dt>
<dd><p>Cook, N. (2007). Use and misuse of the receiver operating characteristic curve in risk prediction, Circulation 116(6): 928–35.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./09-Additive-Models-Trees-and-Related-Methods"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="9.1-Generalized-Additive-Models.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">9.1 广义可加模型</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="9.3-PRIM.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">9.3 PRIM</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>