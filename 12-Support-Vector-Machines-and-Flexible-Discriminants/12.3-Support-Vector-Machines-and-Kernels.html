
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>12.3 支持向量机和核 &#8212; 统计学习精要(中文)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12.4 广义线性判别分析" href="12.4-Generalizing-Linear-Discriminant-Analysis.html" />
    <link rel="prev" title="12.2 支持向量分类器" href="12.2-The-Support-Vector-Classifier.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">统计学习精要(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Welcome to your Jupyter Book
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-Introduction/1.1-Introduction.html">
   第一章 引言
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.0-Overview.html">
   第二章 监督学习概览
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.1-Introduction.html">
     2.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.2-Variable-Types-and-Terminology.html">
     2.2 变量类型和术语
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.3-Two-Simple-Approaches-to-Prediction.html">
     2.3 两种简单的预测方式：最小二乘和最近邻
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.4-Statistical-Decision-Theory.html">
     2.4 统计判别理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.5-Local-Methods-in-High-Dimensions.html">
     2.5 高维问题的局部方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.7-Structured-Regression-Models.html">
     2.7 结构化的回归模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.8-Classes-of-Restricted-Estimators.html">
     2.8 限制性估计的种类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/2.9-Model-Selection-and-the-Bias-Variance-Tradeoff.html">
     2.9 模型选择和偏差-方差的权衡
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02-Overview-of-Supervised-Learning/Bibliographic-Notes.html">
     2.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.0-Overview.html">
   第三章 线性回归模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.1-Introduction.html">
     3.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.2-Linear-Regression-Models-and-Least-Squares.html">
     3.2 线性回归模型和最小二乘法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.3-Subset-Selection.html">
     3.3 子集的选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.4-Shrinkage-Methods.html">
     3.4 收缩的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.5-Methods-Using-Derived-Input-Directions.html">
     3.5 运用派生输入方向的方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.6-A-Comparison-of-the-Selection-and-Shrinkage-Methods.html">
     3.6 讨论：选择和收缩方法的比较
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.7-Multiple-Outcome-Shrinkage-and-Selection.html">
     3.7 多输出的收缩和选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.8-More-on-the-Lasso-and-Related-Path-Algorithms.html">
     3.8 Lasso 和相关路径算法的补充
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/3.9-Computational-Considerations.html">
     3.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03-Linear-Methods-for-Regression/Bibliographic-Notes.html">
     3.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.0-Overview.html">
   第四章 线性分类模型
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.1-Introduction.html">
     4.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.2-Linear-Regression-of-an-Indicator-Matrix.html">
     4.2 指示矩阵的线性回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.3-Linear-Discriminant-Analysis.html">
     4.3 线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.4-Logistic-Regression.html">
     4.4 逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/4.5-Separating-Hyperplanes.html">
     4.5 分离超平面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04-Linear-Methods-for-Classification/Bibliographic-Notes.html">
     4.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.0-Overview.html">
   第五章 基展开与正则化
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html">
     5.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.2-Piecewise-Polynomials-and-Splines.html">
     5.2 分段多项式和样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.3-Filtering-and-Feature-Extraction.html">
     5.3 过滤和特征提取
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines.html">
     5.4 光滑样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.5-Automatic-Selection-of-the-Smoothing-Parameters.html">
     5.5 光滑参数的自动选择
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression.html">
     5.6 非参逻辑斯蒂回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.7-Multidimensional-Splines.html">
     5.7 多维样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces.html">
     5.8 正则化和再生核希尔伯特空间理论
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.9-Wavelet-Smoothing.html">
     5.9 小波光滑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Bibliographic-Notes.html">
     5.10 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../05-Basis-Expansions-and-Regularization/Appendix-Computations-for-B-splines.html">
     附录
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.0-Overview.html">
   第六章 核平滑方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.1-One-Dimensional-Kernel-Smoothers.html">
     6.1 一维核光滑器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.2-Selecting-the-Width-of-the-Kernel.html">
     6.2 选择核的宽度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.3-Local-Regression-in-Rp.html">
     6.3
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中的局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.4-Structured-Local-Regression-Models-in-Rp.html">
     6.4
     <span class="math notranslate nohighlight">
      \(\mathcal{IR}^p\)
     </span>
     中结构化局部回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.5-Local-Likelihood-and-Other-Models.html">
     6.5 局部似然和其他模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.6-Kernel-Density-Estimation-and-Classification.html">
     6.6 核密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels.html">
     6.7 径向基函数和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.8-Mixture-Models-for-Density-Estimation-and-Classification.html">
     6.8 混合模型的密度估计和分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/6.9-Computational-Consoderations.html">
     6.9 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06-Kernel-Smoothing-Methods/Bibliographic-Notes.html">
     6.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.0-Overview.html">
   第七章 模型评估与选择
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.1-Introduction.html">
     7.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.2-Bias-Variance-and-Model-Complexity.html">
     7.2 偏差、方差和模型复杂度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.3-The-Bias-Variance-Decomposition.html">
     7.3 偏差-方差分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.4-Optimism-of-the-Training-Error-Rate.html">
     7.4 训练误差率的乐观估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.5-Estimates-of-In-Sample-Prediction-Error.html">
     7.5 样本内误差的估计
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.6-The-Effective-Number-of-Parameters.html">
     7.6 参数的有效个数
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.7-The-Bayesian-Approach-and-BIC.html">
     7.7 贝叶斯方法和 BIC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.8-Minimum-Description-Length.html">
     7.8 最小描述长度
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html">
     7.9 VC维
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.10-Cross-Validation.html">
     7.10 交叉验证
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/7.12-Conditional-or-Expected-Test-Error.html">
     7.12 “条件测试误差”还是“测试误差的期望”？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07-Model-Assessment-and-Selection/Bibliographic-Notes.html">
     7.13 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.0-Overview.html">
   第八章 模型推断与模型平均
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.1-Introduction.html">
     8.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.3-Bayesian-Methods.html">
     8.3 贝叶斯方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm.html">
     8.5 EM 算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior.html">
     8.6 从后验分布采样的 MCMC
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.7-Bagging.html">
     8.7 Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.8-Model-Averaging-and-Stacking.html">
     8.8 模型平均和堆栈
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/8.9-Stochastic-Search.html">
     8.9 随机搜索： Bumping
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../08-Model-Inference-and-Averaging/Bibliographic-Notes.html">
     8.10 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.0-Overview.html">
   第九章 加法模型、树及相关方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.1-Generalized-Additive-Models.html">
     9.1 广义可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.2-Tree-Based-Methods.html">
     9.2 基于树的方法(CART)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.3-PRIM.html">
     9.3 PRIM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.4-MARS.html">
     9.4 MARS: 多变量自适应回归样条
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.5-Hierarchical-Mixtures-of-Experts.html">
     9.5 专家的分层混合
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.6-Missing-Data.html">
     9.6 缺失数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/9.7-Computational-Considerations.html">
     9.7 计算的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09-Additive-Models-Trees-and-Related-Methods/Bibliographic-Notes.html">
     9.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.0-Overview.html">
   第十章 提升方法和加法树
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.1-Boosting-Methods.html">
     10.1 boosting方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.2-Boosting-Fits-an-Additive-Model.html">
     10.2 Boosting 拟合可加模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.3-Forward-Stagewise-Additive-Modeling.html">
     10.3 向前逐步加法建模
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.4-Exponential-Loss-and-AdaBoost.html">
     10.4 指数损失和AdaBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.5-Why-Exponential-Loss.html">
     10.5 为什么是指数损失？
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.6-Loss-Functions-and-Robustness.html">
     10.6 损失函数和鲁棒性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.7-Off-the-Shelf-Procedures-for-Data-Mining.html">
     10.7 数据挖掘的现货方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.8-Spam-Data.html">
     10.8 例子: 垃圾邮件
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.9-Boosting-Trees.html">
     10.9 Boosting 树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.10-Numerical-Optimization-via-Gradient-Boosting.html">
     10.10 Gradient Boosting的数值优化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.11-Right-Sized-Trees-for-Boosting.html">
     10.11 大小合适的boosting树
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.12-Regularization.html">
     10.12 正则化
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.13-Interpretation.html">
     10.13 解释性
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/10.14-Illustrations.html">
     10.14 例子
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10-Boosting-and-Additive-Trees/Bibliographic-Notes.html">
     10.15 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../11-Neural-Networks/11.0-Overview.html">
   第十一章 神经网络
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.1-Introduction.html">
     11.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.2-Projection-Pursuit-Regression.html">
     11.2 投影寻踪回归
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.3-Neural-Networks.html">
     11.3 神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.4-Fitting-Neural-Networks.html">
     11.4 拟合神经网络
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.5-Some-Issues-in-Training-Neural-Networks.html">
     11.5 训练神经网络的一些问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.6-Example-of-Simulated-Data.html">
     11.6 例子：模拟数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/11.7-Example-ZIP-Code-Data.html">
     11.7 例子：邮编数据
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../11-Neural-Networks/Bibliographic-Notes.html">
     11.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="12.0-Overview.html">
   第十二章 支持向量机与柔性判别分析
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="12.1-Introduction.html">
     12.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12.2-The-Support-Vector-Classifier.html">
     12.2 支持向量分类器
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     12.3 支持向量机和核
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12.4-Generalizing-Linear-Discriminant-Analysis.html">
     12.4 广义线性判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12.5-Flexible-Disciminant-Analysis.html">
     12.5 FDA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12.6-Penalized-Discriminant-Analysis.html">
     12.6 惩罚判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12.7-Mixture-Discriminant-Analysis.html">
     12.7 混合判别分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Bibliographic-Notes.html">
     12.8 文献笔记
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Computational-Considerations.html">
     计算上的考虑
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.0-Overview.html">
   第十三章 原型方法与最近邻方法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction.html">
     13.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.2-Prototype-Methods.html">
     13.2 原型方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.3-k-Nearest-Neighbor-Classifiers.html">
     13.3 k最近邻分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.4-Adaptive-Nearest-Neighbor-Methods.html">
     13.4 自适应最近邻方法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/13.5-Computational-Considerations.html">
     13.5 计算上的考虑
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../13-Prototype-Methods-and-Nearest-Neighbors/Bibliographic-Notes.html">
     13.6 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../14-Unsupervised-Learning/14.0-Overview.html">
   第十四章 非监督学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.1-Introduction.html">
     14.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.2-Association-Rules.html">
     14.2 关联规则
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.3-Cluster-Analysis.html">
     14.3 聚类分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.4-Self-Organizing-Maps.html">
     14.4 自组织图
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.5-Principal-Components-Curves-and-Surfaces.html">
     14.5 主成分，主曲线和主曲面
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.6-Non-negative-Matrix-Factorization.html">
     14.6 非负矩阵分解
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.7-Independent-Component-Analysis-and-Exploratory-Projection-Pursuit.html">
     14.7 独立成分分析和探索投影寻踪
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.8-Multidimensional-Scaling.html">
     14.8 多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.9-Nonlinear-Dimension-Reduction-and-Local-Multidimensional-Scaling.html">
     14.9 非线性降维和局部多维缩放
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/14.10-The-Google-PageRank-Algorithm.html">
     14.10 谷歌的PageRank算法
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../14-Unsupervised-Learning/Bibliographic-Notes.html">
     14.11 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../15-Random-Forests/15.0-Overview.html">
   第十五章 随机森林
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.1-Introduction.html">
     15.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.2-Definition-of-Random-Forests.html">
     15.2 随机森林的定义
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.3-Details-of-Random-Forests.html">
     15.3 随机森林的细节
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/15.4-Analysis-of-Random-Forests.html">
     15.4 随机森林的分析
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../15-Random-Forests/Bibliographic-Notes.html">
     15.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../16-Ensemble-Learning/16.0-Overview.html">
   第十六章 集成学习
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.1-Introduction.html">
     16.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.2-Boosting-and-Regularization-Paths.html">
     16.2 Boosting 和正则化路径
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/16.3-Learning-Ensembles.html">
     16.3 学习集成
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../16-Ensemble-Learning/Bibliographic-Notes.html">
     16.4 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../17-Undirected-Graphical-Models/17.0-Overview.html">
   第十七章 马尔科夫随机场
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.1-Introduction.html">
     17.1 导言
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.2-Markov-Graphs-and-Their-Properties.html">
     17.2 马尔科夫图及其性质
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.3-Undirected-Graphical-Models-for-Continuous-Variables.html">
     17.3 连续变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/17.4-Undirected-Graphical-Models-for-Discrete-Variables.html">
     17.4 离散变量的无向图模型
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../17-Undirected-Graphical-Models/Bibliographic-Notes.html">
     17.5 文献笔记
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../18-High-Dimensional-Problems/18.0-Overview.html">
   第十八章 高维度问题
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.1-When-p-is-Much-Bigger-than-N.html">
     18.1 当p远大于N
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.2-Diagonal-Linear-Discriminant-Analysis-and-Nearest-Shrunken-Centroids.html">
     18.2 对角线性判别分析和最近收缩重心
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.3-Linear-Classifiers-with-Quadratic-Regularization.html">
     18.3 二次正则化的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.4-Linear-Classifiers-with-L1-Regularization.html">
     18.4
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     正则的线性分类器
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.5-Classification-When-Features-are-Unavailable.html">
     18.5 当特征不可用时的分类
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.6-High-Dimensional-Regression.html">
     18.6 高维回归: 有监督的主成分
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/18.7-Feature-Assessment-and-the-Multiple-Testing-Problem.html">
     18.7 特征评估和多重检验问题
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../18-High-Dimensional-Problems/Bioliographic-Notes.html">
     18.8 文献笔记
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/ElementsOfStatisticalLearning/issues/new?title=Issue%20on%20page%20%2F12-Support-Vector-Machines-and-Flexible-Discriminants/12.3-Support-Vector-Machines-and-Kernels.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#svm">
   用于分类的 SVM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   SVM 作为惩罚的方法
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   函数估计和再生核
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   SVM 和维数灾难
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id12">
   SVM 分类器的路径算法
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id14">
   用于回归的支持向量机
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id16">
   回归和核
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id17">
   讨论
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>12.3 支持向量机和核<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>至今所描述的支持向量机是在输入特征空间中寻找线性边界。和其他的线性方法一样，我们可以用基展开的方式来增大特征空间，从而让这一过程更灵活，如多项式或者样条（<a class="reference internal" href="../05-Basis-Expansions-and-Regularization/5.1-Introduction.html"><span class="doc std std-doc">第 5 章</span></a>）。一般地，在增长的空间的线性边界能够达到更好的训练类别分割的效果，进而转换为原始空间中的非线性边界。一旦选定基函数 <span class="math notranslate nohighlight">\(h_m(x),m=1,\ldots,M\)</span>，则该过程和之前一样。我们采用输入特征 <span class="math notranslate nohighlight">\(h(x_i)=(h_1(x_i),h_2(x_i),\ldots,h_M(x_i)),i=1,\ldots,N\)</span> 拟合 SV 分类器，并且得到（非线性）函数 <span class="math notranslate nohighlight">\(\hat f(x)=h(x)^T\hat\beta+\hat\beta_0\)</span>。分类器和之前一样，为 <span class="math notranslate nohighlight">\(\hat G(x)=\mathrm{sign}(\hat f(x))\)</span>。</p>
<p><strong>支持向量机 (support vector machine)</strong> 分类器是这个想法的拓展，其中增长空间的维数允许非常大，有时甚至可以是无限维的。计算似乎会变得难以承受。而且似乎当有了充分多的基函数，数据会变得可分，同时也会发生过拟合。我们首先介绍 SVM 怎样处理这些问题。接着我们将看到实际中 SVM 分类器是采用特定的准则和正则化形式来解决函数拟合问题，而且这是包含第 5 章的光滑样条的更大类别的问题的一部分。读者可能希望参照 <span class="xref myst">5.8 节</span>，它提供了背景材料并且与接下来的两节会有一定的重复。</p>
<div class="section" id="svm">
<h2>用于分类的 SVM<a class="headerlink" href="#svm" title="Permalink to this headline">¶</a></h2>
<p>!!! note “Recall”
$<span class="math notranslate nohighlight">\(
    L_P=\frac{1}{2}\Vert\beta\Vert^2+C\sum\limits_{i=1}^N\xi_i-\sum\limits_{i=1}^N\alpha_i[y_i(x_i^T\beta+\beta_0)-(1-\xi_i)]-\sum\limits_{i=1}^{N}\mu_i\xi_i\tag{12.9}
    \)</span><span class="math notranslate nohighlight">\(
    \)</span><span class="math notranslate nohighlight">\(
    \begin{align*}
    \beta&amp;=\sum\limits_{i=1}^N\alpha_iy_ix_i\tag{12.10}\\
    0&amp;= \sum\limits_{i=1}^N\alpha_iy_i \tag{12.11}\\
    \alpha_i&amp;=C-\mu_i,\;\forall i,\tag{12.12}
    \end{align*}
    \)</span><span class="math notranslate nohighlight">\(
    \)</span><span class="math notranslate nohighlight">\(
    L_D = \sum\limits_{i=1}^N\alpha_i-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{i'=1}^N\alpha_i\alpha_{i'}y_iy_{i'}x_i^Tx_{i'}\tag{12.13}
    \)</span>$</p>
<p>我们可以用一种特殊的方式来表示优化问题 式（ 12.9 ） 和它的解，这种方式只通过内积来涉及输入特征。我们对这个变换后的特征向量 <span class="math notranslate nohighlight">\(h(x_i)\)</span> 直接进行这样的操作。我们将看到对于特定选择的 <span class="math notranslate nohighlight">\(h\)</span>，这些内积可以很方便地进行计算。</p>
<p>Lagrange 对偶函数 式（ 12.13 ） 有如下形式：</p>
<div class="math notranslate nohighlight">
\[
L_D=\sum\limits_{i=1}^N\alpha_i-\frac{1}{2}\sum\limits_{i=1}^N\sum\limits_{i'=1}^N\alpha_i\alpha_{i'}y_iy_{i'}\langle h(x_i), h(x_{i'})\rangle\tag{12.19}
\]</div>
<p>从 式（ 12.10 ） 中我们看到解函数 <span class="math notranslate nohighlight">\(f(x)\)</span> 可以写成</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
f(x)&amp;=h(x)^T\beta+\beta_0\\
&amp;=\sum\limits_{i=1}^N\alpha_iy_i\langle h(x),h(x_i) \rangle+\beta_0\tag{12.20}
\end{align*}
\end{split}\]</div>
<p>和之前一样，给定 <span class="math notranslate nohighlight">\(\alpha_i\)</span>，当 <span class="math notranslate nohighlight">\(0 &lt; \alpha_i &lt; C\)</span> 时，<span class="math notranslate nohighlight">\(\beta_0\)</span> 可以通过对任意（或所有）<span class="math notranslate nohighlight">\(x_i\)</span> 求解 <span class="math notranslate nohighlight">\(y_if(x_i)=1\)</span> 来确定。</p>
<p>所以 式（ 12.19 ） 和 式（ 12.20 ） 仅仅通过内积涉及 <span class="math notranslate nohighlight">\(h(x)\)</span>。实际上，我们根本不需要明确变换关系 <span class="math notranslate nohighlight">\(h(x)\)</span>，而仅仅要求知道在转换后的空间中计算内积的核函数</p>
<div class="math notranslate nohighlight">
\[
K(x,x')=\langle h(x), h(x') \rangle\tag{12.21}
\]</div>
<p><span class="math notranslate nohighlight">\(K\)</span> 应该是一个对称的(半)正定函数：见<a class="reference external" href="../05-Basis-Expansions-and-Regularization/5.8-Regularization-and-Reproducing-Kernel-Hibert-Spaces/index.html#_1">5.8.1节</a>。</p>
<p>在 SVM 中有三种流行的 <span class="math notranslate nohighlight">\(K\)</span> 可以选择</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{rl}
d\text{ 阶多项式：} &amp; K(x,x')=(1+\langle x,x' \rangle)^d\\
\text{径向基：} &amp; K(x, x')=\exp(-\gamma \Vert x-x'\Vert^2)\tag{12.22}\\
\text{神经网络：} &amp; K(x,x')=\tanh(\kappa_1\langle x,x' \rangle+\kappa_2)\\
\end{array}
\end{split}\]</div>
<p>考虑含有两个输入变量 <span class="math notranslate nohighlight">\(X_1\)</span> 和 <span class="math notranslate nohighlight">\(X_2\)</span> 的特征空间，以及 2 阶的多项式核。则</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{ll}
K(x,x')&amp;=(1+\langle X,X' \rangle)^2\\
&amp;=(1+X_1X_1'+X_2X_2')^2\\
&amp;=1+2X_1X_1'+2X_2X_2'+(X_1X_1')^2+(X_2X_2')^2+2X_1X_1'X_2X_2'\tag{12.23}
\end{array}
\end{split}\]</div>
<p>则 <span class="math notranslate nohighlight">\(M=6\)</span>，而且如果我们选择 <span class="math notranslate nohighlight">\(h_1(X)=1,h_2(X)=\sqrt{2}X_1,h_3(X)=\sqrt{2}X_2,h_4(X)=X_1^2,h_5(X)=X_2^2\)</span>，以及 <span class="math notranslate nohighlight">\(h_6(X)=\sqrt{2}X_1X_2\)</span>，则<span class="math notranslate nohighlight">\(K(X,X')=\langle h(X),h(X')\rangle\)</span>。</p>
<p>从 式（ 12.20 ） 我们可以看出该解可以写成</p>
<div class="math notranslate nohighlight">
\[
\hat f(x)=\sum\limits_{i=1}^N\hat \alpha_iy_iK(x,x_i)+\hat\beta_0\tag{12.24}
\]</div>
<p>在增广的特征空间中，参数 <span class="math notranslate nohighlight">\(C\)</span> 的角色更加清晰，因为在这里经常会存在完美的分割。较大的 <span class="math notranslate nohighlight">\(C\)</span> 会抑制任何正的 <span class="math notranslate nohighlight">\(\xi_i\)</span>，并且得到在原始特征空间中过拟合的弯弯曲曲的边界；较小的 <span class="math notranslate nohighlight">\(C\)</span> 值会鼓励较小的 <span class="math notranslate nohighlight">\(\Vert\beta\Vert\)</span>，反过来会导致 <span class="math notranslate nohighlight">\(f(x)\)</span> 以及边界更加地光滑。图 12.3 显示了应用到第 2 章中的混合模型的两个支持向量机。两种情形下的正则化参数都是为了实现更好的测试误差。对该例子而言，径向基得到类似贝叶斯最优边界；与图 2.5 进行比较。</p>
<p><img alt="" src="../_images/fig12.3.png" /></p>
<p>在支持向量的早期研究中，有断言称，支持向量机的核性质是唯一的，并且允许对维数灾难进行巧妙地处理。</p>
<p>这些断言都不是正确的，我们将在下一节详细讨论这些问题。</p>
<p>!!! note “weiya 注：SVM with Gaussian kernel”
知乎上看到这样一个问题，<a class="reference external" href="https://www.zhihu.com/question/267999284">支持向量机高斯核方法？ - 知乎</a>，觉得很有意思，具体探索过程见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/77">Issue 77: SVM with Gaussian kernel</a>。</p>
</div>
<div class="section" id="id2">
<h2>SVM 作为惩罚的方法<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>对于 <span class="math notranslate nohighlight">\(f(x)=h(x)^T\beta+\beta_0\)</span>，考虑下列的优化问题</p>
<div class="math notranslate nohighlight">
\[
\underset{\beta_0,\beta}{\min}\sum\limits_{i=1}^N[1-y_if(x_i)]_++\frac{\lambda}{2}\Vert\beta\Vert^2\tag{12.25}
\]</div>
<p>其中下标“<span class="math notranslate nohighlight">\(+\)</span>”表示正的部分。上式是“损失+惩罚”的形式，这在函数估计中是个很熟悉的范例。可以很简单地证明当 <span class="math notranslate nohighlight">\(\lambda=1/C\)</span> 时，式（ 12.25 ） 的解与 式（ 12.8 ） 的解相同（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/78">练习 12.1</a>）。</p>
<p>!!! note “Recall”
$$
\begin{array}{ll}
\underset{\beta,\beta_0}{\min}&amp;;\frac{1}{2}\Vert\beta\Vert^2+C\sum\limits_{i=1}^N\xi_i\
s.t.&amp; \xi_i\ge 0,y_i(x_i^T\beta+\beta_0)\ge 1-\xi_i,\forall i
\end{array}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\tag{12.8}
$$
</pre></div>
</div>
<p>!!! info “weiya 注：Ex. 12.1”
已解决。详细证明过程见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/78">Issue 78: Ex. 12.1</a></p>
<p>对“hinge”损失函数 <span class="math notranslate nohighlight">\(L(y,f)=[1-yf]\_+\)</span> 的检验表明，它用于二类别分类是很合理的。图 12.4 将之与逻辑斯蒂回归的 <strong>对数似然损失(log-likelihood loss)</strong> 进行比较，以及平方误差损失及其的变种。负对数似然或者二项偏差和 SVM 损失有着类似的尾分布，对 margin 中的点赋予零惩罚，对在错误一侧的点赋予线性惩罚。另一方面，平方误差给出了平方惩罚，在各自margin中的点对模型也有很强的影响。<strong>squared hinge</strong>损失<span class="math notranslate nohighlight">\(L(y,f)=[1-yf]_+^2\)</span>类似平方损失，但它对于各自margin中的点惩罚为0。它在左尾中平方增长，而且对于误分类的观测点，没有hinge或者偏差稳健。最近 Rosset and Zhu (2007)<a class="footnote-reference brackets" href="#id21" id="id3">1</a> 提出 <strong>squared hinge 损失的“Huberized”版本</strong>，在 <span class="math notranslate nohighlight">\(yf=-1\)</span> 处将其平滑转换为了线性损失。</p>
<p>!!! note “weiya 注：Huberized squared hinge loss”
文献Rosset and Zhu (2007)<a class="footnote-reference brackets" href="#id21" id="id4">1</a>中提出的 Huberized squared hinge loss 的表述更一般，可以在任意 <span class="math notranslate nohighlight">\(t &lt; 1\)</span> 处进行 Huberized，文献截图如下：
<img alt="" src="../img/12/ref_eq_11.png" />
<img alt="" src="../img/12/ref_eq_15.png" /></p>
<p><img alt="" src="../_images/fig12.4.png" /></p>
<p>我们可以在总体水平上用估计量来区分这些损失函数。我们考虑最小化 <span class="math notranslate nohighlight">\(\E L(Y,f(x)\)</span>。</p>
<p><img alt="" src="../_images/tab12.1.png" /></p>
<p>表 12.1 总结了这些结果。然而 hinge 损失估计了分类器 <span class="math notranslate nohighlight">\(G(x)\)</span> 本身，而其他的都估计类别后验概率的某个变换。“Huberized” square hinge损失有着与逻辑斯蒂回归相同的吸引人的性质（光滑的损失函数，估计概率），以及 SVM 的 hinge 损失的良好性质（支撑点）。</p>
<p>式 式（ 12.25 ） 将 SVM 看成正则化的函数估计问题，其中线性展开 <span class="math notranslate nohighlight">\(f(x)=\beta_0+h(x)^T\beta\)</span> 的系数向 0 收缩（除了常值）。如果 <span class="math notranslate nohighlight">\(h(x)\)</span> 表示有相同的序结构（比如按粗糙程度排序）的 <strong>分层基 (hierarchical basis)</strong>，如果在向量 <span class="math notranslate nohighlight">\(h\)</span> 中更粗糙的元素 <span class="math notranslate nohighlight">\(h_j\)</span> 有更小的范数，则均一化收缩是更有意义的。</p>
<p>!!! question
什么是 hierarchical basis？探索过程见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/79">Issues 79: hierarchical basis</a></p>
<p>除了平方误差，表 12.1 中所有的损失函数，都称之为 “margin maximizing loss-functions”（Rosset et al., 2004b<a class="footnote-reference brackets" href="#id22" id="id5">2</a>）。这意味着如果数据是可分的，则当 <span class="math notranslate nohighlight">\(\lambda\rightarrow 0\)</span> 时，<span class="math notranslate nohighlight">\(\hat\beta_\lambda\)</span> 的极限定义为最优分离超平面。</p>
<p>!!! note “weiya 注：原书脚注”
对于可分数据的逻辑斯蒂回归，<span class="math notranslate nohighlight">\(\hat\beta_\lambda\)</span> 发散，但是 <span class="math notranslate nohighlight">\(\hat\beta_\lambda/\Vert\hat\beta_\lambda\Vert\)</span> 收敛到最优分离超平面。(Why?)</p>
</div>
<div class="section" id="id6">
<h2>函数估计和再生核<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>这里我们用 <strong>再生核希尔伯特空间 (reproducing kernel Hilbert spaces)</strong> 中的函数估计来描述 SVM，其中有许多的核性质。这里的内容在<span class="xref myst"> 5.8 节</span>有详细讨论。这里给出支持向量分类器的另一个视角，并且帮助理清它如何工作。</p>
<p>假设基 <span class="math notranslate nohighlight">\(h\)</span> 来自与正定核 <span class="math notranslate nohighlight">\(K\)</span> 的（可能是有限的）特征展开，</p>
<div class="math notranslate nohighlight">
\[
K(x, x')=\sum\limits_{m=1}^{\infty}\phi_m(x)\phi_m(x')\delta_m\tag{12.26}
\]</div>
<p>!!! note “weiya 注：Mercer’s theorem”
式（ 12.26 ） 的分解由 Mercer’s theorem 保证
<img alt="" src="../img/12/wiki_mercer.PNG" />
图片来源: <a class="reference external" href="https://en.wikipedia.org/wiki/Mercer%27s_theorem">Wiki: Mercer’s theorem</a></p>
<p>并且 <span class="math notranslate nohighlight">\(h_m(x)=\sqrt{\delta_m}\phi_m(x)\)</span>。则当<span class="math notranslate nohighlight">\(\theta_m=\sqrt{\delta_m}\beta_m\)</span>，我们可以把 式（ 12.25 ） 写成</p>
<div class="math notranslate nohighlight">
\[
\underset{\beta_0, \theta}{\min}\sum\limits_{i=1}^N\Big[
    1-y_i(\beta_0+\sum\limits_{m=1}^{\infty}\theta_m\phi_m(x_i))
    \Big]_+ + \frac{\lambda}{2}\sum\limits_{m=1}^{\infty}\frac{\theta_m^2}{\delta_m}
    \tag{12.27}
\]</div>
<p>!!! note “Recall”
$<span class="math notranslate nohighlight">\(
    \underset{\{c_j\}_1^\infty}{\min}\Big[\sum\limits_{i=1}^NL(y_i, \sum\limits_{j=1}^\infty c_j\phi_j(x_i))+\lambda \sum\limits_{j=1}^\infty c_j^2/\gamma_j\Big]\tag{5.49}
    \)</span><span class="math notranslate nohighlight">\(
    \)</span><span class="math notranslate nohighlight">\(
    \underset{\alpha_0,\alpha}{\min}\Big\{
    \sum\limits_{i=1}^N[1-y_if(x_i)]_+ + \frac{\lambda}{2}\alpha^TK\alpha
    \Big\}\tag{5.67}
    \)</span>$</p>
<p>现在 式（ 12.27 ） 与 <span class="xref myst">5.8 节</span>中的 式（ 5.49 ） 形式相同，并且再生希尔伯特空间的理论保证了形如下式的有限维的解</p>
<div class="math notranslate nohighlight">
\[
f(x)=\beta_0+\sum\limits_{i=1}^N\alpha_iK(x, x_i)\tag{12.28}
\]</div>
<p>特别地，我们看到存在等价的优化准则【5.8.2节的式 式（ 5.67 ）；也可以参见 Wahba et al. (2000)<a class="footnote-reference brackets" href="#id23" id="id7">3</a>】</p>
<div class="math notranslate nohighlight">
\[
\underset{\beta_0, \alpha}{\min}\sum\limits_{i=1}^N(1-y_if(x_i))_++\frac{\lambda}{2}\alpha^TK\alpha\tag{12.29}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(K\)</span> 是所有成对训练特征的核的赋值（<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/80">练习 12.2</a>）。</p>
<p>!!! info “weiya 注：Ex. 12.2”
已解决，详见 <a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/80">Issue 80: Ex. 12.2</a>，欢迎交流讨论。</p>
<p>这些模型非常一般，举个例子，包含第 5 和第 9 章中讨论的整个光滑样条族，可加和交叉样条模型，更多细节参见 Wahba (1990)<a class="footnote-reference brackets" href="#id24" id="id8">4</a> 和 Hastie and Tibshirani (1990)<a class="footnote-reference brackets" href="#id25" id="id9">5</a>。它们可以更一般地表示为</p>
<div class="math notranslate nohighlight">
\[
\underset{f\in{\mathcal H}}{\min}\sum\limits_{i=1}^N[1-y_if(x_i)]_+ + \lambda J(f)\tag{12.30}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mathcal H\)</span> 是 <strong>函数的结构空间(the structured space of functions)</strong>，并且 <span class="math notranslate nohighlight">\(J(f)\)</span> 是在该空间上合适的正则器。举个例子，假设<span class="math notranslate nohighlight">\(\mathcal H\)</span>是可加函数 <span class="math notranslate nohighlight">\(f(x)=\sum_{j=1}^pf_j(x_j)\)</span>，并且<span class="math notranslate nohighlight">\(J(f)=\sum_j\int\\{f^{\prime\prime}\_j(x_j)\\}^2dx_j\)</span>。则 式（ 12.30 ） 的解为可加三次样条，并且有核表示 <span class="math notranslate nohighlight">\(K(x,x')=\sum_{j=1}^pK_j(x_j,x_j')\)</span>。每个 <span class="math notranslate nohighlight">\(K_j\)</span> 是关于 <span class="math notranslate nohighlight">\(x_j\)</span> 的合适的单变量光滑样条的核(Wahba, 1990)<a class="footnote-reference brackets" href="#id24" id="id10">4</a>。</p>
<p>反过来，这个讨论也表明，举个例子，任意 式（ 12.22 ） 中提到的核都可以和凸损失函数一起使用，并且将会得到形如 式（ 12.28 ） 的有限维表示。图 12.5 采用和图 12.3 同样的核函数，除了采用二项对数似然作为损失函数。因此拟合函数是 log-odds 的估计</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat f(x) &amp;=\log \frac{\hat\pr (Y=+1\mid x)}{\hat\pr (Y=-1\mid x)}\\
&amp;=\hat\beta_0+\sum\limits_{i=1}^N\hat\alpha_iK(x,x_i)\tag{12.31}
\end{align*}
\end{split}\]</div>
<p>或者反过来，我们得到类别概率的估计</p>
<div class="math notranslate nohighlight">
\[
\hat\pr(Y=+1\mid x)=\frac{1}{1+e^{-\hat\beta_0-\sum_{i=1}^N\hat\alpha_iK(x,x_i)}}\tag{12.32}
\]</div>
<p>这些拟合的模型在形状和表现效果上都非常相似。<span class="xref myst">5.8节</span>给出了例子以及更多的细节。</p>
<p>对于 SVM 也是一样，<span class="math notranslate nohighlight">\(N\)</span> 个 <span class="math notranslate nohighlight">\(\alpha_i\)</span> 中的一部分可以为 0（非支持点）。在图 12.3 的两个例子中，这些比例分别为 <span class="math notranslate nohighlight">\(42\%\)</span> 和 <span class="math notranslate nohighlight">\(45\%\)</span>。这是准则 式（ 12.25 ） 的第一部分分段线性的本质。（在训练数据上）类别重叠越少，这个比例越大。降低 <span class="math notranslate nohighlight">\(\lambda\)</span> 一般会降低重叠（允许更灵活的<span class="math notranslate nohighlight">\(f\)</span>）。较少的支持点意味着 <span class="math notranslate nohighlight">\(\hat f(x)\)</span> 可以更快地进行赋值，这在查找时很重要。当然，过分降低重叠会导致泛化能力太差。</p>
</div>
<div class="section" id="id11">
<h2>SVM 和维数灾难<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<p>在这一节，我们回答“SVMs是否存在维数灾难”的问题。注意到在展开式 式（ 12.23 ） 中，我们不允许在幂和乘积的空间中有完全一般的内积。举个例子，所有形如 <span class="math notranslate nohighlight">\(2X_jX_j'\)</span> 赋予相同的权重，并且核不能自适应地集中到某子空间中。</p>
<p>!!! note
基于下文的比较，特别是高阶多项式与低阶多项式，这里应当是指 SVM 中的核不允许退化，比如某个参数为 0，并不具有像 MARS 等方法的自适应能力。</p>
<p>如果特征的数量 <span class="math notranslate nohighlight">\(p\)</span> 很大，但是类别分类仅仅发生在 <span class="math notranslate nohighlight">\(X_1\)</span> 和 <span class="math notranslate nohighlight">\(X_2\)</span> 张成的子空间，这个核不会简单地找到其结构并且需要在很多维度下搜寻。必须将子空间的信息考虑进核中，也就是，忽略 <strong>除了</strong> 前两个的输入。如果有信息能够得到先验，大多数统计学习会变得很简单。自适应方法的主要目标是发现类似的结构。</p>
<p>我们用一个例子来支持上面的陈述。我们在两个类别中产生 100 个观测值。第一类有 4 个标准正态独立特征 <span class="math notranslate nohighlight">\(X_1,X_2,X_3,X_4\)</span>。第二类也有四个标准正态独立特征，但是条件为 <span class="math notranslate nohighlight">\(9\le \sum X_j^2\le 16\)</span>。这是个相对简单的问题。同时考虑第二个更难的问题，用 6 个标准高斯噪声特征作为增广特征。因此在四维子空间中第二类完全包围了第一类，类似橘子皮包围着橘子。该问题的贝叶斯误差率为 0.029（不考虑维数）。我们产生 1000 个测试观测量来比较不同的过程。在有无噪声特征的情况下，平均测试误差在 50 个模拟中得到，如表 12.2 所示。</p>
<p><img alt="" src="../_images/tab12.2.png" /></p>
<p>表中第一行在原特征空间中使用支持向量分类器。第 2 到第 4 行表示使用 2，5 和 10 维多项式核的支持向量机。对于所有的支持向量过程，我们选择代价参数 <span class="math notranslate nohighlight">\(C\)</span> 来最小化测试误差，使得对这个方法尽可能公平。第 5 行采用可加模型的 BRUTO 算法（Hastie and Tibshirani (1990)），通过最小二乘对 <span class="math notranslate nohighlight">\((-1,+1)\)</span> 响应变量拟合可加样条模型。第 6 行采用 MARS（多变量自适应回归样条）允许任意阶的交叉影响，在第 9 章中有介绍；同样地，其与 SVM/poly 10 进行比较。BRUTO 和 MARS 算法均有能力忽略冗余变量。第 5 和第 6 行并不采用测试误差来选择光滑参数。</p>
<p>!!! info
笔记<span class="xref myst">模拟：Tab. 12.2</span>重现了表12.2的结果。</p>
<p>在原始特征空间中，超平面不能分离类别，而且支持向量分类器（第一行）表现很差。多项式支持向量机在测试误差率方面取得显著改善，但同时也被6个噪声特征严重影响。这对核的选择非常敏感：二阶多项式核（第2行）表现最好，因为真实的判别边界就是二阶多项式。然而，高阶多项式（第3和第4行）表现很差。BRUTO 表现得很好，因为其边界是可加的。BRUTO 和 MARS 自适应得很好：因为噪声的存在，所以它们的表现不会退化太大。</p>
</div>
<div class="section" id="id12">
<h2>SVM 分类器的路径算法<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h2>
<p>SVM 分类器的正则化参数是代价参数 <span class="math notranslate nohighlight">\(C\)</span>，或者 式（ 12.25 ） 中 <span class="math notranslate nohighlight">\(\lambda\)</span> 的倒数。通常用法是将 <span class="math notranslate nohighlight">\(C\)</span> 设得比较高，这通常会过拟合分类器。</p>
<p>图 12.6 展示了混合数据的测试误差作为 <span class="math notranslate nohighlight">\(C\)</span> 的函数的变化图像，其中采用不同的径向核参数 <span class="math notranslate nohighlight">\(\gamma\)</span>。</p>
<p><img alt="" src="../_images/fig12.6.png" /></p>
<p>当 <span class="math notranslate nohighlight">\(\gamma=5\)</span> 时(narrow peaked kernels)，需要最重的正则化（即较小 <span class="math notranslate nohighlight">\(C\)</span>）。当 <span class="math notranslate nohighlight">\(\gamma=1\)</span>时（图 12.3 所用的值），需要中等大小的 <span class="math notranslate nohighlight">\(C\)</span>。显然在这些情形中，我们需要确定合适的 <span class="math notranslate nohighlight">\(C\)</span>，或许可以通过交叉验证实现。这里我们描述一种路径算法（受<span class="xref myst">3.8节</span>的启发），可以用来有效拟合改变 <span class="math notranslate nohighlight">\(C\)</span> 得到的 SVM 模型序列。</p>
<p>使用 式（ 12.25 ） 的损失+惩罚的形式以及图 12.4 中的损失函数是很方便的。这可以得到在给定 <span class="math notranslate nohighlight">\(\lambda\)</span> 处 <span class="math notranslate nohighlight">\(\beta\)</span> 的解：</p>
<div class="math notranslate nohighlight">
\[
\beta_\lambda = \frac{1}{\lambda}\sum\limits_{i=1}^N\alpha_iy_ix_i\tag{12.33}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\alpha_i\)</span> 仍然是 Lagrange 乘子，但是在这种情形下它们都在 [0, 1] 中取值。</p>
<p>图 12.7 说明了这个过程。</p>
<p><img alt="" src="../_images/fig12.7.png" /></p>
<p>可以看到 KKT 优化条件表明点 <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> 落在三个不同的区域中：</p>
<ul class="simple">
<li><p>正确分类且落在各自 margins 之外。它们满足 <span class="math notranslate nohighlight">\(y_if(x_i) &gt; 1\)</span>，并且 Lagrange 乘子满足 <span class="math notranslate nohighlight">\(\alpha_i=0\)</span>。比如图中橘黄色的点 8，9和 11，以及蓝色点 1 和 4。</p></li>
<li><p>正好落在各自 margin 上，满足 <span class="math notranslate nohighlight">\(y_if(x_i)=1\)</span>，其中 Lagrange 乘子 <span class="math notranslate nohighlight">\(\alpha_i\in [0, 1]\)</span>。比如图中橘黄色点7，以及蓝色点2和6。</p></li>
<li><p>在 margin 之内，满足 <span class="math notranslate nohighlight">\(y_if(x_i) &lt; 1\)</span>，且 <span class="math notranslate nohighlight">\(\alpha_i=1\)</span>。比如蓝色点3和5，以及橘黄色点10和12。</p></li>
</ul>
<p>!!! note “weiya注：KKT条件及三种划分”
这在上一节也有讨论，参见<span class="xref myst">12.2 支持向量分类器</span>。唯一区别在于这里<span class="math notranslate nohighlight">\(\alpha_i\)</span>的分界点为1，而上一节为<span class="math notranslate nohighlight">\(C\)</span>。但这很好理解，因为用<span class="math notranslate nohighlight">\(\frac{1}{\lambda}\)</span>作为参数，相对于(12.8)，除以了<span class="math notranslate nohighlight">\(C\)</span>，更多细节可以参见<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/78">Issue 78: Ex. 12.1</a></p>
<p>路径算法的思想如下。选择较大的 <span class="math notranslate nohighlight">\(\lambda\)</span> 来初始化，则 margin 的宽度 <span class="math notranslate nohighlight">\(1/\Vert \beta_\lambda\Vert\)</span> 较宽，所有点都在各自margin 中且 <span class="math notranslate nohighlight">\(\alpha_i=1\)</span>。当 <span class="math notranslate nohighlight">\(\lambda\)</span> 减小时，<span class="math notranslate nohighlight">\(\frac{1}{\Vert\beta_\lambda\Vert}\)</span> 减小，则 margin 变窄。有些点会各自margin 内移到 margin 之外，并且它们 <span class="math notranslate nohighlight">\(\alpha_i\)</span> 从 1 变为 0。由 <span class="math notranslate nohighlight">\(\alpha_i(\lambda)\)</span> 的连续性，这些点在转移的过程中会在 margin 上停留(linger)。由 式（ 12.33 ），我们看到 <span class="math notranslate nohighlight">\(\alpha_i=1\)</span> 的点对 <span class="math notranslate nohighlight">\(\beta(\lambda)\)</span> 的贡献是固定的，而那些 <span class="math notranslate nohighlight">\(\alpha_i=0\)</span> 的点没有贡献。所以当 <span class="math notranslate nohighlight">\(\lambda\)</span> 减小时，改变量是那些在 margin 边界上的点，它们满足 <span class="math notranslate nohighlight">\(\alpha_i\in[0,1]\)</span>。因为这些点都满足 <span class="math notranslate nohighlight">\(y_if(x_i)=1\)</span>，这一小部分的线性等式的解决定了 <span class="math notranslate nohighlight">\(\alpha_i(\lambda)\)</span> 以及由其导致的 <span class="math notranslate nohighlight">\(\beta(\lambda)\)</span> 在这些转移过程中的变化。对于 <span class="math notranslate nohighlight">\(\alpha_i(\lambda)\)</span> 的每个值，这些解是分段线性路径。断点仅发生在穿过 margin 的时候。图 12.7 的右面板展示了 <span class="math notranslate nohighlight">\(\alpha_i(\lambda)\)</span> 在左面板例子中的图象。</p>
<p>尽管我们只介绍了线性 SVMs 的方法，但是对于非线性模型，想法完全一样，只需要将 式（ 12.33 ） 换成</p>
<div class="math notranslate nohighlight">
\[
f_\lambda(x) = \frac{1}{\lambda}\sum\limits_{i=1}^N\alpha_iy_iK(x,x_i)\tag{12.34}
\]</div>
<p>细节可以在 Hastie et al. (2004)<a class="footnote-reference brackets" href="#id26" id="id13">6</a>中找到。CRAN上<code class="docutils literal notranslate"><span class="pre">R</span></code>包<code class="docutils literal notranslate"><span class="pre">svmpath</span></code>可以用来拟合这些模型。</p>
</div>
<div class="section" id="id14">
<h2>用于回归的支持向量机<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h2>
<p>在这一节，我们将要展示 SVMs 如何用于定量响应变量的回归问题，方法继承了 SVM 分类器的某些性质。我们首先讨论线性回归模型</p>
<div class="math notranslate nohighlight">
\[
f(x)=x^T\beta+\beta_0\tag{12.35}
\]</div>
<p>接下来处理非线性的推广情形。为了估计 <span class="math notranslate nohighlight">\(\beta\)</span>，我们考虑下式的最小化</p>
<div class="math notranslate nohighlight">
\[
H(\beta, \beta_0) = \sum\limits_{i=1}^N V(y_i-f(x_i)) + \frac{\lambda}{2}\Vert \beta\Vert^2\tag{12.36}
\]</div>
<p>其中</p>
<div class="math notranslate nohighlight">
\[\begin{split}
V_{\epsilon}(r)=
\left\{
\begin{array}{ll}
0 &amp; \text{if}\; \vert r\vert &lt; \epsilon\\
\vert r\vert -\epsilon, &amp;\text{otherwise}
\end{array}
\right.
\tag{12.37}
\end{split}\]</div>
<p>这是一种“<span class="math notranslate nohighlight">\(\epsilon\)</span>-insensitive”的误差度量，它忽略了小于 <span class="math notranslate nohighlight">\(\epsilon\)</span> 的误差（图 12.8 的左图）。</p>
<p><img alt="" src="../_images/fig12.8.png" /></p>
<p>这一点类似支持向量机在分类问题中的设定，处于判别边界正确一侧的点以及远离边界的点在优化过程中都被忽略了。在回归中，这些“low error”的点是残差较小的点。</p>
<p>将这个误差度量与统计中鲁棒回归中采用的误差度量比较是很有趣的。图 12.8 的右图展示了鲁棒回归中误差度量的最受欢迎的形式(Huber (1964)<a class="footnote-reference brackets" href="#id27" id="id15">7</a>)：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
V_{H}(r)=
\left\{
\begin{array}{ll}
r^2/2 &amp; \text{if}\; \vert r\vert \le c\\
c\vert r\vert -c^2/2, &amp;\vert r\vert &gt; c
\end{array}
\right.
\tag{12.38}
\end{split}\]</div>
<p>当绝对残差大于预先给定的某个常值 <span class="math notranslate nohighlight">\(c\)</span> 时，函数将观测的贡献从二次降至一次。这使得拟合对离群点更加不敏感。支持向量误差衡量 式（ 12.37 ） 也有线性尾 (tail)（大于 <span class="math notranslate nohighlight">\(\epsilon\)</span>），但是除此之外，它压缩了小偏差情形下观测点的贡献。</p>
<p>如果 <span class="math notranslate nohighlight">\(\hat\beta,\; \hat\beta_0\)</span> 是 <span class="math notranslate nohighlight">\(H\)</span> 的最小值点，则解函数有如下形式</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat\beta &amp; = \sum\limits_{i=1}^N(\hat\alpha_i^*-\hat\alpha_i)x_i \tag{12.39}\\
\hat f(x) &amp; = \sum\limits_{i=1}^N(\hat\alpha_i^*-\hat\alpha_i)\langle x, x_i\rangle+\beta_0 \tag{12.40}
\end{align*}
\end{split}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\hat\alpha_i,\; \hat\alpha_i^*\)</span> 是正的，且是下列的二次规划问题的解</p>
<div class="math notranslate nohighlight">
\[
\underset{\alpha_i,\alpha_i^*}{\min}\;\epsilon \sum\limits_{i=1}^N(\alpha_i^*+\alpha_i)-\sum\limits_{i=1}^Ny_i(\alpha_i^*-\alpha_i)+\frac 12\sum_{i,i'=1}^N(\alpha_i^*-\alpha_i)(\alpha_{i'}^*-\alpha_{i'})\langle x_i, x_{i'}\rangle
\]</div>
<p>约束条件如下</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
0\le \alpha_i,\alpha_i^*\le \frac{1}{\lambda}\\
\sum\limits_{i=1}^N(\alpha_i^*-\alpha_i)=0\\
\alpha_i\alpha_i^*=0
\end{align*}
\tag{12.41}
\end{split}\]</div>
<p>考虑到这些约束的本质，一般仅仅有部分的解 <span class="math notranslate nohighlight">\((\hat \alpha_i^*-\hat\alpha_i)\)</span> 非零， 并且对应的数据称为支持向量。和分类问题中的设定一样，解仅仅通过内积 <span class="math notranslate nohighlight">\(\langle x_i, x_{i'}\rangle\)</span> 来关联输入的数据。因此我们可以通过定义合适的内积将这个方法推广到高维空间，举个例子，采用 式（ 12.22 ） 中定义的任意一个。</p>
<p>注意到参数 <span class="math notranslate nohighlight">\(\epsilon\)</span> 和 <span class="math notranslate nohighlight">\(\lambda\)</span> 与准则 式（ 12.36 ） 有关。它们似乎扮演不同的角色，<span class="math notranslate nohighlight">\(\epsilon\)</span> 是损失函数 <span class="math notranslate nohighlight">\(V_{\epsilon}\)</span> 的参数，这和 <span class="math notranslate nohighlight">\(c\)</span> 是 <span class="math notranslate nohighlight">\(V_H\)</span> 的参数一样。注意到 <span class="math notranslate nohighlight">\(V_{\epsilon}\)</span> 和 <span class="math notranslate nohighlight">\(V_H\)</span> 都依赖 <span class="math notranslate nohighlight">\(y\)</span> 的尺度，因此依赖于 <span class="math notranslate nohighlight">\(r\)</span>。如果我们对响应变量进行缩放（也因此需要用 <span class="math notranslate nohighlight">\(V_H(r/\sigma)\)</span> 和 <span class="math notranslate nohighlight">\(V_\epsilon(r/\sigma)\)</span>），则我们可能考虑采用预先设定的 <span class="math notranslate nohighlight">\(c\)</span> 和 <span class="math notranslate nohighlight">\(\epsilon\)</span>（对于高斯情形，取 <span class="math notranslate nohighlight">\(c=1.345\)</span> 能达到 <span class="math notranslate nohighlight">\(95\%\)</span> 的有效度）。<span class="math notranslate nohighlight">\(\lambda\)</span> 是更传统的正则参数，并且可以通过交叉验证来估计。</p>
</div>
<div class="section" id="id16">
<h2>回归和核<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h2>
<p>正如 <span class="xref myst">12.3.3 节</span>讨论的一样，SVM 的核性质不唯一。假设我们考虑用基函数 <span class="math notranslate nohighlight">\(\\{h_m(x)\\},m=1,2,\ldots,M\)</span> 近似的回归函数：</p>
<div class="math notranslate nohighlight">
\[
f(x) = \sum\limits_{m=1}^M\beta_mh_m(x)+\beta_0\tag{12.42}
\]</div>
<p>为了估计 <span class="math notranslate nohighlight">\(\beta\)</span> 和 <span class="math notranslate nohighlight">\(\beta_0\)</span>，对于一般的误差度量<span class="math notranslate nohighlight">\(V(r)\)</span>，我们需要最小化</p>
<div class="math notranslate nohighlight">
\[
H(\beta, \beta_0) = \sum\limits_{i=1}^NV(y_i-f(x_i)) + \frac{\lambda}{2}\sum\beta_m^2\tag{12.43}
\]</div>
<p>对于任意选择的 <span class="math notranslate nohighlight">\(V(r)\)</span>，解 <span class="math notranslate nohighlight">\(\hat f(x)=\sum\hat\beta_mh_m(x)+\hat\beta_0\)</span> 有如下形式</p>
<div class="math notranslate nohighlight">
\[
\hat f(x) = \sum\limits_{i=1}^N\hat\alpha_iK(x,x_i)\tag{12.44}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(K(x,y)=\sum_{m=1}^Mh_m(x)h_m(y)\)</span>。注意到这与径向基函数展开和正则化估计的形式一样，它们在<span class="xref myst">第 5 章</span>和<span class="xref myst">第 6 章</span>有讨论。</p>
<p>具体地(For concreteness)，考虑<span class="math notranslate nohighlight">\(V(r)=r^2\)</span>的情形。令 <span class="math notranslate nohighlight">\(\H\)</span> 为 <span class="math notranslate nohighlight">\(N\times M\)</span> 的基矩阵，第 <span class="math notranslate nohighlight">\(im\)</span> 个元素为 <span class="math notranslate nohighlight">\(h_m(x_i)\)</span>，并且假设 <span class="math notranslate nohighlight">\(M &gt; N\)</span> 较大。为了简单起见，假设 <span class="math notranslate nohighlight">\(\beta_0 = 0\)</span>，或者将常数包含进 <span class="math notranslate nohighlight">\(h\)</span> 中；<a class="reference external" href="https://github.com/szcf-weiya/ESL-CN/issues/82">练习12.3</a>给出了另一种方式。</p>
<p>通过最小化带惩罚的最小二乘准则来估计<span class="math notranslate nohighlight">\(\beta\)</span></p>
<div class="math notranslate nohighlight">
\[
H(\beta) = (y -\H\beta)^T(y-\H\beta)+\lambda\Vert \beta\Vert^2\tag{12.45}
\]</div>
<p>解为</p>
<div class="math notranslate nohighlight">
\[
\haty = \H\hat\beta\tag{12.46}
\]</div>
<p>其中<span class="math notranslate nohighlight">\(\hat\beta\)</span>由下式确定</p>
<div class="math notranslate nohighlight">
\[
-\H^T(y-\H\hat\beta)+\lambda\hat\beta=0\tag{12.47}
\]</div>
<p>从这里看，我们似乎需要在转换后的空间中对内积的 <span class="math notranslate nohighlight">\(M\times M\)</span> 的矩阵进行赋值。然而，我们可以乘以 <span class="math notranslate nohighlight">\(\H\)</span> 得到</p>
<div class="math notranslate nohighlight">
\[
\H\hat\beta = (\H\H^T+\lambda\I)^{-1}\H\H^Ty\tag{12.48}
\]</div>
<p><span class="math notranslate nohighlight">\(N\times N\)</span> 的矩阵 <span class="math notranslate nohighlight">\(\H\H^T\)</span> 包含了成对观测 <span class="math notranslate nohighlight">\(i,i'\)</span> 间的内积；也就是，内积核<span class="math notranslate nohighlight">\(\\{\H\H^T\\}\_{i.i'}=K(x,x_{i'})\)</span>的赋值。易证 式（ 12.44 ） 正是这种情形，在任意点 <span class="math notranslate nohighlight">\(x\)</span> 的预测值满足</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat f(x) &amp; = h(x)^T\hat\beta\\
&amp;=\sum\limits_{i=1}^N\hat\alpha_iK(x,x_i)\tag{12.49}
\end{align*}
\end{split}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\hat\alpha=(\H\H^T+\lambda\I)^{-1}y\)</span>。和 SVM 中一样，我们不需要确定或者计算函数集 <span class="math notranslate nohighlight">\(h_1(x), h_2(x),\ldots,h_M(x)\)</span>。仅仅需要在 <span class="math notranslate nohighlight">\(N\)</span> 个训练点中，对每个 <span class="math notranslate nohighlight">\(i,i'\)</span> 计算内积核 <span class="math notranslate nohighlight">\(K(x_i,x_{i'})\)</span>，以及预测时对预测点 <span class="math notranslate nohighlight">\(x\)</span> 计算内积核。小心选择 <span class="math notranslate nohighlight">\(h_m(x)\)</span>（比如特殊的、容易计算的核<span class="math notranslate nohighlight">\(K\)</span> <span class="math notranslate nohighlight">\(K\)</span> 时<span class="math notranslate nohighlight">\(\H\H^T\)</span> 计算量为 <span class="math notranslate nohighlight">\(N^2/2\)</span>，而不是直接计算时的 <span class="math notranslate nohighlight">\(N^2M\)</span>。</p>
<p>然而，注意到，这个性质取决于惩罚项中平方范数 <span class="math notranslate nohighlight">\(\Vert\beta\Vert^2\)</span> 的选择。举个例子，对于可能会得到更好模型的 <span class="math notranslate nohighlight">\(L_1\)</span> 范数 <span class="math notranslate nohighlight">\(\vert\beta\vert\)</span>，这个性质并不满足。</p>
</div>
<div class="section" id="id17">
<h2>讨论<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h2>
<p>SVM可以拓展到多类别问题，本质上是求解多个两个类别的问题。对每个成对类别建立分类器，最终的分类器是起主要作用的分类器(Kressel, 1999<a class="footnote-reference brackets" href="#id28" id="id18">8</a>; Friedman, 1996<a class="footnote-reference brackets" href="#id29" id="id19">9</a>; Hastie and Tibshirani, 1998<a class="footnote-reference brackets" href="#id30" id="id20">10</a>)。另外，可以采用合适核的多项损失函数，就像<a class="reference external" href="#_1">12.3.3节</a>一样。SVMs在许多监督学习和非监督学习的问题中都有应用。在本文的写作时间，经验表明它在许多实际学习问题中表现得非常好。</p>
<p>最后，我们提出 SVM 和 <strong>结构风险最小化 (SRM, structural risk minimization)</strong> （<a class="reference internal" href="../07-Model-Assessment-and-Selection/7.9-Vapnik-Chervonenkis-Dimension.html"><span class="doc std std-doc">7.9 节</span></a>）间的联系。</p>
<p>假设训练点（或者它们的基函数展开）在半径为 <span class="math notranslate nohighlight">\(R\)</span> 的球体中，并且令 式（ 12.2 ） 中的 <span class="math notranslate nohighlight">\(G(x)=\mathrm{sign}[f(x)]=\mathrm{sign}[\beta^Tx+\beta_0]\)</span>。</p>
<p>!!! note “Recall”
$<span class="math notranslate nohighlight">\(
    G(x)=\mathrm{sign}[x^T\beta+\beta_0]\tag{12.2}
    \)</span>$</p>
<p>则可以证明类别函数 <span class="math notranslate nohighlight">\(\{G(x),\Vert\beta\Vert\le A\}\)</span> 的 VC 维 <span class="math notranslate nohighlight">\(h\)</span> 满足</p>
<div class="math notranslate nohighlight">
\[
h\le R^2A^2\tag{12.50}
\]</div>
<p>如果 <span class="math notranslate nohighlight">\(f(x)\)</span> 分离训练数据，对于 <span class="math notranslate nohighlight">\(\Vert \beta\Vert\le A\)</span> 是最优的，则在训练集上至少有概率 <span class="math notranslate nohighlight">\(1-\eta\)</span> 满足</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Error}_{\mathrm{Test}}\le 4\frac{h[\log(2N/h)+1]-\log(\eta/4)}{N}\tag{12.51}
\]</div>
<p>支持向量分类器是第一个可以得到 VC 维的有效界的实际学习过程，因此可以应用 SRM。然而，在推导的时候，计算过程依赖特征的观测值，称为“balls are put around the data points”。因此在严格意义上，类别的 VC 复杂度在知道特征之前不是预先固定的。</p>
<p>正则参数 <span class="math notranslate nohighlight">\(C\)</span> 控制了分类器的 VC 维的上界。遵循 SRM 的范式，我们可以通过最小化测试误差 式（ 12.51 ） 的上界来选择 <span class="math notranslate nohighlight">\(C\)</span>。然而，并不清楚它与通过交叉验证来选 <span class="math notranslate nohighlight">\(C\)</span> 有什么优势。</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id21"><span class="brackets">1</span><span class="fn-backref">(<a href="#id3">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Rosset, S. and Zhu, J. (2007). Piecewise linear regularized solution paths, Annals of Statistics 35(3): 1012–1030. <span class="xref myst">下载</span></p>
</dd>
<dt class="label" id="id22"><span class="brackets"><a class="fn-backref" href="#id5">2</a></span></dt>
<dd><p>Rosset, S., Zhu, J. and Hastie, T. (2004b). Margin maximizing loss functions, in S. Thrun, L. Saul and B. Schölkopf (eds), Advances in Neural Information Processing Systems 16, MIT Press, Cambridge, MA.</p>
</dd>
<dt class="label" id="id23"><span class="brackets"><a class="fn-backref" href="#id7">3</a></span></dt>
<dd><p>Wahba, G., Lin, Y. and Zhang, H. (2000). GACV for support vector machines, in A. Smola, P. Bartlett, B. Sch¨olkopf and D. Schuurmans (eds), Advances in Large Margin Classifiers, MIT Press, Cambridge, MA., pp. 297–311.</p>
</dd>
<dt class="label" id="id24"><span class="brackets">4</span><span class="fn-backref">(<a href="#id8">1</a>,<a href="#id10">2</a>)</span></dt>
<dd><p>Wahba, G. (1990). Spline Models for Observational Data, SIAM, Philadelphia.</p>
</dd>
<dt class="label" id="id25"><span class="brackets"><a class="fn-backref" href="#id9">5</a></span></dt>
<dd><p>Hastie, T. and Herman, A. (1990). An analysis of gestational age, neonatal size and neonatal death using nonparametric logistic regression, Journal of Clinical Epidemiology 43: 1179–90.</p>
</dd>
<dt class="label" id="id26"><span class="brackets"><a class="fn-backref" href="#id13">6</a></span></dt>
<dd><p>Hastie, T., Rosset, S., Tibshirani, R. and Zhu, J. (2004). The entire regularization path for the support vector machine, Journal of Machine Learning Research 5: 1391–1415.</p>
</dd>
<dt class="label" id="id27"><span class="brackets"><a class="fn-backref" href="#id15">7</a></span></dt>
<dd><p>Huber, P. (1964). Robust estimation of a location parameter, Annals of Mathematical Statistics 53: 73–101.</p>
</dd>
<dt class="label" id="id28"><span class="brackets"><a class="fn-backref" href="#id18">8</a></span></dt>
<dd><p>Kressel, U. (1999). Pairwise classification and support vector machines, in B. Sch¨olkopf, C. Burges and A. Smola (eds), Advances in Kernel Methods - Support Vector Learning, MIT Press, Cambridge, MA.,pp. 255–268.</p>
</dd>
<dt class="label" id="id29"><span class="brackets"><a class="fn-backref" href="#id19">9</a></span></dt>
<dd><p>Friedman, J. (1996). Another approach to polychotomous classification, Technical report, Stanford University</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id20">10</a></span></dt>
<dd><p>Hastie, T. and Tibshirani, R. (1998). Classification by pairwise coupling, Annals of Statistics 26(2): 451–471.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./12-Support-Vector-Machines-and-Flexible-Discriminants"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="12.2-The-Support-Vector-Classifier.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">12.2 支持向量分类器</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="12.4-Generalizing-Linear-Discriminant-Analysis.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">12.4 广义线性判别分析</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Trevor Hastie and Robert Tibshirani and Jerome Friedman<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>